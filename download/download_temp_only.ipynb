{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üå°Ô∏è Descarga y Procesamiento de Datos de Temperatura\n",
        "\n",
        "Este notebook descarga datos de temperatura desde InfluxDB, los procesa y genera un archivo final limpio.\n",
        "\n",
        "## üìã Proceso:\n",
        "1. **Configuraci√≥n** inicial y conexi√≥n a InfluxDB\n",
        "2. **Descarga** de datos hist√≥ricos (TempModFixed) y recientes (fixed_plant_atamo_1)\n",
        "3. **Correcci√≥n** del formato deformado (sensores en filas separadas)\n",
        "4. **Combinaci√≥n** de ambos datasets\n",
        "5. **Limpieza** final y generaci√≥n de `data_temp.csv`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-25 09:22:36 - INFO - üå°Ô∏è DESCARGA DE DATOS DE TEMPERATURA\n",
            "2025-06-25 09:22:36 - INFO - üìÖ Per√≠odo: 2024-07-01 a 2025-06-30\n",
            "2025-06-25 09:22:36 - INFO - üìÅ Directorio: /home/nicole/SR/SOILING/datos\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üêç Python: 3.12.3 (main, Jun 18 2025, 17:59:45) [GCC 13.3.0]\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# üìö IMPORTACIONES Y CONFIGURACI√ìN INICIAL\n",
        "# ============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import logging\n",
        "import gc\n",
        "from datetime import datetime, timedelta\n",
        "from influxdb_client import InfluxDBClient\n",
        "\n",
        "# Configurar logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    datefmt='%Y-%m-%d %H:%M:%S'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Configuraci√≥n de InfluxDB\n",
        "INFLUX_CONFIG = {\n",
        "    'url': \"http://146.83.153.212:27017\",\n",
        "    'token': \"piDbFR_bfRWO5Epu1IS96WbkNpSZZCYgwZZR29PcwUsxXwKdIyLMhVAhU4-5ohWeXIsX7Dp_X-WiPIDx0beafg==\",\n",
        "    'org': \"atamostec\",\n",
        "    'timeout': 300000\n",
        "}\n",
        "\n",
        "# Configuraci√≥n de fechas y rutas\n",
        "START_DATE = pd.to_datetime('01/07/2024', dayfirst=True).tz_localize('UTC')\n",
        "END_DATE = pd.to_datetime('30/06/2025', dayfirst=True).tz_localize('UTC')\n",
        "OUTPUT_DIR = \"/home/nicole/SR/SOILING/datos\"\n",
        "CUTOFF_DATE = pd.to_datetime('2024-12-05 16:00:00+00:00')  # Fecha donde cambia la fuente\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "logger.info(f\"üå°Ô∏è DESCARGA DE DATOS DE TEMPERATURA\")\n",
        "logger.info(f\"üìÖ Per√≠odo: {START_DATE.strftime('%Y-%m-%d')} a {END_DATE.strftime('%Y-%m-%d')}\")\n",
        "logger.info(f\"üìÅ Directorio: {OUTPUT_DIR}\")\n",
        "print(f\"üêç Python: {sys.version}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-25 09:22:37 - INFO - ‚úÖ Clase InfluxDBManager definida\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# üîå CLASE PARA MANEJO DE INFLUXDB\n",
        "# ============================================================================\n",
        "\n",
        "class InfluxDBManager:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.client = None\n",
        "        self.query_api = None\n",
        "        \n",
        "    def connect(self):\n",
        "        try:\n",
        "            self.client = InfluxDBClient(\n",
        "                url=self.config['url'],\n",
        "                token=self.config['token'],\n",
        "                org=self.config['org'],\n",
        "                timeout=self.config['timeout']\n",
        "            )\n",
        "            self.query_api = self.client.query_api()\n",
        "            logger.info(\"‚úÖ Cliente InfluxDB inicializado\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚ùå Error conectando a InfluxDB: {e}\")\n",
        "            return False\n",
        "    \n",
        "    def query_influxdb(self, bucket, tables, attributes, start_date, end_date):\n",
        "        try:\n",
        "            # Construir filtros correctamente para Flux\n",
        "            if len(tables) == 1:\n",
        "                tables_filter = f'r._measurement == \"{tables[0]}\"'\n",
        "            else:\n",
        "                tables_conditions = [f'r._measurement == \"{table}\"' for table in tables]\n",
        "                tables_filter = ' or '.join(tables_conditions)\n",
        "            \n",
        "            if len(attributes) == 1:\n",
        "                fields_filter = f'r._field == \"{attributes[0]}\"'\n",
        "            else:\n",
        "                fields_conditions = [f'r._field == \"{field}\"' for field in attributes]\n",
        "                fields_filter = ' or '.join(fields_conditions)\n",
        "            \n",
        "            query = f\"\"\"\n",
        "            from(bucket: \"{bucket}\")\n",
        "            |> range(start: {start_date.strftime('%Y-%m-%dT%H:%M:%SZ')}, stop: {end_date.strftime('%Y-%m-%dT%H:%M:%SZ')})\n",
        "            |> filter(fn: (r) => {tables_filter})\n",
        "            |> filter(fn: (r) => {fields_filter})\n",
        "            |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n",
        "            \"\"\"\n",
        "            \n",
        "            logger.info(f\"üìä Consultando: bucket={bucket}, tables={tables}\")\n",
        "            logger.info(f\"üìÖ Per√≠odo: {start_date.strftime('%Y-%m-%dT%H:%M:%SZ')} a {end_date.strftime('%Y-%m-%dT%H:%M:%SZ')}\")\n",
        "            \n",
        "            result = self.query_api.query_data_frame(query)\n",
        "            \n",
        "            if result is not None and not result.empty:\n",
        "                logger.info(f\"‚úÖ Datos obtenidos: {len(result)} registros\")\n",
        "                return result\n",
        "            else:\n",
        "                logger.warning(\"‚ö†Ô∏è No se encontraron datos para el per√≠odo consultado\")\n",
        "                return None\n",
        "                \n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚ùå Error en consulta: {e}\")\n",
        "            return None\n",
        "    \n",
        "    def disconnect(self):\n",
        "        if self.client:\n",
        "            self.client.close()\n",
        "            logger.info(\"üîå Conexi√≥n a InfluxDB cerrada\")\n",
        "\n",
        "logger.info(\"‚úÖ Clase InfluxDBManager definida\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-25 09:22:38 - INFO - ‚úÖ Funci√≥n de correcci√≥n de formato definida\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# üîß FUNCI√ìN PARA CORREGIR FORMATO DEFORMADO\n",
        "# ============================================================================\n",
        "\n",
        "def fix_temperature_format(df):\n",
        "    \"\"\"\n",
        "    Corrige el formato deformado donde cada sensor est√° en filas separadas.\n",
        "    Consolida los datos para que todos los sensores est√©n en una sola fila por timestamp.\n",
        "    \"\"\"\n",
        "    logger.info(\"üîß Aplicando correcci√≥n de formato...\")\n",
        "    \n",
        "    try:\n",
        "        temp_cols = ['1TE416(C)', '1TE417(C)', '1TE418(C)', '1TE419(C)']\n",
        "        \n",
        "        # Contar valores no nulos por fila\n",
        "        df['non_null_count'] = df[temp_cols].notna().sum(axis=1)\n",
        "        \n",
        "        # Separar datos correctos (4 valores) de deformados (1 valor)\n",
        "        df_correct = df[df['non_null_count'] == 4].copy()\n",
        "        df_deformed = df[df['non_null_count'] == 1].copy()\n",
        "        \n",
        "        logger.info(f\"üìä Datos correctos: {len(df_correct):,} filas\")\n",
        "        logger.info(f\"üîß Datos deformados: {len(df_deformed):,} filas\")\n",
        "        \n",
        "        if len(df_deformed) == 0:\n",
        "            logger.info(\"‚úÖ No hay datos deformados que corregir\")\n",
        "            return df_correct.drop('non_null_count', axis=1)\n",
        "        \n",
        "        # Procesar datos deformados\n",
        "        logger.info(\"üîÑ Consolidando datos deformados...\")\n",
        "        \n",
        "        # Redondear timestamps para agrupar\n",
        "        df_deformed['_time_rounded'] = df_deformed.index.round('1min')\n",
        "        \n",
        "        consolidated_rows = []\n",
        "        \n",
        "        for timestamp, group in df_deformed.groupby('_time_rounded'):\n",
        "            # Crear fila consolidada\n",
        "            consolidated_row = {\n",
        "                '_time': timestamp,\n",
        "                '1TE416(C)': np.nan,\n",
        "                '1TE417(C)': np.nan,\n",
        "                '1TE418(C)': np.nan,\n",
        "                '1TE419(C)': np.nan\n",
        "            }\n",
        "            \n",
        "            # Extraer valores de cada sensor\n",
        "            for _, row in group.iterrows():\n",
        "                for col in temp_cols:\n",
        "                    if pd.notna(row[col]):\n",
        "                        consolidated_row[col] = row[col]\n",
        "            \n",
        "            # Solo agregar si tiene al menos un valor\n",
        "            if any(pd.notna(consolidated_row[col]) for col in temp_cols):\n",
        "                consolidated_rows.append(consolidated_row)\n",
        "        \n",
        "        if consolidated_rows:\n",
        "            df_consolidated = pd.DataFrame(consolidated_rows)\n",
        "            df_consolidated.set_index('_time', inplace=True)\n",
        "            logger.info(f\"‚úÖ Datos consolidados: {len(df_consolidated):,} filas\")\n",
        "            \n",
        "            # Combinar datos correctos y consolidados\n",
        "            df_final = pd.concat([df_correct.drop('non_null_count', axis=1), df_consolidated])\n",
        "            df_final = df_final.sort_index()\n",
        "            \n",
        "            logger.info(f\"üìä Total final: {len(df_final):,} filas\")\n",
        "            return df_final\n",
        "        else:\n",
        "            logger.warning(\"‚ö†Ô∏è No se pudieron consolidar los datos deformados\")\n",
        "            return df_correct.drop('non_null_count', axis=1)\n",
        "            \n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error en correcci√≥n de formato: {e}\")\n",
        "        return df.drop('non_null_count', axis=1, errors='ignore')\n",
        "\n",
        "logger.info(\"‚úÖ Funci√≥n de correcci√≥n de formato definida\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-25 09:22:39 - INFO - ‚úÖ Funci√≥n principal de descarga definida\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# üì• FUNCI√ìN PRINCIPAL DE DESCARGA\n",
        "# ============================================================================\n",
        "\n",
        "def download_temperature_data(influx_client, start_date, end_date, output_dir):\n",
        "    \"\"\"\n",
        "    Descarga datos de temperatura de ambas fuentes y los procesa.\n",
        "    \"\"\"\n",
        "    logger.info(\"üå°Ô∏è Iniciando descarga de datos de temperatura...\")\n",
        "    \n",
        "    try:\n",
        "        bucket = \"PSDA\"\n",
        "        attributes = [\"1TE416(C)\", \"1TE417(C)\", \"1TE418(C)\", \"1TE419(C)\"]\n",
        "        \n",
        "        all_dataframes = []\n",
        "        \n",
        "        # PER√çODO 1: Datos hist√≥ricos (TempModFixed)\n",
        "        logger.info(f\"\\nüîÑ DESCARGANDO DATOS HIST√ìRICOS (TempModFixed)...\")\n",
        "        logger.info(f\"üìÖ Per√≠odo: {start_date.strftime('%Y-%m-%d')} a {CUTOFF_DATE.strftime('%Y-%m-%d')}\")\n",
        "        \n",
        "        current_date = start_date\n",
        "        period_1_end = min(CUTOFF_DATE, end_date)\n",
        "        \n",
        "        while current_date < period_1_end:\n",
        "            week_end = min(current_date + timedelta(days=7), period_1_end)\n",
        "            \n",
        "            df_temp = influx_client.query_influxdb(bucket, [\"TempModFixed\"], attributes, current_date, week_end)\n",
        "            \n",
        "            if df_temp is not None and not df_temp.empty:\n",
        "                # Procesar DataFrame\n",
        "                if '_time' in df_temp.columns:\n",
        "                    df_temp.set_index('_time', inplace=True)\n",
        "                \n",
        "                if not isinstance(df_temp.index, pd.DatetimeIndex):\n",
        "                    df_temp.index = pd.to_datetime(df_temp.index)\n",
        "                \n",
        "                # Filtrar horario (13:00-18:00)\n",
        "                df_temp = df_temp.between_time('13:00', '18:00')\n",
        "                \n",
        "                if not df_temp.empty:\n",
        "                    # Limpiar columnas\n",
        "                    cols_to_drop = [col for col in ['result', 'table', '_start', '_stop', '_measurement'] if col in df_temp.columns]\n",
        "                    if cols_to_drop:\n",
        "                        df_temp.drop(columns=cols_to_drop, inplace=True)\n",
        "                    \n",
        "                    all_dataframes.append(df_temp)\n",
        "                    logger.info(f\"   ‚úÖ Semana procesada: {len(df_temp):,} registros\")\n",
        "            \n",
        "            current_date = week_end\n",
        "            gc.collect()\n",
        "        \n",
        "        # PER√çODO 2: Datos recientes (fixed_plant_atamo_1)\n",
        "        if CUTOFF_DATE < end_date:\n",
        "            logger.info(f\"\\nüîÑ DESCARGANDO DATOS RECIENTES (fixed_plant_atamo_1)...\")\n",
        "            logger.info(f\"üìÖ Per√≠odo: {CUTOFF_DATE.strftime('%Y-%m-%d')} a {end_date.strftime('%Y-%m-%d')}\")\n",
        "            \n",
        "            current_date = CUTOFF_DATE\n",
        "            while current_date < end_date:\n",
        "                week_end = min(current_date + timedelta(days=7), end_date)\n",
        "                \n",
        "                df_temp = influx_client.query_influxdb(bucket, [\"fixed_plant_atamo_1\"], attributes, current_date, week_end)\n",
        "                \n",
        "                if df_temp is not None and not df_temp.empty:\n",
        "                    # Procesar DataFrame\n",
        "                    if '_time' in df_temp.columns:\n",
        "                        df_temp.set_index('_time', inplace=True)\n",
        "                    \n",
        "                    if not isinstance(df_temp.index, pd.DatetimeIndex):\n",
        "                        df_temp.index = pd.to_datetime(df_temp.index)\n",
        "                    \n",
        "                    # Filtrar horario (13:00-18:00)\n",
        "                    df_temp = df_temp.between_time('13:00', '18:00')\n",
        "                    \n",
        "                    if not df_temp.empty:\n",
        "                        # Limpiar columnas\n",
        "                        cols_to_drop = [col for col in ['result', 'table', '_start', '_stop', '_measurement'] if col in df_temp.columns]\n",
        "                        if cols_to_drop:\n",
        "                            df_temp.drop(columns=cols_to_drop, inplace=True)\n",
        "                        \n",
        "                        all_dataframes.append(df_temp)\n",
        "                        logger.info(f\"   ‚úÖ Semana procesada: {len(df_temp):,} registros\")\n",
        "                \n",
        "                current_date = week_end\n",
        "                gc.collect()\n",
        "        \n",
        "        if not all_dataframes:\n",
        "            logger.error(\"‚ùå No se obtuvieron datos\")\n",
        "            return False\n",
        "        \n",
        "        # COMBINAR TODOS LOS DATOS\n",
        "        logger.info(f\"\\nüìä COMBINANDO Y PROCESANDO TODOS LOS DATOS...\")\n",
        "        df_combined = pd.concat(all_dataframes, ignore_index=False)\n",
        "        del all_dataframes\n",
        "        gc.collect()\n",
        "        \n",
        "        # Procesar datos combinados\n",
        "        df_combined = df_combined.sort_index()\n",
        "        df_combined = df_combined[~df_combined.index.duplicated(keep='first')]\n",
        "        \n",
        "        logger.info(f\"üìä Datos antes de correcci√≥n: {len(df_combined):,} registros\")\n",
        "        \n",
        "        # APLICAR CORRECCI√ìN DE FORMATO\n",
        "        df_corrected = fix_temperature_format(df_combined)\n",
        "        \n",
        "        # GUARDAR ARCHIVO FINAL\n",
        "        final_file = os.path.join(output_dir, 'data_temp.csv')\n",
        "        logger.info(f\"üíæ Guardando archivo final: {final_file}\")\n",
        "        df_corrected.to_csv(final_file)\n",
        "        \n",
        "        # ESTAD√çSTICAS FINALES\n",
        "        file_size_mb = os.path.getsize(final_file) / (1024*1024)\n",
        "        \n",
        "        logger.info(f\"\\n\" + \"=\"*70)\n",
        "        logger.info(f\"‚úÖ DESCARGA COMPLETADA EXITOSAMENTE\")\n",
        "        logger.info(f\"üìä Registros finales: {len(df_corrected):,}\")\n",
        "        logger.info(f\"üìÖ Rango: {df_corrected.index.min()} a {df_corrected.index.max()}\")\n",
        "        logger.info(f\"üìÅ Archivo: data_temp.csv ({file_size_mb:.2f} MB)\")\n",
        "        \n",
        "        # Estad√≠sticas por sensor\n",
        "        for col in df_corrected.columns:\n",
        "            valid_count = df_corrected[col].notna().sum()\n",
        "            percentage = valid_count/len(df_corrected)*100\n",
        "            logger.info(f\"üå°Ô∏è {col}: {valid_count:,} valores ({percentage:.1f}%)\")\n",
        "        \n",
        "        logger.info(\"=\"*70)\n",
        "        \n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error en la descarga: {e}\")\n",
        "        import traceback\n",
        "        logger.error(f\"üîç Detalles: {traceback.format_exc()}\")\n",
        "        return False\n",
        "\n",
        "logger.info(\"‚úÖ Funci√≥n principal de descarga definida\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-25 09:22:40 - INFO - \n",
            "üöÄ INICIANDO DESCARGA COMPLETA DE DATOS DE TEMPERATURA\n",
            "2025-06-25 09:22:40 - INFO - ================================================================================\n",
            "2025-06-25 09:22:41 - INFO - ‚úÖ Cliente InfluxDB inicializado\n",
            "2025-06-25 09:22:41 - INFO - üå°Ô∏è Iniciando descarga de datos de temperatura...\n",
            "2025-06-25 09:22:41 - INFO - \n",
            "üîÑ DESCARGANDO DATOS HIST√ìRICOS (TempModFixed)...\n",
            "2025-06-25 09:22:41 - INFO - üìÖ Per√≠odo: 2024-07-01 a 2024-12-05\n",
            "2025-06-25 09:22:41 - INFO - üìä Consultando: bucket=PSDA, tables=['TempModFixed']\n",
            "2025-06-25 09:22:41 - INFO - üìÖ Per√≠odo: 2024-07-01T00:00:00Z a 2024-07-08T00:00:00Z\n",
            "2025-06-25 09:22:48 - INFO - ‚úÖ Datos obtenidos: 120934 registros\n",
            "2025-06-25 09:22:48 - INFO -    ‚úÖ Semana procesada: 25,194 registros\n",
            "2025-06-25 09:22:48 - INFO - üìä Consultando: bucket=PSDA, tables=['TempModFixed']\n",
            "2025-06-25 09:22:48 - INFO - üìÖ Per√≠odo: 2024-07-08T00:00:00Z a 2024-07-15T00:00:00Z\n",
            "2025-06-25 09:22:54 - INFO - ‚úÖ Datos obtenidos: 120864 registros\n",
            "2025-06-25 09:22:54 - INFO -    ‚úÖ Semana procesada: 25,134 registros\n",
            "2025-06-25 09:22:54 - INFO - üìä Consultando: bucket=PSDA, tables=['TempModFixed']\n",
            "2025-06-25 09:22:54 - INFO - üìÖ Per√≠odo: 2024-07-15T00:00:00Z a 2024-07-22T00:00:00Z\n",
            "2025-06-25 09:23:00 - INFO - ‚úÖ Datos obtenidos: 120936 registros\n",
            "2025-06-25 09:23:00 - INFO -    ‚úÖ Semana procesada: 25,200 registros\n",
            "2025-06-25 09:23:00 - INFO - üìä Consultando: bucket=PSDA, tables=['TempModFixed']\n",
            "2025-06-25 09:23:00 - INFO - üìÖ Per√≠odo: 2024-07-22T00:00:00Z a 2024-07-29T00:00:00Z\n",
            "2025-06-25 09:23:05 - INFO - ‚úÖ Datos obtenidos: 120943 registros\n",
            "2025-06-25 09:23:05 - INFO -    ‚úÖ Semana procesada: 25,199 registros\n",
            "2025-06-25 09:23:05 - INFO - üìä Consultando: bucket=PSDA, tables=['TempModFixed']\n",
            "2025-06-25 09:23:05 - INFO - üìÖ Per√≠odo: 2024-07-29T00:00:00Z a 2024-08-05T00:00:00Z\n",
            "2025-06-25 09:23:07 - INFO - ‚úÖ Datos obtenidos: 120947 registros\n",
            "2025-06-25 09:23:07 - INFO -    ‚úÖ Semana procesada: 25,199 registros\n",
            "2025-06-25 09:23:07 - INFO - üìä Consultando: bucket=PSDA, tables=['TempModFixed']\n",
            "2025-06-25 09:23:07 - INFO - üìÖ Per√≠odo: 2024-08-05T00:00:00Z a 2024-08-12T00:00:00Z\n",
            "2025-06-25 09:23:11 - INFO - ‚úÖ Datos obtenidos: 120942 registros\n",
            "2025-06-25 09:23:11 - INFO -    ‚úÖ Semana procesada: 25,200 registros\n",
            "2025-06-25 09:23:11 - INFO - üìä Consultando: bucket=PSDA, tables=['TempModFixed']\n",
            "2025-06-25 09:23:12 - INFO - üìÖ Per√≠odo: 2024-08-12T00:00:00Z a 2024-08-19T00:00:00Z\n",
            "2025-06-25 09:23:16 - INFO - ‚úÖ Datos obtenidos: 120948 registros\n",
            "2025-06-25 09:23:16 - INFO -    ‚úÖ Semana procesada: 25,198 registros\n",
            "2025-06-25 09:23:16 - INFO - üìä Consultando: bucket=PSDA, tables=['TempModFixed']\n",
            "2025-06-25 09:23:16 - INFO - üìÖ Per√≠odo: 2024-08-19T00:00:00Z a 2024-08-26T00:00:00Z\n",
            "2025-06-25 09:23:20 - INFO - ‚úÖ Datos obtenidos: 120935 registros\n",
            "2025-06-25 09:23:20 - INFO -    ‚úÖ Semana procesada: 25,196 registros\n",
            "2025-06-25 09:23:20 - INFO - üìä Consultando: bucket=PSDA, tables=['TempModFixed']\n",
            "2025-06-25 09:23:20 - INFO - üìÖ Per√≠odo: 2024-08-26T00:00:00Z a 2024-09-02T00:00:00Z\n",
            "2025-06-25 09:23:24 - INFO - ‚úÖ Datos obtenidos: 120953 registros\n",
            "2025-06-25 09:23:24 - INFO -    ‚úÖ Semana procesada: 25,201 registros\n",
            "2025-06-25 09:23:25 - INFO - üìä Consultando: bucket=PSDA, tables=['TempModFixed']\n",
            "2025-06-25 09:23:25 - INFO - üìÖ Per√≠odo: 2024-09-02T00:00:00Z a 2024-09-09T00:00:00Z\n",
            "2025-06-25 09:23:28 - INFO - ‚úÖ Datos obtenidos: 101759 registros\n",
            "2025-06-25 09:23:28 - INFO -    ‚úÖ Semana procesada: 21,600 registros\n",
            "2025-06-25 09:23:28 - INFO - üìä Consultando: bucket=PSDA, tables=['TempModFixed']\n",
            "2025-06-25 09:23:28 - INFO - üìÖ Per√≠odo: 2024-09-09T00:00:00Z a 2024-09-16T00:00:00Z\n",
            "2025-06-25 09:23:32 - INFO - ‚úÖ Datos obtenidos: 110006 registros\n",
            "2025-06-25 09:23:32 - INFO -    ‚úÖ Semana procesada: 22,911 registros\n",
            "2025-06-25 09:23:32 - INFO - üìä Consultando: bucket=PSDA, tables=['TempModFixed']\n",
            "2025-06-25 09:23:32 - INFO - üìÖ Per√≠odo: 2024-09-16T00:00:00Z a 2024-09-23T00:00:00Z\n",
            "2025-06-25 09:23:37 - INFO - ‚úÖ Datos obtenidos: 120937 registros\n",
            "2025-06-25 09:23:37 - INFO -    ‚úÖ Semana procesada: 25,197 registros\n",
            "2025-06-25 09:23:37 - INFO - üìä Consultando: bucket=PSDA, tables=['TempModFixed']\n",
            "2025-06-25 09:23:37 - INFO - üìÖ Per√≠odo: 2024-09-23T00:00:00Z a 2024-09-30T00:00:00Z\n",
            "2025-06-25 09:23:40 - INFO - ‚úÖ Datos obtenidos: 120940 registros\n",
            "2025-06-25 09:23:40 - INFO -    ‚úÖ Semana procesada: 25,197 registros\n",
            "2025-06-25 09:23:40 - INFO - üìä Consultando: bucket=PSDA, tables=['TempModFixed']\n",
            "2025-06-25 09:23:40 - INFO - üìÖ Per√≠odo: 2024-09-30T00:00:00Z a 2024-10-07T00:00:00Z\n",
            "2025-06-25 09:23:44 - INFO - ‚úÖ Datos obtenidos: 120946 registros\n",
            "2025-06-25 09:23:44 - INFO -    ‚úÖ Semana procesada: 25,200 registros\n",
            "2025-06-25 09:23:45 - INFO - üìä Consultando: bucket=PSDA, tables=['TempModFixed']\n",
            "2025-06-25 09:23:45 - INFO - üìÖ Per√≠odo: 2024-10-07T00:00:00Z a 2024-10-14T00:00:00Z\n",
            "2025-06-25 09:23:50 - INFO - ‚úÖ Datos obtenidos: 119689 registros\n",
            "2025-06-25 09:23:50 - INFO -    ‚úÖ Semana procesada: 23,949 registros\n",
            "2025-06-25 09:23:50 - INFO - üìä Consultando: bucket=PSDA, tables=['TempModFixed']\n",
            "2025-06-25 09:23:50 - INFO - üìÖ Per√≠odo: 2024-10-14T00:00:00Z a 2024-10-21T00:00:00Z\n",
            "2025-06-25 09:23:54 - INFO - ‚úÖ Datos obtenidos: 120939 registros\n",
            "2025-06-25 09:23:54 - INFO -    ‚úÖ Semana procesada: 25,199 registros\n",
            "2025-06-25 09:23:54 - INFO - üìä Consultando: bucket=PSDA, tables=['TempModFixed']\n",
            "2025-06-25 09:23:54 - INFO - üìÖ Per√≠odo: 2024-10-21T00:00:00Z a 2024-10-28T00:00:00Z\n",
            "2025-06-25 09:23:59 - INFO - ‚úÖ Datos obtenidos: 120907 registros\n",
            "2025-06-25 09:23:59 - INFO -    ‚úÖ Semana procesada: 25,159 registros\n",
            "2025-06-25 09:23:59 - INFO - üìä Consultando: bucket=PSDA, tables=['TempModFixed']\n",
            "2025-06-25 09:23:59 - INFO - üìÖ Per√≠odo: 2024-10-28T00:00:00Z a 2024-11-04T00:00:00Z\n",
            "2025-06-25 09:24:07 - INFO - ‚úÖ Datos obtenidos: 120940 registros\n",
            "2025-06-25 09:24:07 - INFO -    ‚úÖ Semana procesada: 25,198 registros\n",
            "2025-06-25 09:24:07 - INFO - üìä Consultando: bucket=PSDA, tables=['TempModFixed']\n",
            "2025-06-25 09:24:07 - INFO - üìÖ Per√≠odo: 2024-11-04T00:00:00Z a 2024-11-11T00:00:00Z\n",
            "2025-06-25 09:24:12 - INFO - ‚úÖ Datos obtenidos: 120938 registros\n",
            "2025-06-25 09:24:12 - INFO -    ‚úÖ Semana procesada: 25,200 registros\n",
            "2025-06-25 09:24:12 - INFO - üìä Consultando: bucket=PSDA, tables=['TempModFixed']\n",
            "2025-06-25 09:24:12 - INFO - üìÖ Per√≠odo: 2024-11-11T00:00:00Z a 2024-11-18T00:00:00Z\n",
            "2025-06-25 09:24:16 - INFO - ‚úÖ Datos obtenidos: 119843 registros\n",
            "2025-06-25 09:24:16 - INFO -    ‚úÖ Semana procesada: 24,099 registros\n",
            "2025-06-25 09:24:17 - INFO - üìä Consultando: bucket=PSDA, tables=['TempModFixed']\n",
            "2025-06-25 09:24:17 - INFO - üìÖ Per√≠odo: 2024-11-18T00:00:00Z a 2024-11-25T00:00:00Z\n",
            "2025-06-25 09:24:22 - INFO - ‚úÖ Datos obtenidos: 120944 registros\n",
            "2025-06-25 09:24:22 - INFO -    ‚úÖ Semana procesada: 25,199 registros\n",
            "2025-06-25 09:24:22 - INFO - üìä Consultando: bucket=PSDA, tables=['TempModFixed']\n",
            "2025-06-25 09:24:22 - INFO - üìÖ Per√≠odo: 2024-11-25T00:00:00Z a 2024-12-02T00:00:00Z\n",
            "2025-06-25 09:24:25 - INFO - ‚úÖ Datos obtenidos: 89252 registros\n",
            "2025-06-25 09:24:25 - INFO -    ‚úÖ Semana procesada: 18,598 registros\n",
            "2025-06-25 09:24:26 - INFO - üìä Consultando: bucket=PSDA, tables=['TempModFixed']\n",
            "2025-06-25 09:24:26 - INFO - üìÖ Per√≠odo: 2024-12-02T00:00:00Z a 2024-12-05T16:00:00Z\n",
            "2025-06-25 09:24:29 - INFO - ‚úÖ Datos obtenidos: 63106 registros\n",
            "2025-06-25 09:24:29 - INFO -    ‚úÖ Semana procesada: 12,715 registros\n",
            "2025-06-25 09:24:29 - INFO - \n",
            "üîÑ DESCARGANDO DATOS RECIENTES (fixed_plant_atamo_1)...\n",
            "2025-06-25 09:24:29 - INFO - üìÖ Per√≠odo: 2024-12-05 a 2025-06-30\n",
            "2025-06-25 09:24:29 - INFO - üìä Consultando: bucket=PSDA, tables=['fixed_plant_atamo_1']\n",
            "2025-06-25 09:24:29 - INFO - üìÖ Per√≠odo: 2024-12-05T16:00:00Z a 2024-12-12T16:00:00Z\n",
            "2025-06-25 09:24:29 - WARNING - ‚ö†Ô∏è No se encontraron datos para el per√≠odo consultado\n",
            "2025-06-25 09:24:30 - INFO - üìä Consultando: bucket=PSDA, tables=['fixed_plant_atamo_1']\n",
            "2025-06-25 09:24:30 - INFO - üìÖ Per√≠odo: 2024-12-12T16:00:00Z a 2024-12-19T16:00:00Z\n",
            "2025-06-25 09:24:31 - INFO - ‚úÖ Datos obtenidos: 34745 registros\n",
            "2025-06-25 09:24:31 - INFO -    ‚úÖ Semana procesada: 7,212 registros\n",
            "2025-06-25 09:24:31 - INFO - üìä Consultando: bucket=PSDA, tables=['fixed_plant_atamo_1']\n",
            "2025-06-25 09:24:31 - INFO - üìÖ Per√≠odo: 2024-12-19T16:00:00Z a 2024-12-26T16:00:00Z\n",
            "2025-06-25 09:24:34 - INFO - ‚úÖ Datos obtenidos: 40215 registros\n",
            "2025-06-25 09:24:34 - INFO -    ‚úÖ Semana procesada: 8,370 registros\n",
            "2025-06-25 09:24:34 - INFO - üìä Consultando: bucket=PSDA, tables=['fixed_plant_atamo_1']\n",
            "2025-06-25 09:24:34 - INFO - üìÖ Per√≠odo: 2024-12-26T16:00:00Z a 2025-01-02T16:00:00Z\n",
            "2025-06-25 09:24:36 - INFO - ‚úÖ Datos obtenidos: 40299 registros\n",
            "2025-06-25 09:24:36 - INFO -    ‚úÖ Semana procesada: 8,400 registros\n",
            "2025-06-25 09:24:37 - INFO - üìä Consultando: bucket=PSDA, tables=['fixed_plant_atamo_1']\n",
            "2025-06-25 09:24:37 - INFO - üìÖ Per√≠odo: 2025-01-02T16:00:00Z a 2025-01-09T16:00:00Z\n",
            "2025-06-25 09:24:38 - INFO - ‚úÖ Datos obtenidos: 33864 registros\n",
            "2025-06-25 09:24:38 - INFO -    ‚úÖ Semana procesada: 6,523 registros\n",
            "2025-06-25 09:24:38 - INFO - üìä Consultando: bucket=PSDA, tables=['fixed_plant_atamo_1']\n",
            "2025-06-25 09:24:38 - INFO - üìÖ Per√≠odo: 2025-01-09T16:00:00Z a 2025-01-16T16:00:00Z\n",
            "2025-06-25 09:24:39 - INFO - ‚úÖ Datos obtenidos: 12316 registros\n",
            "2025-06-25 09:24:39 - INFO -    ‚úÖ Semana procesada: 3,114 registros\n",
            "2025-06-25 09:24:39 - INFO - üìä Consultando: bucket=PSDA, tables=['fixed_plant_atamo_1']\n",
            "2025-06-25 09:24:39 - INFO - üìÖ Per√≠odo: 2025-01-16T16:00:00Z a 2025-01-23T16:00:00Z\n",
            "2025-06-25 09:24:39 - INFO - ‚úÖ Datos obtenidos: 40163 registros\n",
            "2025-06-25 09:24:39 - INFO -    ‚úÖ Semana procesada: 8,300 registros\n",
            "2025-06-25 09:24:39 - INFO - üìä Consultando: bucket=PSDA, tables=['fixed_plant_atamo_1']\n",
            "2025-06-25 09:24:39 - INFO - üìÖ Per√≠odo: 2025-01-23T16:00:00Z a 2025-01-30T16:00:00Z\n",
            "2025-06-25 09:24:41 - INFO - ‚úÖ Datos obtenidos: 40073 registros\n",
            "2025-06-25 09:24:41 - INFO -    ‚úÖ Semana procesada: 8,385 registros\n",
            "2025-06-25 09:24:41 - INFO - üìä Consultando: bucket=PSDA, tables=['fixed_plant_atamo_1']\n",
            "2025-06-25 09:24:41 - INFO - üìÖ Per√≠odo: 2025-01-30T16:00:00Z a 2025-02-06T16:00:00Z\n",
            "2025-06-25 09:24:43 - INFO - ‚úÖ Datos obtenidos: 36238 registros\n",
            "2025-06-25 09:24:43 - INFO -    ‚úÖ Semana procesada: 7,241 registros\n",
            "2025-06-25 09:24:43 - INFO - üìä Consultando: bucket=PSDA, tables=['fixed_plant_atamo_1']\n",
            "2025-06-25 09:24:43 - INFO - üìÖ Per√≠odo: 2025-02-06T16:00:00Z a 2025-02-13T16:00:00Z\n",
            "2025-06-25 09:24:44 - INFO - ‚úÖ Datos obtenidos: 39820 registros\n",
            "2025-06-25 09:24:44 - INFO -    ‚úÖ Semana procesada: 8,115 registros\n",
            "2025-06-25 09:24:45 - INFO - üìä Consultando: bucket=PSDA, tables=['fixed_plant_atamo_1']\n",
            "2025-06-25 09:24:45 - INFO - üìÖ Per√≠odo: 2025-02-13T16:00:00Z a 2025-02-20T16:00:00Z\n",
            "2025-06-25 09:24:46 - INFO - ‚úÖ Datos obtenidos: 39469 registros\n",
            "2025-06-25 09:24:46 - INFO -    ‚úÖ Semana procesada: 7,917 registros\n",
            "2025-06-25 09:24:46 - INFO - üìä Consultando: bucket=PSDA, tables=['fixed_plant_atamo_1']\n",
            "2025-06-25 09:24:46 - INFO - üìÖ Per√≠odo: 2025-02-20T16:00:00Z a 2025-02-27T16:00:00Z\n",
            "2025-06-25 09:24:48 - INFO - ‚úÖ Datos obtenidos: 35371 registros\n",
            "2025-06-25 09:24:48 - INFO -    ‚úÖ Semana procesada: 7,969 registros\n",
            "2025-06-25 09:24:48 - INFO - üìä Consultando: bucket=PSDA, tables=['fixed_plant_atamo_1']\n",
            "2025-06-25 09:24:48 - INFO - üìÖ Per√≠odo: 2025-02-27T16:00:00Z a 2025-03-06T16:00:00Z\n",
            "2025-06-25 09:24:50 - INFO - ‚úÖ Datos obtenidos: 40086 registros\n",
            "2025-06-25 09:24:50 - INFO -    ‚úÖ Semana procesada: 8,273 registros\n",
            "2025-06-25 09:24:50 - INFO - üìä Consultando: bucket=PSDA, tables=['fixed_plant_atamo_1']\n",
            "2025-06-25 09:24:50 - INFO - üìÖ Per√≠odo: 2025-03-06T16:00:00Z a 2025-03-13T16:00:00Z\n",
            "2025-06-25 09:24:52 - INFO - ‚úÖ Datos obtenidos: 40161 registros\n",
            "2025-06-25 09:24:52 - INFO -    ‚úÖ Semana procesada: 8,293 registros\n",
            "2025-06-25 09:24:52 - INFO - üìä Consultando: bucket=PSDA, tables=['fixed_plant_atamo_1']\n",
            "2025-06-25 09:24:52 - INFO - üìÖ Per√≠odo: 2025-03-13T16:00:00Z a 2025-03-20T16:00:00Z\n",
            "2025-06-25 09:24:53 - INFO - ‚úÖ Datos obtenidos: 40084 registros\n",
            "2025-06-25 09:24:53 - INFO -    ‚úÖ Semana procesada: 8,192 registros\n",
            "2025-06-25 09:24:53 - INFO - üìä Consultando: bucket=PSDA, tables=['fixed_plant_atamo_1']\n",
            "2025-06-25 09:24:53 - INFO - üìÖ Per√≠odo: 2025-03-20T16:00:00Z a 2025-03-27T16:00:00Z\n",
            "2025-06-25 09:24:55 - INFO - ‚úÖ Datos obtenidos: 40152 registros\n",
            "2025-06-25 09:24:55 - INFO -    ‚úÖ Semana procesada: 8,341 registros\n",
            "2025-06-25 09:24:55 - INFO - üìä Consultando: bucket=PSDA, tables=['fixed_plant_atamo_1']\n",
            "2025-06-25 09:24:55 - INFO - üìÖ Per√≠odo: 2025-03-27T16:00:00Z a 2025-04-03T16:00:00Z\n",
            "2025-06-25 09:24:57 - INFO - ‚úÖ Datos obtenidos: 40193 registros\n",
            "2025-06-25 09:24:57 - INFO -    ‚úÖ Semana procesada: 8,345 registros\n",
            "2025-06-25 09:24:58 - INFO - üìä Consultando: bucket=PSDA, tables=['fixed_plant_atamo_1']\n",
            "2025-06-25 09:24:58 - INFO - üìÖ Per√≠odo: 2025-04-03T16:00:00Z a 2025-04-10T16:00:00Z\n",
            "2025-06-25 09:25:00 - INFO - ‚úÖ Datos obtenidos: 40192 registros\n",
            "2025-06-25 09:25:00 - INFO -    ‚úÖ Semana procesada: 8,372 registros\n",
            "2025-06-25 09:25:00 - INFO - üìä Consultando: bucket=PSDA, tables=['fixed_plant_atamo_1']\n",
            "2025-06-25 09:25:00 - INFO - üìÖ Per√≠odo: 2025-04-10T16:00:00Z a 2025-04-17T16:00:00Z\n",
            "2025-06-25 09:25:02 - INFO - ‚úÖ Datos obtenidos: 38909 registros\n",
            "2025-06-25 09:25:02 - INFO -    ‚úÖ Semana procesada: 7,488 registros\n",
            "2025-06-25 09:25:02 - INFO - üìä Consultando: bucket=PSDA, tables=['fixed_plant_atamo_1']\n",
            "2025-06-25 09:25:02 - INFO - üìÖ Per√≠odo: 2025-04-17T16:00:00Z a 2025-04-24T16:00:00Z\n",
            "2025-06-25 09:25:04 - INFO - ‚úÖ Datos obtenidos: 29848 registros\n",
            "2025-06-25 09:25:04 - INFO -    ‚úÖ Semana procesada: 6,907 registros\n",
            "2025-06-25 09:25:04 - INFO - üìä Consultando: bucket=PSDA, tables=['fixed_plant_atamo_1']\n",
            "2025-06-25 09:25:04 - INFO - üìÖ Per√≠odo: 2025-04-24T16:00:00Z a 2025-05-01T16:00:00Z\n",
            "2025-06-25 09:25:05 - INFO - ‚úÖ Datos obtenidos: 11416 registros\n",
            "2025-06-25 09:25:05 - INFO -    ‚úÖ Semana procesada: 2,877 registros\n",
            "2025-06-25 09:25:05 - INFO - üìä Consultando: bucket=PSDA, tables=['fixed_plant_atamo_1']\n",
            "2025-06-25 09:25:05 - INFO - üìÖ Per√≠odo: 2025-05-01T16:00:00Z a 2025-05-08T16:00:00Z\n",
            "2025-06-25 09:25:06 - WARNING - ‚ö†Ô∏è No se encontraron datos para el per√≠odo consultado\n",
            "2025-06-25 09:25:06 - INFO - üìä Consultando: bucket=PSDA, tables=['fixed_plant_atamo_1']\n",
            "2025-06-25 09:25:06 - INFO - üìÖ Per√≠odo: 2025-05-08T16:00:00Z a 2025-05-15T16:00:00Z\n",
            "2025-06-25 09:25:07 - INFO - ‚úÖ Datos obtenidos: 16931 registros\n",
            "2025-06-25 09:25:07 - INFO -    ‚úÖ Semana procesada: 3,263 registros\n",
            "2025-06-25 09:25:07 - INFO - üìä Consultando: bucket=PSDA, tables=['fixed_plant_atamo_1']\n",
            "2025-06-25 09:25:07 - INFO - üìÖ Per√≠odo: 2025-05-15T16:00:00Z a 2025-05-22T16:00:00Z\n",
            "2025-06-25 09:25:10 - INFO - ‚úÖ Datos obtenidos: 31547 registros\n",
            "2025-06-25 09:25:10 - INFO -    ‚úÖ Semana procesada: 6,561 registros\n",
            "2025-06-25 09:25:10 - INFO - üìä Consultando: bucket=PSDA, tables=['fixed_plant_atamo_1']\n",
            "2025-06-25 09:25:10 - INFO - üìÖ Per√≠odo: 2025-05-22T16:00:00Z a 2025-05-29T16:00:00Z\n",
            "2025-06-25 09:25:11 - INFO - ‚úÖ Datos obtenidos: 33825 registros\n",
            "2025-06-25 09:25:11 - INFO -    ‚úÖ Semana procesada: 6,570 registros\n",
            "2025-06-25 09:25:11 - INFO - üìä Consultando: bucket=PSDA, tables=['fixed_plant_atamo_1']\n",
            "2025-06-25 09:25:11 - INFO - üìÖ Per√≠odo: 2025-05-29T16:00:00Z a 2025-06-05T16:00:00Z\n",
            "2025-06-25 09:25:12 - INFO - ‚úÖ Datos obtenidos: 38795 registros\n",
            "2025-06-25 09:25:12 - INFO -    ‚úÖ Semana procesada: 7,604 registros\n",
            "2025-06-25 09:25:12 - INFO - üìä Consultando: bucket=PSDA, tables=['fixed_plant_atamo_1']\n",
            "2025-06-25 09:25:12 - INFO - üìÖ Per√≠odo: 2025-06-05T16:00:00Z a 2025-06-12T16:00:00Z\n",
            "2025-06-25 09:25:14 - INFO - ‚úÖ Datos obtenidos: 40031 registros\n",
            "2025-06-25 09:25:14 - INFO -    ‚úÖ Semana procesada: 8,335 registros\n",
            "2025-06-25 09:25:14 - INFO - üìä Consultando: bucket=PSDA, tables=['fixed_plant_atamo_1']\n",
            "2025-06-25 09:25:14 - INFO - üìÖ Per√≠odo: 2025-06-12T16:00:00Z a 2025-06-19T16:00:00Z\n",
            "2025-06-25 09:25:16 - INFO - ‚úÖ Datos obtenidos: 39385 registros\n",
            "2025-06-25 09:25:16 - INFO -    ‚úÖ Semana procesada: 7,587 registros\n",
            "2025-06-25 09:25:16 - INFO - üìä Consultando: bucket=PSDA, tables=['fixed_plant_atamo_1']\n",
            "2025-06-25 09:25:16 - INFO - üìÖ Per√≠odo: 2025-06-19T16:00:00Z a 2025-06-26T16:00:00Z\n",
            "2025-06-25 09:25:17 - INFO - ‚úÖ Datos obtenidos: 33875 registros\n",
            "2025-06-25 09:25:17 - INFO -    ‚úÖ Semana procesada: 6,584 registros\n",
            "2025-06-25 09:25:17 - INFO - üìä Consultando: bucket=PSDA, tables=['fixed_plant_atamo_1']\n",
            "2025-06-25 09:25:17 - INFO - üìÖ Per√≠odo: 2025-06-26T16:00:00Z a 2025-06-30T00:00:00Z\n",
            "2025-06-25 09:25:18 - WARNING - ‚ö†Ô∏è No se encontraron datos para el per√≠odo consultado\n",
            "2025-06-25 09:25:18 - INFO - \n",
            "üìä COMBINANDO Y PROCESANDO TODOS LOS DATOS...\n",
            "2025-06-25 09:25:18 - INFO - üìä Datos antes de correcci√≥n: 747,280 registros\n",
            "2025-06-25 09:25:18 - INFO - üîß Aplicando correcci√≥n de formato...\n",
            "2025-06-25 09:25:18 - INFO - üìä Datos correctos: 554,220 filas\n",
            "2025-06-25 09:25:18 - INFO - üîß Datos deformados: 193,060 filas\n",
            "2025-06-25 09:25:18 - INFO - üîÑ Consolidando datos deformados...\n",
            "2025-06-25 09:26:01 - INFO - ‚úÖ Datos consolidados: 48,035 filas\n",
            "2025-06-25 09:26:01 - INFO - üìä Total final: 602,255 filas\n",
            "2025-06-25 09:26:01 - INFO - üíæ Guardando archivo final: /home/nicole/SR/SOILING/datos/data_temp.csv\n",
            "2025-06-25 09:26:11 - INFO - \n",
            "======================================================================\n",
            "2025-06-25 09:26:11 - INFO - ‚úÖ DESCARGA COMPLETADA EXITOSAMENTE\n",
            "2025-06-25 09:26:11 - INFO - üìä Registros finales: 602,255\n",
            "2025-06-25 09:26:11 - INFO - üìÖ Rango: 2024-07-01 13:00:02+00:00 a 2025-06-25 13:25:00+00:00\n",
            "2025-06-25 09:26:11 - INFO - üìÅ Archivo: data_temp.csv (27.15 MB)\n",
            "2025-06-25 09:26:11 - INFO - üå°Ô∏è 1TE416(C): 602,255 valores (100.0%)\n",
            "2025-06-25 09:26:11 - INFO - üå°Ô∏è 1TE417(C): 602,255 valores (100.0%)\n",
            "2025-06-25 09:26:11 - INFO - üå°Ô∏è 1TE418(C): 602,255 valores (100.0%)\n",
            "2025-06-25 09:26:11 - INFO - üå°Ô∏è 1TE419(C): 602,255 valores (100.0%)\n",
            "2025-06-25 09:26:11 - INFO - ======================================================================\n",
            "2025-06-25 09:26:11 - INFO - \n",
            "üéâ ¬°PROCESO COMPLETADO EXITOSAMENTE!\n",
            "2025-06-25 09:26:11 - INFO - üìã El archivo data_temp.csv est√° disponible y listo para usar\n",
            "2025-06-25 09:26:11 - INFO - üßπ Formato corregido autom√°ticamente\n",
            "2025-06-25 09:26:11 - INFO - üîå Conexi√≥n a InfluxDB cerrada\n",
            "2025-06-25 09:26:11 - INFO - \n",
            "üèÅ PROCESO FINALIZADO\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# üöÄ EJECUTAR DESCARGA COMPLETA\n",
        "# ============================================================================\n",
        "\n",
        "logger.info(\"\\nüöÄ INICIANDO DESCARGA COMPLETA DE DATOS DE TEMPERATURA\")\n",
        "logger.info(\"=\"*80)\n",
        "\n",
        "# Inicializar cliente InfluxDB\n",
        "influx_manager = InfluxDBManager(INFLUX_CONFIG)\n",
        "\n",
        "try:\n",
        "    if not influx_manager.connect():\n",
        "        logger.error(\"‚ùå No se pudo establecer conexi√≥n con InfluxDB\")\n",
        "    else:\n",
        "        # Ejecutar descarga completa\n",
        "        success = download_temperature_data(influx_manager, START_DATE, END_DATE, OUTPUT_DIR)\n",
        "        \n",
        "        if success:\n",
        "            logger.info(\"\\nüéâ ¬°PROCESO COMPLETADO EXITOSAMENTE!\")\n",
        "            logger.info(\"üìã El archivo data_temp.csv est√° disponible y listo para usar\")\n",
        "            logger.info(\"üßπ Formato corregido autom√°ticamente\")\n",
        "        else:\n",
        "            logger.error(\"‚ùå ERROR EN EL PROCESO DE DESCARGA\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"‚ùå Error general: {e}\")\n",
        "    import traceback\n",
        "    logger.error(f\"üîç Detalles del error: {traceback.format_exc()}\")\n",
        "    \n",
        "finally:\n",
        "    influx_manager.disconnect()\n",
        "\n",
        "logger.info(\"\\nüèÅ PROCESO FINALIZADO\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-25 09:27:38 - INFO - ‚úÖ Funci√≥n de limpieza definida (ejecutar manualmente si es necesario)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# üßπ LIMPIEZA DE ARCHIVOS TEMPORALES (OPCIONAL)\n",
        "# ============================================================================\n",
        "\n",
        "def cleanup_temp_files():\n",
        "    \"\"\"\n",
        "    Elimina archivos temporales que ya no son necesarios.\n",
        "    Solo ejecutar si se confirma que data_temp.csv est√° correcto.\n",
        "    \"\"\"\n",
        "    logger.info(\"üßπ Iniciando limpieza de archivos temporales...\")\n",
        "    \n",
        "    temp_files_to_remove = [\n",
        "        'temp_historical_TempModFixed.csv',\n",
        "        'temp_recent_fixed_plant_atamo_1.csv',\n",
        "        'temp_mod_fixed_data.csv',\n",
        "        'temp_mod_fixed_data_backup.csv',\n",
        "        'temp_mod_fixed_data_backup_improved.csv',\n",
        "        'test_recent_temp.csv'\n",
        "    ]\n",
        "    \n",
        "    files_removed = 0\n",
        "    \n",
        "    for temp_file in temp_files_to_remove:\n",
        "        file_path = os.path.join(OUTPUT_DIR, temp_file)\n",
        "        \n",
        "        if os.path.exists(file_path):\n",
        "            try:\n",
        "                file_size_mb = os.path.getsize(file_path) / (1024*1024)\n",
        "                os.remove(file_path)\n",
        "                logger.info(f\"   ‚úÖ Eliminado: {temp_file} ({file_size_mb:.2f} MB)\")\n",
        "                files_removed += 1\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"   ‚ö†Ô∏è No se pudo eliminar {temp_file}: {e}\")\n",
        "        else:\n",
        "            logger.info(f\"   ‚ûñ No existe: {temp_file}\")\n",
        "    \n",
        "    logger.info(f\"\\nüéâ Limpieza completada: {files_removed} archivos eliminados\")\n",
        "    \n",
        "    # Verificar archivo final\n",
        "    final_file = os.path.join(OUTPUT_DIR, 'data_temp.csv')\n",
        "    if os.path.exists(final_file):\n",
        "        file_size_mb = os.path.getsize(final_file) / (1024*1024)\n",
        "        logger.info(f\"‚úÖ Archivo final disponible: data_temp.csv ({file_size_mb:.2f} MB)\")\n",
        "    else:\n",
        "        logger.error(\"‚ùå ADVERTENCIA: No se encuentra data_temp.csv\")\n",
        "\n",
        "# Comentar la siguiente l√≠nea si NO quieres ejecutar la limpieza autom√°ticamente\n",
        "# cleanup_temp_files()\n",
        "\n",
        "logger.info(\"‚úÖ Funci√≥n de limpieza definida (ejecutar manualmente si es necesario)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-25 09:27:45 - INFO - \\nüîß INICIANDO CORRECCI√ìN DEL ARCHIVO EXISTENTE\n",
            "2025-06-25 09:27:45 - INFO - ================================================================================\n",
            "2025-06-25 09:27:45 - INFO - üîß Corrigiendo archivo data_temp.csv existente...\n",
            "2025-06-25 09:27:45 - INFO - ======================================================================\n",
            "2025-06-25 09:27:45 - INFO - üìÅ Archivo original: 27.15 MB\n",
            "2025-06-25 09:27:45 - INFO - üìñ Leyendo archivo...\n",
            "2025-06-25 09:27:53 - INFO - üìä Registros originales: 602,255\n",
            "2025-06-25 09:27:53 - INFO - üìÖ Rango: 2024-07-01 13:00:02+00:00 a 2025-06-25 13:25:00+00:00\n",
            "2025-06-25 09:27:53 - INFO - \\nüìä ESTADO ANTES DE LA CORRECCI√ìN:\n",
            "2025-06-25 09:27:53 - INFO - üå°Ô∏è 1TE416(C): 602,255 valores (100.0%)\n",
            "2025-06-25 09:27:53 - INFO - üå°Ô∏è 1TE417(C): 602,255 valores (100.0%)\n",
            "2025-06-25 09:27:53 - INFO - üå°Ô∏è 1TE418(C): 602,255 valores (100.0%)\n",
            "2025-06-25 09:27:53 - INFO - üå°Ô∏è 1TE419(C): 602,255 valores (100.0%)\n",
            "2025-06-25 09:27:53 - INFO - \\nüíæ Creando backup: /home/nicole/SR/SOILING/datos/data_temp_backup_before_fix.csv\n",
            "2025-06-25 09:28:06 - INFO - \\nüîß Aplicando correcci√≥n de formato...\n",
            "2025-06-25 09:28:06 - INFO - üîß Aplicando correcci√≥n de formato...\n",
            "2025-06-25 09:28:06 - INFO - üìä Datos correctos: 602,255 filas\n",
            "2025-06-25 09:28:06 - INFO - üîß Datos deformados: 0 filas\n",
            "2025-06-25 09:28:06 - INFO - ‚úÖ No hay datos deformados que corregir\n",
            "2025-06-25 09:28:06 - INFO - üíæ Guardando archivo corregido...\n",
            "2025-06-25 09:28:33 - INFO - \\n======================================================================\n",
            "2025-06-25 09:28:33 - INFO - ‚úÖ CORRECCI√ìN COMPLETADA\n",
            "2025-06-25 09:28:33 - INFO - üìä Registros finales: 602,255\n",
            "2025-06-25 09:28:33 - INFO - üìÅ Tama√±o final: 27.15 MB\n",
            "2025-06-25 09:28:33 - INFO - üìÖ Rango final: 2024-07-01 13:00:02+00:00 a 2025-06-25 13:25:00+00:00\n",
            "2025-06-25 09:28:33 - INFO - \\nüìä ESTADO DESPU√âS DE LA CORRECCI√ìN:\n",
            "2025-06-25 09:28:33 - INFO - üå°Ô∏è 1TE416(C): 602,255 valores (100.0%)\n",
            "2025-06-25 09:28:33 - INFO - üå°Ô∏è 1TE417(C): 602,255 valores (100.0%)\n",
            "2025-06-25 09:28:33 - INFO - üå°Ô∏è 1TE418(C): 602,255 valores (100.0%)\n",
            "2025-06-25 09:28:33 - INFO - üå°Ô∏è 1TE419(C): 602,255 valores (100.0%)\n",
            "2025-06-25 09:28:33 - INFO - \\nüíæ Backup guardado en: /home/nicole/SR/SOILING/datos/data_temp_backup_before_fix.csv\n",
            "2025-06-25 09:28:33 - INFO - ======================================================================\n",
            "2025-06-25 09:28:33 - INFO - \\nüéâ ¬°ARCHIVO CORREGIDO EXITOSAMENTE!\n",
            "2025-06-25 09:28:33 - INFO - üìã El archivo data_temp.csv ahora tiene formato consistente\n",
            "2025-06-25 09:28:33 - INFO - üßπ Se cre√≥ un backup del archivo original por seguridad\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# üîß CORRECCI√ìN DEL ARCHIVO data_temp.csv EXISTENTE\n",
        "# ============================================================================\n",
        "\n",
        "def fix_existing_data_temp_file():\n",
        "    \"\"\"\n",
        "    Corrige el archivo data_temp.csv existente aplicando la correcci√≥n de formato.\n",
        "    \"\"\"\n",
        "    logger.info(\"üîß Corrigiendo archivo data_temp.csv existente...\")\n",
        "    logger.info(\"=\"*70)\n",
        "    \n",
        "    file_path = os.path.join(OUTPUT_DIR, 'data_temp.csv')\n",
        "    \n",
        "    if not os.path.exists(file_path):\n",
        "        logger.error(f\"‚ùå Archivo no encontrado: {file_path}\")\n",
        "        return False\n",
        "    \n",
        "    try:\n",
        "        # Mostrar informaci√≥n del archivo original\n",
        "        file_size_mb = os.path.getsize(file_path) / (1024*1024)\n",
        "        logger.info(f\"üìÅ Archivo original: {file_size_mb:.2f} MB\")\n",
        "        \n",
        "        # Leer archivo\n",
        "        logger.info(\"üìñ Leyendo archivo...\")\n",
        "        df = pd.read_csv(file_path, index_col='_time', parse_dates=['_time'])\n",
        "        \n",
        "        logger.info(f\"üìä Registros originales: {len(df):,}\")\n",
        "        logger.info(f\"üìÖ Rango: {df.index.min()} a {df.index.max()}\")\n",
        "        \n",
        "        # Verificar estado actual (porcentaje de valores v√°lidos por sensor)\n",
        "        temp_cols = ['1TE416(C)', '1TE417(C)', '1TE418(C)', '1TE419(C)']\n",
        "        logger.info(f\"\\\\nüìä ESTADO ANTES DE LA CORRECCI√ìN:\")\n",
        "        for col in temp_cols:\n",
        "            if col in df.columns:\n",
        "                valid_count = df[col].notna().sum()\n",
        "                percentage = valid_count/len(df)*100\n",
        "                logger.info(f\"üå°Ô∏è {col}: {valid_count:,} valores ({percentage:.1f}%)\")\n",
        "        \n",
        "        # Crear backup del archivo original\n",
        "        backup_file = file_path.replace('.csv', '_backup_before_fix.csv')\n",
        "        logger.info(f\"\\\\nüíæ Creando backup: {backup_file}\")\n",
        "        df.to_csv(backup_file)\n",
        "        \n",
        "        # Aplicar correcci√≥n de formato\n",
        "        logger.info(f\"\\\\nüîß Aplicando correcci√≥n de formato...\")\n",
        "        df_corrected = fix_temperature_format(df)\n",
        "        \n",
        "        # Guardar archivo corregido\n",
        "        logger.info(f\"üíæ Guardando archivo corregido...\")\n",
        "        df_corrected.to_csv(file_path)\n",
        "        \n",
        "        # Mostrar estad√≠sticas finales\n",
        "        new_file_size_mb = os.path.getsize(file_path) / (1024*1024)\n",
        "        \n",
        "        logger.info(f\"\\\\n\" + \"=\"*70)\n",
        "        logger.info(f\"‚úÖ CORRECCI√ìN COMPLETADA\")\n",
        "        logger.info(f\"üìä Registros finales: {len(df_corrected):,}\")\n",
        "        logger.info(f\"üìÅ Tama√±o final: {new_file_size_mb:.2f} MB\")\n",
        "        logger.info(f\"üìÖ Rango final: {df_corrected.index.min()} a {df_corrected.index.max()}\")\n",
        "        \n",
        "        # Estad√≠sticas finales por sensor\n",
        "        logger.info(f\"\\\\nüìä ESTADO DESPU√âS DE LA CORRECCI√ìN:\")\n",
        "        for col in temp_cols:\n",
        "            if col in df_corrected.columns:\n",
        "                valid_count = df_corrected[col].notna().sum()\n",
        "                percentage = valid_count/len(df_corrected)*100\n",
        "                logger.info(f\"üå°Ô∏è {col}: {valid_count:,} valores ({percentage:.1f}%)\")\n",
        "        \n",
        "        logger.info(f\"\\\\nüíæ Backup guardado en: {backup_file}\")\n",
        "        logger.info(\"=\"*70)\n",
        "        \n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error en la correcci√≥n: {e}\")\n",
        "        import traceback\n",
        "        logger.error(f\"üîç Detalles: {traceback.format_exc()}\")\n",
        "        return False\n",
        "\n",
        "# Ejecutar correcci√≥n del archivo existente\n",
        "logger.info(\"\\\\nüîß INICIANDO CORRECCI√ìN DEL ARCHIVO EXISTENTE\")\n",
        "logger.info(\"=\"*80)\n",
        "\n",
        "success = fix_existing_data_temp_file()\n",
        "\n",
        "if success:\n",
        "    logger.info(\"\\\\nüéâ ¬°ARCHIVO CORREGIDO EXITOSAMENTE!\")\n",
        "    logger.info(\"üìã El archivo data_temp.csv ahora tiene formato consistente\")\n",
        "    logger.info(\"üßπ Se cre√≥ un backup del archivo original por seguridad\")\n",
        "else:\n",
        "    logger.error(\"\\\\n‚ùå ERROR EN LA CORRECCI√ìN\")\n",
        "    logger.error(\"üîç Revisa los logs anteriores para m√°s detalles\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-25 09:30:29 - INFO - \\nüî¢ INICIANDO CORRECCI√ìN DE CIFRAS SIGNIFICATIVAS\n",
            "2025-06-25 09:30:29 - INFO - ================================================================================\n",
            "2025-06-25 09:30:29 - INFO - üî¢ Aplicando correcci√≥n de cifras significativas...\n",
            "2025-06-25 09:30:29 - INFO - ======================================================================\n",
            "2025-06-25 09:30:29 - INFO - üìñ Leyendo archivo...\n",
            "2025-06-25 09:30:36 - INFO - üìä Procesando 602,229 registros...\n",
            "2025-06-25 09:30:36 - INFO - üíæ Creando backup: /home/nicole/SR/SOILING/datos/data_temp_backup_before_precision.csv\n",
            "2025-06-25 09:30:46 - INFO - üî¢ Normalizando precisi√≥n a 1 decimal(es)...\n",
            "2025-06-25 09:30:46 - INFO - üìä PRECISI√ìN ANTES DEL REDONDEO:\n",
            "2025-06-25 09:30:46 - INFO - üå°Ô∏è 1TE416(C): Min=0.000000, Max=76.700000, Media=56.268454\n",
            "2025-06-25 09:30:47 - INFO - üå°Ô∏è 1TE417(C): Min=0.000000, Max=75.900000, Media=53.852447\n",
            "2025-06-25 09:30:47 - INFO - üå°Ô∏è 1TE418(C): Min=0.000000, Max=76.100000, Media=54.342333\n",
            "2025-06-25 09:30:47 - INFO - üå°Ô∏è 1TE419(C): Min=0.000000, Max=75.600000, Media=54.123413\n",
            "2025-06-25 09:30:47 - INFO - ‚úÖ 1TE416(C): 602229 ‚Üí 602229 valores v√°lidos\n",
            "2025-06-25 09:30:47 - INFO - ‚úÖ 1TE417(C): 602229 ‚Üí 602229 valores v√°lidos\n",
            "2025-06-25 09:30:47 - INFO - ‚úÖ 1TE418(C): 602229 ‚Üí 602229 valores v√°lidos\n",
            "2025-06-25 09:30:47 - INFO - ‚úÖ 1TE419(C): 602229 ‚Üí 602229 valores v√°lidos\n",
            "2025-06-25 09:30:47 - INFO - \\nüìä PRECISI√ìN DESPU√âS DEL REDONDEO:\n",
            "2025-06-25 09:30:47 - INFO - üå°Ô∏è 1TE416(C): Min=0.0, Max=76.7, Media=56.3\n",
            "2025-06-25 09:30:47 - INFO - üå°Ô∏è 1TE417(C): Min=0.0, Max=75.9, Media=53.9\n",
            "2025-06-25 09:30:47 - INFO - üå°Ô∏è 1TE418(C): Min=0.0, Max=76.1, Media=54.3\n",
            "2025-06-25 09:30:47 - INFO - üå°Ô∏è 1TE419(C): Min=0.0, Max=75.6, Media=54.1\n",
            "2025-06-25 09:30:47 - INFO - ‚úÖ Precisi√≥n normalizada a 1 decimal(es)\n",
            "2025-06-25 09:30:47 - INFO - üíæ Guardando archivo con precisi√≥n corregida...\n",
            "2025-06-25 09:31:10 - INFO - \\n======================================================================\n",
            "2025-06-25 09:31:10 - INFO - ‚úÖ CORRECCI√ìN DE PRECISI√ìN COMPLETADA\n",
            "2025-06-25 09:31:10 - INFO - üìÅ Tama√±o original: 27.15 MB\n",
            "2025-06-25 09:31:10 - INFO - üìÅ Tama√±o nuevo: 26.42 MB\n",
            "2025-06-25 09:31:10 - INFO - üìâ Reducci√≥n: 2.7%\n",
            "2025-06-25 09:31:10 - INFO - üíæ Backup guardado en: /home/nicole/SR/SOILING/datos/data_temp_backup_before_precision.csv\n",
            "2025-06-25 09:31:10 - INFO - ======================================================================\n",
            "2025-06-25 09:31:10 - INFO - \\nüéâ ¬°CIFRAS SIGNIFICATIVAS CORREGIDAS EXITOSAMENTE!\n",
            "2025-06-25 09:31:10 - INFO - üìã Los datos de temperatura ahora tienen precisi√≥n consistente (1 decimal)\n",
            "2025-06-25 09:31:10 - INFO - üìâ Tama√±o del archivo reducido manteniendo calidad de datos\n",
            "2025-06-25 09:31:10 - INFO - üíæ Se cre√≥ backup del archivo original por seguridad\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# üî¢ CORRECCI√ìN DE CIFRAS SIGNIFICATIVAS EN DATOS DE TEMPERATURA\n",
        "# ============================================================================\n",
        "\n",
        "def normalize_temperature_precision(df, decimal_places=1):\n",
        "    \"\"\"\n",
        "    Normaliza la precisi√≥n de los datos de temperatura a un n√∫mero espec√≠fico de decimales.\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame con datos de temperatura\n",
        "        decimal_places: N√∫mero de decimales a mantener (default: 1)\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame con valores redondeados\n",
        "    \"\"\"\n",
        "    logger.info(f\"üî¢ Normalizando precisi√≥n a {decimal_places} decimal(es)...\")\n",
        "    \n",
        "    try:\n",
        "        temp_cols = ['1TE416(C)', '1TE417(C)', '1TE418(C)', '1TE419(C)']\n",
        "        df_normalized = df.copy()\n",
        "        \n",
        "        # Mostrar estad√≠sticas antes del redondeo\n",
        "        logger.info(\"üìä PRECISI√ìN ANTES DEL REDONDEO:\")\n",
        "        for col in temp_cols:\n",
        "            if col in df_normalized.columns:\n",
        "                valid_data = df_normalized[col].dropna()\n",
        "                if not valid_data.empty:\n",
        "                    min_val = valid_data.min()\n",
        "                    max_val = valid_data.max()\n",
        "                    mean_val = valid_data.mean()\n",
        "                    logger.info(f\"üå°Ô∏è {col}: Min={min_val:.6f}, Max={max_val:.6f}, Media={mean_val:.6f}\")\n",
        "        \n",
        "        # Redondear valores de temperatura\n",
        "        for col in temp_cols:\n",
        "            if col in df_normalized.columns:\n",
        "                # Contar valores antes del redondeo\n",
        "                valid_before = df_normalized[col].notna().sum()\n",
        "                \n",
        "                # Aplicar redondeo\n",
        "                df_normalized[col] = df_normalized[col].round(decimal_places)\n",
        "                \n",
        "                # Verificar que no se perdieron valores\n",
        "                valid_after = df_normalized[col].notna().sum()\n",
        "                logger.info(f\"‚úÖ {col}: {valid_before} ‚Üí {valid_after} valores v√°lidos\")\n",
        "        \n",
        "        # Mostrar estad√≠sticas despu√©s del redondeo\n",
        "        logger.info(f\"\\\\nüìä PRECISI√ìN DESPU√âS DEL REDONDEO:\")\n",
        "        for col in temp_cols:\n",
        "            if col in df_normalized.columns:\n",
        "                valid_data = df_normalized[col].dropna()\n",
        "                if not valid_data.empty:\n",
        "                    min_val = valid_data.min()\n",
        "                    max_val = valid_data.max()\n",
        "                    mean_val = valid_data.mean()\n",
        "                    logger.info(f\"üå°Ô∏è {col}: Min={min_val:.{decimal_places}f}, Max={max_val:.{decimal_places}f}, Media={mean_val:.{decimal_places}f}\")\n",
        "        \n",
        "        logger.info(f\"‚úÖ Precisi√≥n normalizada a {decimal_places} decimal(es)\")\n",
        "        return df_normalized\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error en normalizaci√≥n de precisi√≥n: {e}\")\n",
        "        return df\n",
        "\n",
        "def apply_precision_correction_to_file():\n",
        "    \"\"\"\n",
        "    Aplica correcci√≥n de cifras significativas al archivo data_temp.csv existente.\n",
        "    \"\"\"\n",
        "    logger.info(\"üî¢ Aplicando correcci√≥n de cifras significativas...\")\n",
        "    logger.info(\"=\"*70)\n",
        "    \n",
        "    file_path = os.path.join(OUTPUT_DIR, 'data_temp.csv')\n",
        "    \n",
        "    if not os.path.exists(file_path):\n",
        "        logger.error(f\"‚ùå Archivo no encontrado: {file_path}\")\n",
        "        return False\n",
        "    \n",
        "    try:\n",
        "        # Leer archivo\n",
        "        logger.info(\"üìñ Leyendo archivo...\")\n",
        "        df = pd.read_csv(file_path, index_col='_time', parse_dates=['_time'])\n",
        "        \n",
        "        logger.info(f\"üìä Procesando {len(df):,} registros...\")\n",
        "        \n",
        "        # Crear backup antes de la correcci√≥n de precisi√≥n\n",
        "        backup_file = file_path.replace('.csv', '_backup_before_precision.csv')\n",
        "        logger.info(f\"üíæ Creando backup: {backup_file}\")\n",
        "        df.to_csv(backup_file)\n",
        "        \n",
        "        # Aplicar normalizaci√≥n de precisi√≥n (1 decimal para temperaturas)\n",
        "        df_normalized = normalize_temperature_precision(df, decimal_places=1)\n",
        "        \n",
        "        # Guardar archivo con precisi√≥n normalizada\n",
        "        logger.info(f\"üíæ Guardando archivo con precisi√≥n corregida...\")\n",
        "        df_normalized.to_csv(file_path)\n",
        "        \n",
        "        # Verificar reducci√≥n de tama√±o del archivo\n",
        "        original_size = os.path.getsize(backup_file) / (1024*1024)\n",
        "        new_size = os.path.getsize(file_path) / (1024*1024)\n",
        "        reduction = ((original_size - new_size) / original_size) * 100\n",
        "        \n",
        "        logger.info(f\"\\\\n\" + \"=\"*70)\n",
        "        logger.info(f\"‚úÖ CORRECCI√ìN DE PRECISI√ìN COMPLETADA\")\n",
        "        logger.info(f\"üìÅ Tama√±o original: {original_size:.2f} MB\")\n",
        "        logger.info(f\"üìÅ Tama√±o nuevo: {new_size:.2f} MB\")\n",
        "        logger.info(f\"üìâ Reducci√≥n: {reduction:.1f}%\")\n",
        "        logger.info(f\"üíæ Backup guardado en: {backup_file}\")\n",
        "        logger.info(\"=\"*70)\n",
        "        \n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error en correcci√≥n de precisi√≥n: {e}\")\n",
        "        import traceback\n",
        "        logger.error(f\"üîç Detalles: {traceback.format_exc()}\")\n",
        "        return False\n",
        "\n",
        "# Ejecutar correcci√≥n de cifras significativas\n",
        "logger.info(\"\\\\nüî¢ INICIANDO CORRECCI√ìN DE CIFRAS SIGNIFICATIVAS\")\n",
        "logger.info(\"=\"*80)\n",
        "\n",
        "success = apply_precision_correction_to_file()\n",
        "\n",
        "if success:\n",
        "    logger.info(\"\\\\nüéâ ¬°CIFRAS SIGNIFICATIVAS CORREGIDAS EXITOSAMENTE!\")\n",
        "    logger.info(\"üìã Los datos de temperatura ahora tienen precisi√≥n consistente (1 decimal)\")\n",
        "    logger.info(\"üìâ Tama√±o del archivo reducido manteniendo calidad de datos\")\n",
        "    logger.info(\"üíæ Se cre√≥ backup del archivo original por seguridad\")\n",
        "else:\n",
        "    logger.error(\"\\\\n‚ùå ERROR EN LA CORRECCI√ìN DE PRECISI√ìN\")\n",
        "    logger.error(\"üîç Revisa los logs anteriores para m√°s detalles\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
