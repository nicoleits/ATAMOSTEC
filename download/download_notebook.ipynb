{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 12:12:12 - INFO - Rango de fechas: 2024-07-01 a 2025-12-31\n",
      "2025-08-07 12:12:12 - INFO - Directorio de salida: /home/nicole/SR/SOILING/datos\n"
     ]
    }
   ],
   "source": [
    "# Importar librerías necesarias - Config. InfluxDB y Clickhouse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import clickhouse_connect\n",
    "from influxdb_client import InfluxDBClient\n",
    "import gc\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuración de InfluxDB\n",
    "INFLUX_CONFIG = {\n",
    "    'url': \"http://146.83.153.212:27017\", #\"http://172.24.61.95:27017\"\n",
    "    'token': \"piDbFR_bfRWO5Epu1IS96WbkNpSZZCYgwZZR29PcwUsxXwKdIyLMhVAhU4-5ohWeXIsX7Dp_X-WiPIDx0beafg==\",\n",
    "    'org': \"atamostec\",\n",
    "    'timeout': 300000\n",
    "}\n",
    "\n",
    "# Configuración de Clickhouse\n",
    "CLICKHOUSE_CONFIG = {\n",
    "    'host': \"146.83.153.212\", #\"172.24.61.95\"\n",
    "    'port': \"30091\",\n",
    "    'user': \"default\",\n",
    "    'password': \"Psda2020\"\n",
    "}\n",
    "\n",
    "# Configuración de fechas\n",
    "START_DATE = pd.to_datetime('01/07/2024', dayfirst=True).tz_localize('UTC')\n",
    "END_DATE = pd.to_datetime('31/12/2025', dayfirst=True).tz_localize('UTC')\n",
    "\n",
    "# Directorio de salida - ruta absoluta desde el notebook en download/\n",
    "OUTPUT_DIR = \"/home/nicole/SR/SOILING/datos\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Mostrar configuración\n",
    "logger.info(f\"Rango de fechas: {START_DATE.strftime('%Y-%m-%d')} a {END_DATE.strftime('%Y-%m-%d')}\")\n",
    "logger.info(f\"Directorio de salida: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfluxDBManager:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.client = None\n",
    "        self.query_api = None\n",
    "        \n",
    "    def connect(self):\n",
    "        try:\n",
    "            self.client = InfluxDBClient(\n",
    "                url=self.config['url'],\n",
    "                token=self.config['token'],\n",
    "                org=self.config['org'],\n",
    "                timeout=self.config['timeout']\n",
    "            )\n",
    "            self.query_api = self.client.query_api()\n",
    "            logger.info(\"Cliente InfluxDB y query_api inicializados.\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error al conectar con InfluxDB: {e}\")\n",
    "            return False\n",
    "            \n",
    "    def disconnect(self):\n",
    "        if self.client:\n",
    "            self.client.close()\n",
    "            logger.info(\"Conexión a InfluxDB cerrada.\")\n",
    "            \n",
    "    def query_influxdb(self, bucket, tables, attributes, start_date, stop_date):\n",
    "        try:\n",
    "            # Convertir fechas al formato correcto para InfluxDB\n",
    "            start_str = start_date.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "            stop_str = stop_date.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "            \n",
    "            # Construir la lista de atributos en formato correcto\n",
    "            attributes_str = \" or \".join([f'r[\"_field\"] == \"{attr}\"' for attr in attributes])\n",
    "\n",
    "            query = f'''\n",
    "            from(bucket: \"{bucket}\")\n",
    "                |> range(start: {start_str}, stop: {stop_str})\n",
    "                |> filter(fn: (r) => {\" or \".join([f'r[\"_measurement\"] == \"{table}\"' for table in tables])})\n",
    "                |> filter(fn: (r) => {attributes_str})\n",
    "                |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n",
    "            '''\n",
    "            \n",
    "            logger.info(f\"Consultando InfluxDB: bucket={bucket}, tables={tables}, attributes={attributes}\")\n",
    "            \n",
    "            result = self.query_api.query_data_frame(query)\n",
    "            \n",
    "            if result.empty:\n",
    "                logger.warning(\"No se encontraron datos en la consulta.\")\n",
    "                return None\n",
    "                \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error en la consulta a InfluxDB: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔋 DESCARGA IV600 \n",
    "def download_iv600(start_date, end_date, output_dir):\n",
    "    \"\"\"Descarga y procesa datos de IV600 desde Clickhouse.\"\"\"\n",
    "    logger.info(\"Iniciando descarga de datos IV600...\")\n",
    "    client = None\n",
    "    \n",
    "    try:\n",
    "        # Conectar a Clickhouse\n",
    "        logger.info(\"Conectando a Clickhouse...\")\n",
    "        client = clickhouse_connect.get_client(\n",
    "            host=CLICKHOUSE_CONFIG['host'],\n",
    "            port=CLICKHOUSE_CONFIG['port'],\n",
    "            username=CLICKHOUSE_CONFIG['user'],\n",
    "            password=CLICKHOUSE_CONFIG['password']\n",
    "        )\n",
    "        logger.info(\"Conexión a Clickhouse establecida\")\n",
    "        \n",
    "        # Consultar datos\n",
    "        logger.info(\"Consultando datos IV600...\")\n",
    "        query = \"SELECT * FROM ref_data.iv_curves_trazador_manual\"\n",
    "        data_iv_curves = client.query(query)\n",
    "        logger.info(f\"Datos obtenidos: {len(data_iv_curves.result_set)} registros\")\n",
    "        \n",
    "        # Procesar datos\n",
    "        logger.info(\"Procesando datos...\")\n",
    "        curves_list = []\n",
    "        for curve in data_iv_curves.result_set:\n",
    "            currents = curve[4]\n",
    "            voltages = curve[3]\n",
    "            powers = [currents[i] * voltages[i] for i in range(len(currents))]\n",
    "            timestamp = curve[0]\n",
    "            module = curve[2]\n",
    "            pmp = max(powers)\n",
    "            isc = max(currents)\n",
    "            voc = max(voltages)\n",
    "            imp = currents[np.argmax(powers)]\n",
    "            vmp = voltages[np.argmax(powers)]\n",
    "            curves_list.append([timestamp, module, pmp, isc, voc, imp, vmp])\n",
    "\n",
    "        # Crear DataFrame\n",
    "        logger.info(\"Creando DataFrame...\")\n",
    "        column_names = [\"timestamp\", \"module\", \"pmp\", \"isc\", \"voc\", \"imp\", \"vmp\"]\n",
    "        df_curves = pd.DataFrame(curves_list, columns=column_names)\n",
    "        \n",
    "        # Convertir timestamp a datetime y asegurar que esté en UTC\n",
    "        df_curves['timestamp'] = pd.to_datetime(df_curves['timestamp'])\n",
    "        if df_curves['timestamp'].dt.tz is None:\n",
    "            df_curves['timestamp'] = df_curves['timestamp'].dt.tz_localize('UTC')\n",
    "        else:\n",
    "            df_curves['timestamp'] = df_curves['timestamp'].dt.tz_convert('UTC')\n",
    "        \n",
    "        df_curves.set_index('timestamp', inplace=True)\n",
    "        \n",
    "        # Mostrar información sobre el rango de fechas en los datos\n",
    "        logger.info(f\"Rango de fechas en los datos:\")\n",
    "        logger.info(f\"Fecha más antigua: {df_curves.index.min()}\")\n",
    "        logger.info(f\"Fecha más reciente: {df_curves.index.max()}\")\n",
    "        \n",
    "        # Filtrar por fecha usando query para mayor flexibilidad\n",
    "        logger.info(f\"Filtrando datos entre {start_date} y {end_date}...\")\n",
    "        df_curves = df_curves.query('@start_date <= index <= @end_date')\n",
    "        \n",
    "        if len(df_curves) == 0:\n",
    "            logger.warning(\"No se encontraron datos en el rango de fechas especificado.\")\n",
    "            logger.info(\"Ajustando el rango de fechas al rango disponible en los datos...\")\n",
    "            df_curves = df_curves.sort_index()\n",
    "        else:\n",
    "            logger.info(f\"Se encontraron {len(df_curves)} registros en el rango especificado.\")\n",
    "\n",
    "        # Guardar datos\n",
    "        output_filepath = os.path.join(output_dir, 'raw_iv600_data.csv')\n",
    "        logger.info(f\"Guardando datos en: {output_filepath}\")\n",
    "        df_curves.to_csv(output_filepath)\n",
    "        logger.info(f\"Datos guardados exitosamente. Total de registros: {len(df_curves)}\")\n",
    "        logger.info(f\"Rango de fechas: {df_curves.index.min()} a {df_curves.index.max()}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error en la descarga de datos IV600: {e}\")\n",
    "        import traceback\n",
    "        logger.error(f\"Detalles del error:\\n{traceback.format_exc()}\")\n",
    "        return False\n",
    "    finally:\n",
    "        if client:\n",
    "            logger.info(\"Cerrando conexión a Clickhouse...\")\n",
    "            client.close()\n",
    "            logger.info(\"Conexión a Clickhouse cerrada\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔋 DESCARGA PV GLASSES\n",
    "def download_pv_glasses(influx_client, start_date, end_date, output_dir):\n",
    "    \"\"\"Descarga y procesa datos de PV Glasses.\"\"\"\n",
    "    logger.info(\"Iniciando descarga de datos PV Glasses...\")\n",
    "    \n",
    "    try:\n",
    "        # Configuración de la consulta\n",
    "        bucket = \"meteo_psda\"\n",
    "        tables = [\"6852_Ftc\"]\n",
    "        attributes = [\"R_FC1_Avg\", \"R_FC2_Avg\", \"R_FC3_Avg\", \"R_FC4_Avg\", \"R_FC5_Avg\"]\n",
    "        \n",
    "        # Obtener datos\n",
    "        df_glasses = influx_client.query_influxdb(bucket, tables, attributes, start_date, end_date)\n",
    "        \n",
    "        if df_glasses is None or df_glasses.empty:\n",
    "            logger.warning(\"No se obtuvieron datos de PV Glasses\")\n",
    "            return False\n",
    "            \n",
    "        # Asegurar que el índice sea DatetimeIndex\n",
    "        if '_time' in df_glasses.columns:\n",
    "            df_glasses.set_index('timestamp', inplace=True)\n",
    "        elif 'time' in df_glasses.columns:\n",
    "            df_glasses.set_index('time', inplace=True)\n",
    "            \n",
    "        # Convertir el índice a DatetimeIndex si no lo es\n",
    "        if not isinstance(df_glasses.index, pd.DatetimeIndex):\n",
    "            df_glasses.index = pd.to_datetime(df_glasses.index)\n",
    "            \n",
    "        # Filtrar por horario (13:00 a 18:00)\n",
    "        df_glasses = df_glasses.between_time('13:00', '18:00')\n",
    "        \n",
    "        # Seleccionar solo las columnas numéricas para el cálculo\n",
    "        numeric_columns = df_glasses.select_dtypes(include=[np.number]).columns\n",
    "        df_glasses_numeric = df_glasses[numeric_columns]\n",
    "        \n",
    "        # Calcular referencia (promedio de R_FC1_Avg)\n",
    "        if 'R_FC1_Avg' in df_glasses_numeric.columns:\n",
    "            df_glasses['Ref'] = df_glasses_numeric['R_FC1_Avg'].mean()\n",
    "        \n",
    "        # Calcular datos diarios solo para columnas numéricas\n",
    "        df_glasses_daily = df_glasses_numeric.resample('1d').sum().div(60000)\n",
    "        \n",
    "        # Guardar datos\n",
    "        output_filepath = os.path.join(output_dir, 'raw_pv_glasses_data.csv')\n",
    "        daily_output_filepath = os.path.join(output_dir, 'raw_pv_glasses_daily_data.csv')\n",
    "        \n",
    "        df_glasses.to_csv(output_filepath)\n",
    "        df_glasses_daily.to_csv(daily_output_filepath)\n",
    "        \n",
    "        logger.info(f\"Datos PV Glasses guardados exitosamente\")\n",
    "        logger.info(f\"Total de registros: {len(df_glasses)}\")\n",
    "        logger.info(f\"Rango de fechas: {df_glasses.index.min()} a {df_glasses.index.max()}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error en la descarga de datos PV Glasses: {e}\")\n",
    "        import traceback\n",
    "        logger.error(f\"Detalles del error:\\n{traceback.format_exc()}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔋 DESCARGA DUSTIQ DESDE INFLUXDB\n",
    "def download_dustiq(influx_client, start_date, end_date, output_dir):\n",
    "    \"\"\"Descarga y procesa datos de DustIQ.\"\"\"\n",
    "    logger.info(\"Iniciando descarga de datos DustIQ...\")\n",
    "    \n",
    "    try:\n",
    "        # Configuración de la consulta\n",
    "        bucket = \"PSDA\"\n",
    "        tables = [\"DustIQ\"]\n",
    "        attributes = [\"SR_C11_Avg\", \"SR_C12_Avg\"]\n",
    "        \n",
    "        # Obtener datos\n",
    "        df_dustiq = influx_client.query_influxdb(bucket, tables, attributes, start_date, end_date)\n",
    "        \n",
    "        if df_dustiq is None or df_dustiq.empty:\n",
    "            logger.warning(\"No se obtuvieron datos de DustIQ\")\n",
    "            return False\n",
    "            \n",
    "        # Guardar datos\n",
    "        output_filepath = os.path.join(output_dir, 'raw_dustiq_data.csv')\n",
    "        df_dustiq.to_csv(output_filepath)\n",
    "        \n",
    "        logger.info(f\"Datos DustIQ guardados exitosamente\")\n",
    "        logger.info(f\"Total de registros: {len(df_dustiq)}\")\n",
    "        logger.info(f\"Rango de fechas: {df_dustiq.index.min()} a {df_dustiq.index.max()}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error en la descarga de datos DustIQ: {e}\")\n",
    "        import traceback\n",
    "        logger.error(f\"Detalles del error:\\n{traceback.format_exc()}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔋 DESCARGA DUSTIQ DESDE CLICKHOUSE\n",
    "def download_dustiq_clickhouse(start_date, end_date, output_dir):\n",
    "    \"\"\"Descarga y procesa datos de DustIQ desde ClickHouse.\"\"\"\n",
    "    logger.info(\"Iniciando descarga de datos DustIQ desde ClickHouse...\")\n",
    "    client = None\n",
    "    \n",
    "    try:\n",
    "        # Conectar a Clickhouse\n",
    "        logger.info(\"Conectando a Clickhouse...\")\n",
    "        client = clickhouse_connect.get_client(\n",
    "            host=CLICKHOUSE_CONFIG['host'],\n",
    "            port=CLICKHOUSE_CONFIG['port'],\n",
    "            username=CLICKHOUSE_CONFIG['user'],\n",
    "            password=CLICKHOUSE_CONFIG['password']\n",
    "        )\n",
    "        logger.info(\"Conexión a Clickhouse establecida\")\n",
    "        \n",
    "        # Convertir fechas al formato correcto para ClickHouse\n",
    "        start_str = start_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        end_str = end_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        # Consultar datos de dustiq desde el bucket PSDA\n",
    "        logger.info(\"Consultando datos DustIQ desde ClickHouse...\")\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            Stamptime,\n",
    "            Attribute,\n",
    "            Measure\n",
    "        FROM PSDA.dustiq \n",
    "        WHERE Stamptime >= '{start_str}' AND Stamptime <= '{end_str}'\n",
    "        AND Attribute IN ('SR_C11_Avg', 'SR_C12_Avg')\n",
    "        ORDER BY Stamptime, Attribute\n",
    "        \"\"\"\n",
    "        \n",
    "        logger.info(f\"Ejecutando consulta: {query[:100]}...\")\n",
    "        result = client.query(query)\n",
    "        \n",
    "        if not result.result_set:\n",
    "            logger.warning(\"No se encontraron datos de DustIQ en ClickHouse\")\n",
    "            return False\n",
    "            \n",
    "        logger.info(f\"Datos obtenidos: {len(result.result_set)} registros\")\n",
    "        \n",
    "        # Convertir a DataFrame\n",
    "        logger.info(\"Procesando datos...\")\n",
    "        df_dustiq = pd.DataFrame(result.result_set, columns=['Stamptime', 'Attribute', 'Measure'])        \n",
    "        # Convertir Stamptime a datetime y asegurar que esté en UTC\n",
    "        df_dustiq['Stamptime'] = pd.to_datetime(df_dustiq['Stamptime'])\n",
    "        if df_dustiq['Stamptime'].dt.tz is None:\n",
    "            df_dustiq['Stamptime'] = df_dustiq['Stamptime'].dt.tz_localize('UTC')\n",
    "        else:\n",
    "            df_dustiq['Stamptime'] = df_dustiq['Stamptime'].dt.tz_convert('UTC')\n",
    "\n",
    "        # Pivotar los datos para convertir de long format a wide format\n",
    "        logger.info(\"Pivotando datos de long format a wide format...\")\n",
    "\n",
    "        # Primero, manejar duplicados agregando por promedio\n",
    "        logger.info(\"Manejando duplicados agrupando por promedio...\")\n",
    "        df_dustiq_grouped = df_dustiq.groupby(['Stamptime', 'Attribute'])['Measure'].mean().reset_index()\n",
    "\n",
    "        # Ahora hacer el pivot sin duplicados\n",
    "        df_dustiq_pivot = df_dustiq_grouped.pivot(index='Stamptime', columns='Attribute', values='Measure')\n",
    "\n",
    "        # Renombrar el índice\n",
    "        df_dustiq_pivot.index.name = 'timestamp'\n",
    "        # Mostrar información sobre el rango de fechas en los datos\n",
    "        logger.info(f\"Rango de fechas en los datos:\")\n",
    "        logger.info(f\"Fecha más antigua: {df_dustiq_pivot.index.min()}\")\n",
    "        logger.info(f\"Fecha más reciente: {df_dustiq_pivot.index.max()}\")\n",
    "\n",
    "        # Verificar que hay datos en el rango especificado\n",
    "        if len(df_dustiq_pivot) == 0:\n",
    "            logger.warning(\"No se encontraron datos en el rango de fechas especificado.\")\n",
    "            return False\n",
    "\n",
    "        # Guardar datos\n",
    "        output_filepath = os.path.join(output_dir, 'raw_dustiq_data.csv')\n",
    "        logger.info(f\"Guardando datos en: {output_filepath}\")\n",
    "        df_dustiq_pivot.to_csv(output_filepath)\n",
    "\n",
    "        logger.info(f\"Datos DustIQ desde ClickHouse guardados exitosamente\")\n",
    "        logger.info(f\"Total de registros: {len(df_dustiq_pivot)}\")\n",
    "        logger.info(f\"Rango de fechas: {df_dustiq_pivot.index.min()} a {df_dustiq_pivot.index.max()}\")\n",
    "\n",
    "        # Mostrar estadísticas básicas\n",
    "        logger.info(\"Estadísticas de los datos:\")\n",
    "        if 'SR_C11_Avg' in df_dustiq_pivot.columns:\n",
    "            logger.info(f\"SR_C11_Avg - Rango: {df_dustiq_pivot['SR_C11_Avg'].min():.3f} a {df_dustiq_pivot['SR_C11_Avg'].max():.3f}\")\n",
    "        if 'SR_C12_Avg' in df_dustiq_pivot.columns:\n",
    "            logger.info(f\"SR_C12_Avg - Rango: {df_dustiq_pivot['SR_C12_Avg'].min():.3f} a {df_dustiq_pivot['SR_C12_Avg'].max():.3f}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error en la descarga de datos DustIQ desde ClickHouse: {e}\")\n",
    "        import traceback\n",
    "        logger.error(f\"Detalles del error:\\n{traceback.format_exc()}\")\n",
    "        return False\n",
    "    finally:\n",
    "        if client:\n",
    "            logger.info(\"Cerrando conexión a Clickhouse...\")\n",
    "            client.close()\n",
    "            logger.info(\"Conexión a Clickhouse cerrada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔋 DESCARGA PVSTAND DESDE CLICKHOUSE (CON IDENTIFICACIÓN DE MÓDULO)\n",
    "# ============================================================================\n",
    "\n",
    "def download_pvstand_clickhouse(start_date, end_date, output_dir):\n",
    "    \"\"\"Descarga y procesa datos de PVStand desde ClickHouse.\"\"\"\n",
    "    logger.info(\"Iniciando descarga de datos PVStand desde ClickHouse...\")\n",
    "    client = None\n",
    "    \n",
    "    try:\n",
    "        # Conectar a Clickhouse\n",
    "        logger.info(\"Conectando a Clickhouse...\")\n",
    "        client = clickhouse_connect.get_client(\n",
    "            host=CLICKHOUSE_CONFIG['host'],\n",
    "            port=CLICKHOUSE_CONFIG['port'],\n",
    "            username=CLICKHOUSE_CONFIG['user'],\n",
    "            password=CLICKHOUSE_CONFIG['password']\n",
    "        )\n",
    "        logger.info(\"Conexión a Clickhouse establecida\")\n",
    "        \n",
    "        # Convertir fechas al formato correcto para ClickHouse\n",
    "        start_str = start_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        end_str = end_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        # Consultar datos de PVStand desde las tablas perc1fixed y perc2fixed\n",
    "        logger.info(\"Consultando datos PVStand desde ClickHouse...\")\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            timestamp,\n",
    "            'perc1fixed' as module,\n",
    "            pmax,\n",
    "            imax,\n",
    "            umax\n",
    "        FROM PSDA.perc1fixed \n",
    "        WHERE timestamp >= '{start_str}' AND timestamp <= '{end_str}'\n",
    "        \n",
    "        UNION ALL\n",
    "        \n",
    "        SELECT \n",
    "            timestamp,\n",
    "            'perc2fixed' as module,\n",
    "            pmax,\n",
    "            imax,\n",
    "            umax\n",
    "        FROM PSDA.perc2fixed \n",
    "        WHERE timestamp >= '{start_str}' AND timestamp <= '{end_str}'\n",
    "        \n",
    "        ORDER BY timestamp\n",
    "        \"\"\"\n",
    "        \n",
    "        logger.info(f\"Ejecutando consulta: {query[:100]}...\")\n",
    "        result = client.query(query)\n",
    "        \n",
    "        if not result.result_set:\n",
    "            logger.warning(\"No se encontraron datos de PVStand en ClickHouse\")\n",
    "            return False\n",
    "            \n",
    "        logger.info(f\"Datos obtenidos: {len(result.result_set)} registros\")\n",
    "        \n",
    "        # Convertir a DataFrame\n",
    "        logger.info(\"Procesando datos...\")\n",
    "        df_pvstand = pd.DataFrame(result.result_set, columns=['timestamp', 'module', 'pmax', 'imax', 'umax'])\n",
    "        \n",
    "        # Convertir timestamp a datetime y asegurar que esté en UTC\n",
    "        df_pvstand['timestamp'] = pd.to_datetime(df_pvstand['timestamp'])\n",
    "        if df_pvstand['timestamp'].dt.tz is None:\n",
    "            df_pvstand['timestamp'] = df_pvstand['timestamp'].dt.tz_localize('UTC')\n",
    "        else:\n",
    "            df_pvstand['timestamp'] = df_pvstand['timestamp'].dt.tz_convert('UTC')\n",
    "\n",
    "        # Establecer timestamp como índice\n",
    "        df_pvstand.set_index('timestamp', inplace=True)\n",
    "        \n",
    "        # Ordenar por timestamp (importante para series temporales)\n",
    "        logger.info(\"Ordenando datos por timestamp...\")\n",
    "        df_pvstand = df_pvstand.sort_index()\n",
    "        \n",
    "        # Mostrar información sobre el rango de fechas en los datos\n",
    "        logger.info(f\"Rango de fechas en los datos:\")\n",
    "        logger.info(f\"Fecha más antigua: {df_pvstand.index.min()}\")\n",
    "        logger.info(f\"Fecha más reciente: {df_pvstand.index.max()}\")\n",
    "\n",
    "        # Verificar que hay datos en el rango especificado\n",
    "        if len(df_pvstand) == 0:\n",
    "            logger.warning(\"No se encontraron datos en el rango de fechas especificado.\")\n",
    "            return False\n",
    "\n",
    "        # Mostrar distribución por módulo\n",
    "        module_counts = df_pvstand['module'].value_counts()\n",
    "        logger.info(\"Distribución por módulo:\")\n",
    "        for module, count in module_counts.items():\n",
    "            logger.info(f\"   - {module}: {count} registros\")\n",
    "\n",
    "        # Guardar datos\n",
    "        output_filepath = os.path.join(output_dir, 'raw_pvstand_clickhouse_data.csv')\n",
    "        logger.info(f\"Guardando datos en: {output_filepath}\")\n",
    "        df_pvstand.to_csv(output_filepath)\n",
    "\n",
    "        logger.info(f\"Datos PVStand desde ClickHouse guardados exitosamente\")\n",
    "        logger.info(f\"Total de registros: {len(df_pvstand)}\")\n",
    "        logger.info(f\"Rango de fechas: {df_pvstand.index.min()} a {df_pvstand.index.max()}\")\n",
    "\n",
    "        # Mostrar estadísticas básicas por módulo\n",
    "        logger.info(\"Estadísticas de los datos por módulo:\")\n",
    "        for module in ['perc1fixed', 'perc2fixed']:\n",
    "            if module in df_pvstand['module'].values:\n",
    "                module_data = df_pvstand[df_pvstand['module'] == module]\n",
    "                logger.info(f\"\\n{module}:\")\n",
    "                logger.info(f\"   pmax - Rango: {module_data['pmax'].min():.3f} a {module_data['pmax'].max():.3f}\")\n",
    "                logger.info(f\"   imax - Rango: {module_data['imax'].min():.3f} a {module_data['imax'].max():.3f}\")\n",
    "                logger.info(f\"   umax - Rango: {module_data['umax'].min():.3f} a {module_data['umax'].max():.3f}\")\n",
    "        \n",
    "        # Mostrar información sobre la estructura de datos\n",
    "        logger.info(\"\\nEstructura de datos del PVStand:\")\n",
    "        logger.info(f\"   - module: Identificador del módulo (perc1fixed/perc2fixed)\")\n",
    "        logger.info(f\"   - pmax: Potencia máxima del módulo\")\n",
    "        logger.info(f\"   - imax: Corriente máxima del módulo\")\n",
    "        logger.info(f\"   - umax: Voltaje máximo del módulo\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error en la descarga de datos PVStand desde ClickHouse: {e}\")\n",
    "        import traceback\n",
    "        logger.error(f\"Detalles del error:\\n{traceback.format_exc()}\")\n",
    "        return False\n",
    "    finally:\n",
    "        if client:\n",
    "            logger.info(\"Cerrando conexión a Clickhouse...\")\n",
    "            client.close()\n",
    "            logger.info(\"Conexión a Clickhouse cerrada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔋 DESCARGA PVSTAND DESDE INFLUXDB\n",
    "def download_pvstand(influx_client, start_date, end_date, output_dir):\n",
    "    \"\"\"Descarga y procesa datos de PVStand.\"\"\"\n",
    "    logger.info(\"Iniciando descarga de datos PVStand...\")\n",
    "    \n",
    "    try:\n",
    "        # Configuración de la consulta\n",
    "        bucket = \"PSDA\"\n",
    "        tables = [\"PERC1_fixed_1MD43420160719\", \"PERC2_fixed_1MD43920160719\"]\n",
    "        attributes = [\"Imax\", \"Umax\", \"Pmax\"]\n",
    "        \n",
    "        # Obtener datos\n",
    "        df_pvstand = influx_client.query_influxdb(bucket, tables, attributes, start_date, end_date)\n",
    "        \n",
    "        if df_pvstand is None or df_pvstand.empty:\n",
    "            logger.warning(\"No se obtuvieron datos de PVStand\")\n",
    "            return False\n",
    "            \n",
    "        # Guardar datos\n",
    "        output_filepath = os.path.join(output_dir, 'raw_pvstand_iv_data.csv')\n",
    "        df_pvstand.to_csv(output_filepath)\n",
    "        \n",
    "        logger.info(f\"Datos PVStand guardados exitosamente\")\n",
    "        logger.info(f\"Total de registros: {len(df_pvstand)}\")\n",
    "        logger.info(f\"Rango de fechas: {df_pvstand.index.min()} a {df_pvstand.index.max()}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error en la descarga de datos PVStand: {e}\")\n",
    "        import traceback\n",
    "        logger.error(f\"Detalles del error:\\n{traceback.format_exc()}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔋 DESCARGA SOILING KIT DESDE INFLUXDB\n",
    "def download_soiling_kit(influx_client, start_date, end_date, output_dir):\n",
    "    \"\"\"Descarga y procesa datos del Soiling Kit.\"\"\"\n",
    "    logger.info(\"Iniciando descarga de datos del Soiling Kit...\")\n",
    "    \n",
    "    try:\n",
    "        # Configuración de la consulta\n",
    "        bucket = \"PSDA\"\n",
    "        tables = [\"soilingkit\"]\n",
    "        attributes = [\"Isc(e)\", \"Isc(p)\", \"Te(C)\", \"Tp(C)\"]\n",
    "        \n",
    "        # Obtener datos\n",
    "        df_sk = influx_client.query_influxdb(bucket, tables, attributes, start_date, end_date)\n",
    "        \n",
    "        if df_sk is None or df_sk.empty:\n",
    "            logger.warning(\"No se obtuvieron datos del Soiling Kit\")\n",
    "            return False\n",
    "            \n",
    "        # Guardar datos\n",
    "        output_filepath = os.path.join(output_dir, 'soiling_kit_raw_data.csv')\n",
    "        df_sk.to_csv(output_filepath)\n",
    "        \n",
    "        logger.info(f\"Datos del Soiling Kit guardados exitosamente\")\n",
    "        logger.info(f\"Total de registros: {len(df_sk)}\")\n",
    "        logger.info(f\"Rango de fechas: {df_sk.index.min()} a {df_sk.index.max()}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error en la descarga de datos del Soiling Kit: {e}\")\n",
    "        import traceback\n",
    "        logger.error(f\"Detalles del error:\\n{traceback.format_exc()}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🌪️ DESCARGA SOILING KIT DESDE CLICKHOUSE\n",
    "# ============================================================================\n",
    "\n",
    "def download_soiling_kit_clickhouse(start_date, end_date, output_dir):\n",
    "    \"\"\"Descarga y procesa datos del Soiling Kit desde ClickHouse.\"\"\"\n",
    "    logger.info(\"Iniciando descarga de datos del Soiling Kit desde ClickHouse...\")\n",
    "    client = None\n",
    "    \n",
    "    try:\n",
    "        # Conectar a Clickhouse\n",
    "        logger.info(\"Conectando a Clickhouse...\")\n",
    "        client = clickhouse_connect.get_client(\n",
    "            host=CLICKHOUSE_CONFIG['host'],\n",
    "            port=CLICKHOUSE_CONFIG['port'],\n",
    "            username=CLICKHOUSE_CONFIG['user'],\n",
    "            password=CLICKHOUSE_CONFIG['password']\n",
    "        )\n",
    "        logger.info(\"Conexión a Clickhouse establecida\")\n",
    "        \n",
    "        # Convertir fechas al formato correcto para ClickHouse\n",
    "        start_str = start_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        end_str = end_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        # Consultar datos del Soiling Kit desde PSDA.soilingkit\n",
    "        logger.info(\"Consultando datos del Soiling Kit desde ClickHouse...\")\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            Stamptime,\n",
    "            Attribute,\n",
    "            Measure\n",
    "        FROM PSDA.soilingkit \n",
    "        WHERE Stamptime >= '{start_str}' AND Stamptime <= '{end_str}'\n",
    "        AND Attribute IN ('Isc(e)', 'Isc(p)', 'Te(C)', 'Tp(C)')\n",
    "        ORDER BY Stamptime, Attribute\n",
    "        \"\"\"\n",
    "        \n",
    "        logger.info(f\"Ejecutando consulta: {query[:100]}...\")\n",
    "        result = client.query(query)\n",
    "        \n",
    "        if not result.result_set:\n",
    "            logger.warning(\"No se encontraron datos del Soiling Kit en ClickHouse\")\n",
    "            return False\n",
    "            \n",
    "        logger.info(f\"Datos obtenidos: {len(result.result_set)} registros\")\n",
    "        \n",
    "        # Convertir a DataFrame\n",
    "        logger.info(\"Procesando datos...\")\n",
    "        df_soilingkit = pd.DataFrame(result.result_set, columns=['Stamptime', 'Attribute', 'Measure'])\n",
    "        \n",
    "        # Convertir Stamptime a datetime y asegurar que esté en UTC\n",
    "        df_soilingkit['Stamptime'] = pd.to_datetime(df_soilingkit['Stamptime'])\n",
    "        if df_soilingkit['Stamptime'].dt.tz is None:\n",
    "            df_soilingkit['Stamptime'] = df_soilingkit['Stamptime'].dt.tz_localize('UTC')\n",
    "        else:\n",
    "            df_soilingkit['Stamptime'] = df_soilingkit['Stamptime'].dt.tz_convert('UTC')\n",
    "\n",
    "        # Pivotar los datos para convertir de long format a wide format\n",
    "        logger.info(\"Pivotando datos de long format a wide format...\")\n",
    "\n",
    "        # Primero, manejar duplicados agregando por promedio\n",
    "        logger.info(\"Manejando duplicados agrupando por promedio...\")\n",
    "        df_soilingkit_grouped = df_soilingkit.groupby(['Stamptime', 'Attribute'])['Measure'].mean().reset_index()\n",
    "\n",
    "        # Ahora hacer el pivot sin duplicados\n",
    "        df_soilingkit_pivot = df_soilingkit_grouped.pivot(index='Stamptime', columns='Attribute', values='Measure')\n",
    "\n",
    "        # Renombrar el índice\n",
    "        df_soilingkit_pivot.index.name = 'timestamp'\n",
    "        \n",
    "        # Mostrar información sobre el rango de fechas en los datos\n",
    "        logger.info(f\"Rango de fechas en los datos:\")\n",
    "        logger.info(f\"Fecha más antigua: {df_soilingkit_pivot.index.min()}\")\n",
    "        logger.info(f\"Fecha más reciente: {df_soilingkit_pivot.index.max()}\")\n",
    "\n",
    "        # Verificar que hay datos en el rango especificado\n",
    "        if len(df_soilingkit_pivot) == 0:\n",
    "            logger.warning(\"No se encontraron datos en el rango de fechas especificado.\")\n",
    "            return False\n",
    "\n",
    "        # Guardar datos\n",
    "        output_filepath = os.path.join(output_dir, 'soiling_kit_raw_data.csv')\n",
    "        logger.info(f\"Guardando datos en: {output_filepath}\")\n",
    "        df_soilingkit_pivot.to_csv(output_filepath)\n",
    "\n",
    "        logger.info(f\"Datos del Soiling Kit desde ClickHouse guardados exitosamente\")\n",
    "        logger.info(f\"Total de registros: {len(df_soilingkit_pivot)}\")\n",
    "        logger.info(f\"Rango de fechas: {df_soilingkit_pivot.index.min()} a {df_soilingkit_pivot.index.max()}\")\n",
    "\n",
    "        # Mostrar estadísticas básicas\n",
    "        logger.info(\"Estadísticas de los datos:\")\n",
    "        if 'Isc(e)' in df_soilingkit_pivot.columns:\n",
    "            logger.info(f\"Isc(e) - Rango: {df_soilingkit_pivot['Isc(e)'].min():.3f} a {df_soilingkit_pivot['Isc(e)'].max():.3f}\")\n",
    "        if 'Isc(p)' in df_soilingkit_pivot.columns:\n",
    "            logger.info(f\"Isc(p) - Rango: {df_soilingkit_pivot['Isc(p)'].min():.3f} a {df_soilingkit_pivot['Isc(p)'].max():.3f}\")\n",
    "        if 'Te(C)' in df_soilingkit_pivot.columns:\n",
    "            logger.info(f\"Te(C) - Rango: {df_soilingkit_pivot['Te(C)'].min():.1f} a {df_soilingkit_pivot['Te(C)'].max():.1f}\")\n",
    "        if 'Tp(C)' in df_soilingkit_pivot.columns:\n",
    "            logger.info(f\"Tp(C) - Rango: {df_soilingkit_pivot['Tp(C)'].min():.1f} a {df_soilingkit_pivot['Tp(C)'].max():.1f}\")\n",
    "        \n",
    "        # Mostrar información sobre la estructura de datos\n",
    "        logger.info(\"Estructura de datos del Soiling Kit:\")\n",
    "        logger.info(f\"   - Isc(e): Corriente de cortocircuito de la celda limpia (referencia)\")\n",
    "        logger.info(f\"   - Isc(p): Corriente de cortocircuito de la celda sucia (panel)\")\n",
    "        logger.info(f\"   - Te(C): Temperatura de la celda limpia en Celsius\")\n",
    "        logger.info(f\"   - Tp(C): Temperatura de la celda sucia en Celsius\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error en la descarga de datos del Soiling Kit desde ClickHouse: {e}\")\n",
    "        import traceback\n",
    "        logger.error(f\"Detalles del error:\\n{traceback.format_exc()}\")\n",
    "        return False\n",
    "    finally:\n",
    "        if client:\n",
    "            logger.info(\"Cerrando conexión a Clickhouse...\")\n",
    "            client.close()\n",
    "            logger.info(\"Conexión a Clickhouse cerrada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔋 DESCARGA TEMPERATURA DE MÓDULOS DESDE INFLUXDB\n",
    "def download_temp_mod_fixed(influx_client, start_date, end_date, output_dir):\n",
    "    \"\"\"Descarga y procesa datos de temperatura de módulos (PT100).\"\"\"\n",
    "    logger.info(\"Iniciando descarga de datos de Temperatura de Módulos (PT100)...\")\n",
    "    \n",
    "    try:\n",
    "        bucket = \"PSDA\"\n",
    "        attributes = [\"1TE416(C)\", \"1TE417(C)\", \"1TE418(C)\", \"1TE419(C)\"] # Sensores PT100\n",
    "        \n",
    "        # Fuente 1: Tabla TempModFixed\n",
    "        tables_source1 = [\"TempModFixed\"]\n",
    "        logger.info(f\"Consultando datos de temperatura de {tables_source1}...\")\n",
    "        df_temp_source1 = influx_client.query_influxdb(bucket, tables_source1, attributes, start_date, end_date)\n",
    "        \n",
    "        if df_temp_source1 is not None and not df_temp_source1.empty:\n",
    "            if '_time' in df_temp_source1.columns:\n",
    "                df_temp_source1.set_index('_time', inplace=True)\n",
    "            elif 'time' in df_temp_source1.columns: # Por si acaso el nombre de la columna de tiempo varía\n",
    "                df_temp_source1.set_index('time', inplace=True)\n",
    "\n",
    "            if not isinstance(df_temp_source1.index, pd.DatetimeIndex):\n",
    "                df_temp_source1.index = pd.to_datetime(df_temp_source1.index)\n",
    "            \n",
    "            df_temp_source1 = df_temp_source1.between_time('13:00', '18:00') # Filtro horario como en soiling_intercomparison.py\n",
    "            logger.info(f\"Datos de {tables_source1} procesados. Registros: {len(df_temp_source1)}\")\n",
    "        else:\n",
    "            logger.warning(f\"No se obtuvieron datos de temperatura de {tables_source1} o el DataFrame está vacío.\")\n",
    "            df_temp_source1 = pd.DataFrame() # Asegurar que sea un DF vacío si no hay datos\n",
    "\n",
    "        # Fuente 2: Tabla fixed_plant_atamo_1 (como en soiling_intercomparison.py)\n",
    "        # El script original usa una fecha de inicio fija para esta fuente: pd.to_datetime('05/12/2024', dayfirst=True)\n",
    "        # Usaremos la 'start_date' global, pero puedes ajustarla si necesitas la fecha fija.\n",
    "        # Para replicar exactamente, podrías usar:\n",
    "        # date_s_fixed_plant = pd.to_datetime('05/12/2024', dayfirst=True).tz_localize('UTC')\n",
    "        # Y luego pasar date_s_fixed_plant a query_influxdb para esta fuente.\n",
    "        # Por ahora, usaremos start_date y end_date globales.\n",
    "        tables_source2 = [\"fixed_plant_atamo_1\"]\n",
    "        logger.info(f\"Consultando datos de temperatura de {tables_source2}...\")\n",
    "        df_temp_source2 = influx_client.query_influxdb(bucket, tables_source2, attributes, start_date, end_date)\n",
    "\n",
    "        if df_temp_source2 is not None and not df_temp_source2.empty:\n",
    "            if '_time' in df_temp_source2.columns:\n",
    "                df_temp_source2.set_index('_time', inplace=True)\n",
    "            elif 'time' in df_temp_source2.columns:\n",
    "                df_temp_source2.set_index('time', inplace=True)\n",
    "\n",
    "            if not isinstance(df_temp_source2.index, pd.DatetimeIndex):\n",
    "                df_temp_source2.index = pd.to_datetime(df_temp_source2.index)\n",
    "                \n",
    "            df_temp_source2 = df_temp_source2.between_time('13:00', '18:00')\n",
    "            logger.info(f\"Datos de {tables_source2} procesados. Registros: {len(df_temp_source2)}\")\n",
    "        else:\n",
    "            logger.warning(f\"No se obtuvieron datos de temperatura de {tables_source2} o el DataFrame está vacío.\")\n",
    "            df_temp_source2 = pd.DataFrame()\n",
    "\n",
    "        # Concatenar datos de ambas fuentes\n",
    "        dataframes_to_concat = []\n",
    "        if not df_temp_source1.empty:\n",
    "            dataframes_to_concat.append(df_temp_source1)\n",
    "        if not df_temp_source2.empty:\n",
    "            dataframes_to_concat.append(df_temp_source2)\n",
    "\n",
    "        if not dataframes_to_concat:\n",
    "            logger.warning(\"No hay datos de temperatura de ninguna fuente para concatenar.\")\n",
    "            return False\n",
    "            \n",
    "        df_temp_combined = pd.concat(dataframes_to_concat)\n",
    "        df_temp_combined = df_temp_combined.sort_index() # Ordenar por tiempo\n",
    "        \n",
    "        # Eliminar columnas que pueden ser resultado de la consulta y no son atributos (result, table)\n",
    "        cols_to_drop = [col for col in ['result', 'table'] if col in df_temp_combined.columns]\n",
    "        if cols_to_drop:\n",
    "            df_temp_combined.drop(columns=cols_to_drop, inplace=True)\n",
    "            \n",
    "        # Guardar datos\n",
    "        output_filepath = os.path.join(output_dir, 'temp_mod_fixed_data.csv')\n",
    "        df_temp_combined.to_csv(output_filepath)\n",
    "        \n",
    "        logger.info(f\"Datos de Temperatura de Módulos guardados exitosamente en {output_filepath}\")\n",
    "        logger.info(f\"Total de registros combinados: {len(df_temp_combined)}\")\n",
    "        if not df_temp_combined.empty:\n",
    "            logger.info(f\"Rango de fechas: {df_temp_combined.index.min()} a {df_temp_combined.index.max()}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error en la descarga de datos de Temperatura de Módulos: {e}\")\n",
    "        import traceback\n",
    "        logger.error(f\"Detalles del error:\\\\n{traceback.format_exc()}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔋 **REFCELLS - CELDAS SIMPLIFICADAS**\n",
    "\n",
    "## 🚫 **CELDAS OBSOLETAS (NO USAR):**\n",
    "\n",
    "### ❌ **Celda 12** - Función básica obsoleta\n",
    "- Versión antigua de `download_refcells()`\n",
    "- **REEMPLAZADA** por Celda 17\n",
    "\n",
    "### ❌ **Celda 14** - Ejecución ya completada  \n",
    "- Código que ya se ejecutó exitosamente\n",
    "- **REDUNDANTE** con Celda 17 + 19\n",
    "\n",
    "### ❌ **Celda 18** - Ejecución automática\n",
    "- Se ejecuta automáticamente al cargar\n",
    "- **CAUSA PROBLEMAS** y duplica funcionalidad\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ **CELDAS A USAR (SOLO ESTAS 2):**\n",
    "\n",
    "### 📝 **Celda 17** - Función Avanzada\n",
    "- `download_refcells()` **optimizada y completa**\n",
    "- Solo **define** la función (no ejecuta)\n",
    "\n",
    "### 🚀 **Celda 19** - Ejecución Simple\n",
    "- **Ejecuta** la función cuando tú quieras\n",
    "- Configuración automática incluida\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **INSTRUCCIONES:**\n",
    "1. **Ejecuta Celda 17** → Define función\n",
    "2. **Ejecuta Celda 19** → Descarga datos\n",
    "3. **¡Listo!** → Ignora las demás celdas de RefCells\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DESCARGA DE TODOS LOS DATOS\n",
    "# ============================================================================\n",
    "\n",
    "def download_all_data(start_date, end_date, output_dir):\n",
    "    \"\"\"Función principal que coordina la descarga de todos los datos.\"\"\"\n",
    "    logger.info(\"Iniciando proceso de descarga de datos...\")\n",
    "    \n",
    "    # Crear directorio de salida si no existe\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Inicializar resultados\n",
    "    results = {}\n",
    "    \n",
    "    try:\n",
    "        # Primero descargar datos de Clickhouse\n",
    "        logger.info(\"\\nIniciando descarga de datos desde Clickhouse...\")\n",
    "        results['iv600'] = download_iv600(start_date, end_date, output_dir)\n",
    "        logger.info(f\"Descarga IV600: {'Exitosa' if results['iv600'] else 'Fallida'}\")\n",
    "        \n",
    "        # Liberar memoria\n",
    "        gc.collect()\n",
    "        \n",
    "        # Luego descargar datos de InfluxDB\n",
    "        logger.info(\"\\nIniciando descargas desde InfluxDB...\")\n",
    "        \n",
    "        # Inicializar cliente InfluxDB\n",
    "        influx_manager = InfluxDBManager(INFLUX_CONFIG)\n",
    "        if not influx_manager.connect():\n",
    "            logger.error(\"No se pudo establecer conexión con InfluxDB\")\n",
    "            return results\n",
    "            \n",
    "        try:\n",
    "            # Descargar datos de PV Glasses\n",
    "            logger.info(\"\\nProcesando PV Glasses...\")\n",
    "            results['pv_glasses'] = download_pv_glasses(influx_manager, start_date, end_date, output_dir)\n",
    "            logger.info(f\"Descarga PV Glasses: {'Exitosa' if results['pv_glasses'] else 'Fallida'}\")\n",
    "            \n",
    "            # Descargar datos de DustIQ\n",
    "            logger.info(\"\\nProcesando DustIQ...\")\n",
    "            results['dustiq'] = download_dustiq(influx_manager, start_date, end_date, output_dir)\n",
    "            logger.info(f\"Descarga DustIQ: {'Exitosa' if results['dustiq'] else 'Fallida'}\")\n",
    "            \n",
    "            # Descargar datos de PVStand\n",
    "            logger.info(\"\\nProcesando PVStand...\")\n",
    "            results['pvstand'] = download_pvstand(influx_manager, start_date, end_date, output_dir)\n",
    "            logger.info(f\"Descarga PVStand: {'Exitosa' if results['pvstand'] else 'Fallida'}\")\n",
    "            \n",
    "            # Descargar datos de celdas de referencia (CON FECHA ESPECÍFICA Y PROCESAMIENTO)\n",
    "            logger.info(\"\\nProcesando celdas de referencia...\")\n",
    "            # Fecha específica para refcells: 23 de julio de 2024\n",
    "            refcells_start_date = pd.to_datetime('23/07/2024', dayfirst=True).tz_localize('UTC')\n",
    "            logger.info(f\"Usando fecha específica para RefCells: {refcells_start_date}\")\n",
    "            \n",
    "            results['refcells'] = download_refcells(influx_manager, refcells_start_date, end_date, output_dir)\n",
    "            logger.info(f\"Descarga celdas de referencia: {'Exitosa' if results['refcells'] else 'Fallida'}\")\n",
    "            \n",
    "            # PROCESAMIENTO ADICIONAL DE REFCELLS\n",
    "            if results['refcells']:\n",
    "                logger.info(\"Aplicando procesamiento adicional a RefCells...\")\n",
    "                try:\n",
    "                    # Leer el archivo generado\n",
    "                    input_filepath = os.path.join(output_dir, 'refcells_data.csv')\n",
    "                    df = pd.read_csv(input_filepath)\n",
    "                    \n",
    "                    # Establecer _time como índice\n",
    "                    df['_time'] = pd.to_datetime(df['_time'])\n",
    "                    df = df.set_index('_time')\n",
    "                    \n",
    "                    # Resample por minuto (promedio)\n",
    "                    logger.info(\"Aplicando resample por minuto a RefCells...\")\n",
    "                    df_resampled = df.resample('1min').mean()\n",
    "                    df_resampled = df_resampled.dropna(how='all')\n",
    "                    \n",
    "                    # Sobrescribir el archivo original con los datos procesados\n",
    "                    final_output_path = os.path.join(output_dir, 'refcells_data.csv')\n",
    "                    df_resampled.to_csv(final_output_path)\n",
    "                    \n",
    "                    logger.info(f\"RefCells procesadas: {len(df)} → {len(df_resampled)} registros\")\n",
    "                    results['refcells_processed'] = True\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error en procesamiento de RefCells: {e}\")\n",
    "                    results['refcells_processed'] = False\n",
    "            \n",
    "            # Descargar datos del Soiling Kit\n",
    "            logger.info(\"\\nProcesando Soiling Kit...\")\n",
    "            results['soiling_kit'] = download_soiling_kit(influx_manager, start_date, end_date, output_dir)\n",
    "            logger.info(f\"Descarga Soiling Kit: {'Exitosa' if results['soiling_kit'] else 'Fallida'}\")\n",
    "\n",
    "            # Descargar datos de Temperatura de Módulos\n",
    "            logger.info(\"\\nProcesando Temperatura de Módulos (PT100)...\")\n",
    "            results['temp_mod_fixed'] = download_temp_mod_fixed(influx_manager, start_date, end_date, output_dir)\n",
    "            logger.info(f\"Descarga Temperatura de Módulos: {'Exitosa' if results['temp_mod_fixed'] else 'Fallida'}\")\n",
    "\n",
    "        finally:\n",
    "            # Cerrar conexión con InfluxDB\n",
    "            influx_manager.disconnect()\n",
    "            \n",
    "        # Resumen de resultados\n",
    "        logger.info(\"\\nResumen de descargas:\")\n",
    "        for key, value in results.items():\n",
    "            logger.info(f\"{key}: {'Exitosa' if value else 'Fallida'}\")\n",
    "            \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error en el proceso de descarga: {e}\")\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 12:40:59 - INFO - \n",
      "================================================================================\n",
      "2025-08-06 12:40:59 - INFO - 🌪️ DESCARGA ACTUALIZADA: SOILING KIT CON VERIFICACIÓN\n",
      "2025-08-06 12:40:59 - INFO - ================================================================================\n",
      "2025-08-06 12:41:00 - INFO - 📅 Rango actualizado: 2024-07-01 a 2025-07-31\n",
      "2025-08-06 12:41:00 - INFO - 📁 Directorio de salida: /home/nicole/SR/SOILING/datos\n",
      "2025-08-06 12:41:02 - INFO - Cliente InfluxDB y query_api inicializados.\n",
      "2025-08-06 12:41:02 - INFO - ✅ Conexión a InfluxDB establecida\n",
      "2025-08-06 12:41:02 - INFO - \n",
      "🔍 VERIFICANDO DATOS DISPONIBLES ANTES DE DESCARGAR...\n",
      "2025-08-06 12:41:02 - INFO - 🔍 Ejecutando consulta de verificación...\n",
      "/home/nicole/SR/SOILING/.venv/lib/python3.12/site-packages/influxdb_client/client/warnings.py:31: MissingPivotFunction: The query doesn't contains the pivot() function.\n",
      "\n",
      "The result will not be shaped to optimal processing by pandas.DataFrame. Use the pivot() function by:\n",
      "\n",
      "    \n",
      "        from(bucket: \"PSDA\")\n",
      "            |> range(start: 2024-07-01T00:00:00Z, stop: 2025-07-31T00:00:00Z)\n",
      "            |> filter(fn: (r) => r[\"_measurement\"] == \"soilingkit\")\n",
      "            |> filter(fn: (r) => r[\"_field\"] == \"Isc(e)\" or r[\"_field\"] == \"Isc(p)\" or r[\"_field\"] == \"Te(C)\" or r[\"_field\"] == \"Tp(C)\")\n",
      "            |> keep(columns: [\"_time\", \"_field\", \"_value\"])\n",
      "            |> sort(columns: [\"_time\"])\n",
      "         |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n",
      "\n",
      "You can disable this warning by:\n",
      "    import warnings\n",
      "    from influxdb_client.client.warnings import MissingPivotFunction\n",
      "\n",
      "    warnings.simplefilter(\"ignore\", MissingPivotFunction)\n",
      "\n",
      "For more info see:\n",
      "    - https://docs.influxdata.com/resources/videos/pivots-in-flux/\n",
      "    - https://docs.influxdata.com/flux/latest/stdlib/universe/pivot/\n",
      "    - https://docs.influxdata.com/flux/latest/stdlib/influxdata/influxdb/schema/fieldsascols/\n",
      "\n",
      "  warnings.warn(message, MissingPivotFunction)\n",
      "2025-08-06 12:41:15 - INFO - ✅ DATOS ENCONTRADOS: 276760 registros\n",
      "2025-08-06 12:41:15 - INFO - 📅 Fecha más antigua: 2024-07-01 12:02:15+00:00\n",
      "2025-08-06 12:41:15 - INFO - 📅 Fecha más reciente: 2025-07-30 16:07:09+00:00\n",
      "2025-08-06 12:41:15 - INFO - 📊 Registros por campo:\n",
      "2025-08-06 12:41:15 - INFO -    - Isc(e): 69190 registros\n",
      "2025-08-06 12:41:15 - INFO -    - Isc(p): 69190 registros\n",
      "2025-08-06 12:41:15 - INFO -    - Te(C): 69190 registros\n",
      "2025-08-06 12:41:15 - INFO -    - Tp(C): 69190 registros\n",
      "/tmp/ipykernel_38525/3687243577.py:96: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  verification_result['year_month'] = verification_result['_time'].dt.to_period('M')\n",
      "2025-08-06 12:41:16 - INFO - \n",
      "📈 Registros por mes:\n",
      "2025-08-06 12:41:16 - INFO -    - 2024-07: 22336 registros\n",
      "2025-08-06 12:41:16 - INFO -    - 2024-08: 22332 registros\n",
      "2025-08-06 12:41:16 - INFO -    - 2024-09: 20992 registros\n",
      "2025-08-06 12:41:16 - INFO -    - 2024-10: 21728 registros\n",
      "2025-08-06 12:41:16 - INFO -    - 2024-11: 21348 registros\n",
      "2025-08-06 12:41:16 - INFO -    - 2024-12: 21600 registros\n",
      "2025-08-06 12:41:16 - INFO -    - 2025-01: 21820 registros\n",
      "2025-08-06 12:41:16 - INFO -    - 2025-02: 19608 registros\n",
      "2025-08-06 12:41:16 - INFO -    - 2025-03: 22484 registros\n",
      "2025-08-06 12:41:16 - INFO -    - 2025-04: 21796 registros\n",
      "2025-08-06 12:41:16 - INFO -    - 2025-05: 20916 registros\n",
      "2025-08-06 12:41:16 - INFO -    - 2025-06: 19904 registros\n",
      "2025-08-06 12:41:16 - INFO -    - 2025-07: 19896 registros\n",
      "2025-08-06 12:41:16 - INFO - ✅ Verificación completada. Procediendo con la descarga...\n",
      "2025-08-06 12:41:16 - INFO - \n",
      "🌪️ Descargando datos actualizados del Soiling Kit...\n",
      "2025-08-06 12:41:16 - INFO - Iniciando descarga de datos del Soiling Kit...\n",
      "2025-08-06 12:41:16 - INFO - Consultando InfluxDB: bucket=PSDA, tables=['soilingkit'], attributes=['Isc(e)', 'Isc(p)', 'Te(C)', 'Tp(C)']\n",
      "2025-08-06 12:41:25 - INFO - Datos del Soiling Kit guardados exitosamente\n",
      "2025-08-06 12:41:25 - INFO - Total de registros: 69190\n",
      "2025-08-06 12:41:25 - INFO - Rango de fechas: 0 a 69189\n",
      "2025-08-06 12:41:25 - INFO - \n",
      "============================================================\n",
      "2025-08-06 12:41:25 - INFO - 🎉 ¡DESCARGA ACTUALIZADA COMPLETADA!\n",
      "2025-08-06 12:41:25 - INFO - 📂 Archivo: soiling_kit_raw_data.csv\n",
      "2025-08-06 12:41:25 - INFO - 📊 Tamaño: 9.21 MB\n",
      "2025-08-06 12:41:25 - INFO - 📈 Total de registros: 69,190\n",
      "2025-08-06 12:41:25 - INFO - 🗂️ Ubicación: /home/nicole/SR/SOILING/datos\n",
      "2025-08-06 12:41:25 - INFO - 🌪️ Datos incluyen: Isc(e), Isc(p), Te(C), Tp(C)\n",
      "2025-08-06 12:41:25 - INFO - 📅 Período: Julio 2024 - Julio 2025\n",
      "2025-08-06 12:41:25 - INFO - ============================================================\n",
      "2025-08-06 12:41:25 - INFO - \n",
      "📋 Muestra de los datos:\n",
      "2025-08-06 12:41:25 - INFO -    Columnas: ['Unnamed: 0', 'result', 'table', '_start', '_stop', '_time', '_measurement', 'device', 'Isc(e)', 'Isc(p)', 'Te(C)', 'Tp(C)']\n",
      "2025-08-06 12:41:25 - INFO -    Isc(e) rango: 1.029 - 1.032\n",
      "2025-08-06 12:41:25 - INFO -    Isc(p) rango: 1.078 - 1.080\n",
      "2025-08-06 12:41:25 - INFO - Conexión a InfluxDB cerrada.\n",
      "2025-08-06 12:41:25 - INFO - 🔌 Conexión cerrada\n",
      "2025-08-06 12:41:25 - INFO - \n",
      "🏁 DESCARGA ACTUALIZADA CON VERIFICACIÓN COMPLETADA\n"
     ]
    }
   ],
   "source": [
    "# 🌪️ DESCARGA ACTUALIZADA: SOILING KIT CON VERIFICACIÓN PREVIA DESDE INFLUXDB\n",
    "# ============================================================================\n",
    "# Descarga más ineficiente que la de Clickhouse, pero se mantiene para compatibilidad con el código de soiling_intercomparison.py\n",
    "# VERSIÓN MEJORADA: Incluye verificación de datos disponibles antes de descargar\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"🌪️ DESCARGA ACTUALIZADA: SOILING KIT CON VERIFICACIÓN\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "# Configurar fechas: desde julio 2024 hasta la actualidad\n",
    "START_DATE_SK = pd.to_datetime('01/07/2024', dayfirst=True).tz_localize('UTC')  # Desde julio 2024\n",
    "END_DATE_SK = pd.to_datetime('31/07/2025', dayfirst=True).tz_localize('UTC')    # Hasta julio 2025 (actualidad)\n",
    "\n",
    "logger.info(f\"📅 Rango actualizado: {START_DATE_SK.strftime('%Y-%m-%d')} a {END_DATE_SK.strftime('%Y-%m-%d')}\")\n",
    "logger.info(f\"📁 Directorio de salida: {OUTPUT_DIR}\")\n",
    "\n",
    "# Inicializar cliente InfluxDB\n",
    "influx_manager = InfluxDBManager(INFLUX_CONFIG)\n",
    "\n",
    "try:\n",
    "    if not influx_manager.connect():\n",
    "        logger.error(\"❌ No se pudo establecer conexión con InfluxDB\")\n",
    "    else:\n",
    "        logger.info(\"✅ Conexión a InfluxDB establecida\")\n",
    "        \n",
    "        # ===== VERIFICACIÓN PREVIA DE DATOS DISPONIBLES =====\n",
    "        logger.info(\"\\n🔍 VERIFICANDO DATOS DISPONIBLES ANTES DE DESCARGAR...\")\n",
    "        \n",
    "        # Consulta para verificar datos disponibles en el rango especificado\n",
    "        verification_query = f'''\n",
    "        from(bucket: \"PSDA\")\n",
    "            |> range(start: {START_DATE_SK.strftime(\"%Y-%m-%dT%H:%M:%SZ\")}, stop: {END_DATE_SK.strftime(\"%Y-%m-%dT%H:%M:%SZ\")})\n",
    "            |> filter(fn: (r) => r[\"_measurement\"] == \"soilingkit\")\n",
    "            |> filter(fn: (r) => r[\"_field\"] == \"Isc(e)\" or r[\"_field\"] == \"Isc(p)\" or r[\"_field\"] == \"Te(C)\" or r[\"_field\"] == \"Tp(C)\")\n",
    "            |> keep(columns: [\"_time\", \"_field\", \"_value\"])\n",
    "            |> sort(columns: [\"_time\"])\n",
    "        '''\n",
    "        \n",
    "        try:\n",
    "            logger.info(\"🔍 Ejecutando consulta de verificación...\")\n",
    "            verification_result = influx_manager.query_api.query_data_frame(verification_query)\n",
    "            \n",
    "            if verification_result.empty:\n",
    "                logger.warning(\"⚠️ NO SE ENCONTRARON DATOS en el rango especificado\")\n",
    "                logger.info(\"🔍 Intentando verificar datos disponibles en un rango más amplio...\")\n",
    "                \n",
    "                # Consulta más amplia para ver qué datos están disponibles\n",
    "                wide_query = f'''\n",
    "                from(bucket: \"PSDA\")\n",
    "                    |> range(start: 2024-01-01T00:00:00Z, stop: 2025-12-31T23:59:59Z)\n",
    "                    |> filter(fn: (r) => r[\"_measurement\"] == \"soilingkit\")\n",
    "                    |> filter(fn: (r) => r[\"_field\"] == \"Isc(e)\" or r[\"_field\"] == \"Isc(p)\" or r[\"_field\"] == \"Te(C)\" or r[\"_field\"] == \"Tp(C)\")\n",
    "                    |> keep(columns: [\"_time\", \"_field\", \"_value\"])\n",
    "                    |> sort(columns: [\"_time\"])\n",
    "                '''\n",
    "                \n",
    "                wide_result = influx_manager.query_api.query_data_frame(wide_query)\n",
    "                \n",
    "                if not wide_result.empty:\n",
    "                    logger.info(f\"📊 Datos disponibles en rango amplio: {len(wide_result)} registros\")\n",
    "                    logger.info(f\"📅 Fecha más antigua disponible: {wide_result['_time'].min()}\")\n",
    "                    logger.info(f\"📅 Fecha más reciente disponible: {wide_result['_time'].max()}\")\n",
    "                    \n",
    "                    # Analizar por campo\n",
    "                    if '_field' in wide_result.columns:\n",
    "                        field_counts = wide_result['_field'].value_counts()\n",
    "                        logger.info(\"📊 Registros por campo:\")\n",
    "                        for field, count in field_counts.items():\n",
    "                            logger.info(f\"   - {field}: {count} registros\")\n",
    "                    \n",
    "                    logger.warning(\"⚠️ El rango de fechas especificado no tiene datos\")\n",
    "                    logger.info(\"💡 Considera ajustar las fechas o usar el rango disponible\")\n",
    "                    \n",
    "                else:\n",
    "                    logger.error(\"❌ No se encontraron datos de Soiling Kit en ningún rango\")\n",
    "                    logger.error(\"🔧 Verifica la configuración de InfluxDB y el bucket PSDA\")\n",
    "                \n",
    "                # No proceder con la descarga\n",
    "                success = False\n",
    "                \n",
    "            else:\n",
    "                # Datos encontrados, mostrar información\n",
    "                logger.info(f\"✅ DATOS ENCONTRADOS: {len(verification_result)} registros\")\n",
    "                logger.info(f\"📅 Fecha más antigua: {verification_result['_time'].min()}\")\n",
    "                logger.info(f\"📅 Fecha más reciente: {verification_result['_time'].max()}\")\n",
    "                \n",
    "                # Analizar por campo\n",
    "                if '_field' in verification_result.columns:\n",
    "                    field_counts = verification_result['_field'].value_counts()\n",
    "                    logger.info(\"📊 Registros por campo:\")\n",
    "                    for field, count in field_counts.items():\n",
    "                        logger.info(f\"   - {field}: {count} registros\")\n",
    "                \n",
    "                # Mostrar estadísticas por mes\n",
    "                verification_result['_time'] = pd.to_datetime(verification_result['_time'])\n",
    "                verification_result['year_month'] = verification_result['_time'].dt.to_period('M')\n",
    "                monthly_counts = verification_result.groupby('year_month').size()\n",
    "                \n",
    "                logger.info(\"\\n📈 Registros por mes:\")\n",
    "                for month, count in monthly_counts.items():\n",
    "                    logger.info(f\"   - {month}: {count} registros\")\n",
    "                \n",
    "                logger.info(\"✅ Verificación completada. Procediendo con la descarga...\")\n",
    "                \n",
    "                # ===== EJECUTAR DESCARGA DEL SOILING KIT =====\n",
    "                logger.info(\"\\n🌪️ Descargando datos actualizados del Soiling Kit...\")\n",
    "                success = download_soiling_kit(influx_manager, START_DATE_SK, END_DATE_SK, OUTPUT_DIR)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Error en la verificación: {e}\")\n",
    "            logger.info(\"⚠️ Procediendo con descarga sin verificación...\")\n",
    "            success = download_soiling_kit(influx_manager, START_DATE_SK, END_DATE_SK, OUTPUT_DIR)\n",
    "        \n",
    "        # ===== VERIFICAR RESULTADO DE LA DESCARGA =====\n",
    "        if success:\n",
    "            # Verificar archivo generado\n",
    "            output_file = os.path.join(OUTPUT_DIR, 'soiling_kit_raw_data.csv')\n",
    "            if os.path.exists(output_file):\n",
    "                file_size_mb = os.path.getsize(output_file) / (1024*1024)\n",
    "                total_lines = sum(1 for line in open(output_file)) - 1\n",
    "                \n",
    "                logger.info(\"\\n\" + \"=\"*60)\n",
    "                logger.info(\"🎉 ¡DESCARGA ACTUALIZADA COMPLETADA!\")\n",
    "                logger.info(f\"📂 Archivo: soiling_kit_raw_data.csv\")\n",
    "                logger.info(f\"📊 Tamaño: {file_size_mb:.2f} MB\")\n",
    "                logger.info(f\"📈 Total de registros: {total_lines:,}\")\n",
    "                logger.info(f\"🗂️ Ubicación: {OUTPUT_DIR}\")\n",
    "                logger.info(\"🌪️ Datos incluyen: Isc(e), Isc(p), Te(C), Tp(C)\")\n",
    "                logger.info(\"📅 Período: Julio 2024 - Julio 2025\")\n",
    "                logger.info(\"=\"*60)\n",
    "                \n",
    "                # Mostrar muestra de los datos\n",
    "                try:\n",
    "                    df_sample = pd.read_csv(output_file, nrows=5)\n",
    "                    logger.info(\"\\n📋 Muestra de los datos:\")\n",
    "                    logger.info(f\"   Columnas: {list(df_sample.columns)}\")\n",
    "                    if 'Isc(e)' in df_sample.columns:\n",
    "                        logger.info(f\"   Isc(e) rango: {df_sample['Isc(e)'].min():.3f} - {df_sample['Isc(e)'].max():.3f}\")\n",
    "                    if 'Isc(p)' in df_sample.columns:\n",
    "                        logger.info(f\"   Isc(p) rango: {df_sample['Isc(p)'].min():.3f} - {df_sample['Isc(p)'].max():.3f}\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"⚠️ Error al mostrar muestra: {e}\")\n",
    "            else:\n",
    "                logger.warning(\"⚠️ Archivo no encontrado después de la descarga\")\n",
    "        else:\n",
    "            logger.error(\"❌ Error en la descarga del Soiling Kit\")\n",
    "            \n",
    "except Exception as e:\n",
    "    logger.error(f\"❌ Error general: {e}\")\n",
    "    import traceback\n",
    "    logger.error(f\"🔍 Detalles:\\n{traceback.format_exc()}\")\n",
    "    \n",
    "finally:\n",
    "    influx_manager.disconnect()\n",
    "    logger.info(\"🔌 Conexión cerrada\")\n",
    "\n",
    "logger.info(\"\\n🏁 DESCARGA ACTUALIZADA CON VERIFICACIÓN COMPLETADA\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 11:47:24 - INFO - \n",
      "================================================================================\n",
      "2025-06-19 11:47:24 - INFO - 🚀 INICIANDO DESCARGA COMPLETA DE TODOS LOS DATOS\n",
      "2025-06-19 11:47:24 - INFO - ================================================================================\n",
      "2025-06-19 11:47:24 - INFO - 📅 Rango general: 2024-07-01 a 2025-12-31\n",
      "2025-06-19 11:47:24 - INFO - 📁 Directorio de salida: /home/nicole/SR/SOILING/datos\n",
      "2025-06-19 11:47:24 - INFO - 📂 RefCells usará fecha específica: 2024-07-23\n",
      "2025-06-19 11:47:24 - INFO - Iniciando proceso de descarga de datos...\n",
      "2025-06-19 11:47:24 - INFO - \n",
      "Iniciando descarga de datos desde Clickhouse...\n",
      "2025-06-19 11:47:24 - INFO - Iniciando descarga de datos IV600...\n",
      "2025-06-19 11:47:24 - INFO - Conectando a Clickhouse...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 11:47:25 - INFO - Conexión a Clickhouse establecida\n",
      "2025-06-19 11:47:25 - INFO - Consultando datos IV600...\n",
      "2025-06-19 11:47:26 - INFO - Datos obtenidos: 2029 registros\n",
      "2025-06-19 11:47:26 - INFO - Procesando datos...\n",
      "2025-06-19 11:47:26 - INFO - Creando DataFrame...\n",
      "2025-06-19 11:47:26 - INFO - Rango de fechas en los datos:\n",
      "2025-06-19 11:47:26 - INFO - Fecha más antigua: 2024-09-24 12:16:00+00:00\n",
      "2025-06-19 11:47:26 - INFO - Fecha más reciente: 2025-04-22 12:43:00+00:00\n",
      "2025-06-19 11:47:26 - INFO - Filtrando datos entre 2024-07-01 00:00:00+00:00 y 2025-12-31 00:00:00+00:00...\n",
      "2025-06-19 11:47:26 - INFO - Se encontraron 2029 registros en el rango especificado.\n",
      "2025-06-19 11:47:26 - INFO - Guardando datos en: /home/nicole/SR/SOILING/datos/raw_iv600_data.csv\n",
      "2025-06-19 11:47:26 - INFO - Datos guardados exitosamente. Total de registros: 2029\n",
      "2025-06-19 11:47:26 - INFO - Rango de fechas: 2024-09-24 12:16:00+00:00 a 2025-04-22 12:43:00+00:00\n",
      "2025-06-19 11:47:26 - INFO - Cerrando conexión a Clickhouse...\n",
      "2025-06-19 11:47:26 - INFO - Conexión a Clickhouse cerrada\n",
      "2025-06-19 11:47:26 - INFO - Descarga IV600: Exitosa\n",
      "2025-06-19 11:47:26 - INFO - \n",
      "Iniciando descargas desde InfluxDB...\n",
      "2025-06-19 11:47:26 - INFO - Cliente InfluxDB y query_api inicializados.\n",
      "2025-06-19 11:47:26 - INFO - \n",
      "Procesando PV Glasses...\n",
      "2025-06-19 11:47:26 - INFO - Iniciando descarga de datos PV Glasses...\n",
      "2025-06-19 11:47:26 - INFO - Consultando InfluxDB: bucket=meteo_psda, tables=['6852_Ftc'], attributes=['R_FC1_Avg', 'R_FC2_Avg', 'R_FC3_Avg', 'R_FC4_Avg', 'R_FC5_Avg']\n",
      "2025-06-19 11:47:48 - INFO - Datos PV Glasses guardados exitosamente\n",
      "2025-06-19 11:47:48 - INFO - Total de registros: 83157\n",
      "2025-06-19 11:47:48 - INFO - Rango de fechas: 2024-09-05 13:00:00+00:00 a 2025-06-19 15:40:00+00:00\n",
      "2025-06-19 11:47:48 - INFO - Descarga PV Glasses: Exitosa\n",
      "2025-06-19 11:47:48 - INFO - \n",
      "Procesando DustIQ...\n",
      "2025-06-19 11:47:48 - INFO - Iniciando descarga de datos DustIQ...\n",
      "2025-06-19 11:47:48 - INFO - Consultando InfluxDB: bucket=PSDA, tables=['DustIQ'], attributes=['SR_C11_Avg', 'SR_C12_Avg']\n",
      "2025-06-19 11:48:19 - INFO - Datos DustIQ guardados exitosamente\n",
      "2025-06-19 11:48:19 - INFO - Total de registros: 492075\n",
      "2025-06-19 11:48:19 - INFO - Rango de fechas: 0 a 492074\n",
      "2025-06-19 11:48:19 - INFO - Descarga DustIQ: Exitosa\n",
      "2025-06-19 11:48:19 - INFO - \n",
      "Procesando PVStand...\n",
      "2025-06-19 11:48:19 - INFO - Iniciando descarga de datos PVStand...\n",
      "2025-06-19 11:48:19 - INFO - Consultando InfluxDB: bucket=PSDA, tables=['PERC1_fixed_1MD43420160719', 'PERC2_fixed_1MD43920160719'], attributes=['Imax', 'Umax', 'Pmax']\n",
      "2025-06-19 11:48:31 - INFO - Datos PVStand guardados exitosamente\n",
      "2025-06-19 11:48:31 - INFO - Total de registros: 199254\n",
      "2025-06-19 11:48:31 - INFO - Rango de fechas: 0 a 199253\n",
      "2025-06-19 11:48:31 - INFO - Descarga PVStand: Exitosa\n",
      "2025-06-19 11:48:31 - INFO - \n",
      "Procesando celdas de referencia...\n",
      "2025-06-19 11:48:31 - INFO - Usando fecha específica para RefCells: 2024-07-23 00:00:00+00:00\n",
      "2025-06-19 11:48:31 - INFO - Iniciando descarga de datos de celdas de referencia...\n",
      "2025-06-19 11:48:31 - INFO - Consultando InfluxDB: bucket=PSDA, tables=['RefCellsFixed'], attributes=['1RC410(w.m-2)', '1RC411(w.m-2)', '1RC412(w.m-2)']\n",
      "2025-06-19 11:49:42 - INFO - Consultando InfluxDB: bucket=PSDA, tables=['fixed_plant_atamo_1'], attributes=['1RC410(w.m-2)', '1RC411(w.m-2)', '1RC412(w.m-2)']\n",
      "2025-06-19 11:49:58 - INFO - Procesando DataFrame 1 con 2303110 registros\n",
      "2025-06-19 11:49:59 - INFO - DataFrame 1 procesado: 476185 registros después del filtrado\n",
      "2025-06-19 11:49:59 - INFO - Procesando DataFrame 2 con 687839 registros\n",
      "2025-06-19 11:49:59 - INFO - DataFrame 2 procesado: 141905 registros después del filtrado\n",
      "2025-06-19 11:50:17 - INFO - Datos de celdas de referencia guardados exitosamente\n",
      "2025-06-19 11:50:17 - INFO - Total de registros: 618090\n",
      "2025-06-19 11:50:17 - INFO - Columnas guardadas: ['_time', '1RC410(w.m-2)', '1RC411(w.m-2)', '1RC412(w.m-2)']\n",
      "2025-06-19 11:50:17 - INFO - Rango de fechas: 2024-07-23 13:00:01+00:00 a 2025-06-19 15:49:04.432230+00:00\n",
      "2025-06-19 11:50:18 - INFO - Descarga celdas de referencia: Exitosa\n",
      "2025-06-19 11:50:18 - INFO - Aplicando procesamiento adicional a RefCells...\n",
      "2025-06-19 11:50:20 - INFO - Aplicando resample por minuto a RefCells...\n",
      "2025-06-19 11:50:24 - INFO - RefCells procesadas: 618090 → 88464 registros\n",
      "2025-06-19 11:50:24 - INFO - \n",
      "Procesando Soiling Kit...\n",
      "2025-06-19 11:50:24 - INFO - Iniciando descarga de datos del Soiling Kit...\n",
      "2025-06-19 11:50:24 - INFO - Consultando InfluxDB: bucket=PSDA, tables=['soilingkit'], attributes=['Isc(e)', 'Isc(p)', 'Te(C)', 'Tp(C)']\n",
      "2025-06-19 11:50:32 - INFO - Datos del Soiling Kit guardados exitosamente\n",
      "2025-06-19 11:50:32 - INFO - Total de registros: 58325\n",
      "2025-06-19 11:50:32 - INFO - Rango de fechas: 0 a 58324\n",
      "2025-06-19 11:50:32 - INFO - Descarga Soiling Kit: Exitosa\n",
      "2025-06-19 11:50:32 - INFO - \n",
      "Procesando Temperatura de Módulos (PT100)...\n",
      "2025-06-19 11:50:32 - INFO - Iniciando descarga de datos de Temperatura de Módulos (PT100)...\n",
      "2025-06-19 11:50:32 - INFO - Consultando datos de temperatura de ['TempModFixed']...\n",
      "2025-06-19 11:50:32 - INFO - Consultando InfluxDB: bucket=PSDA, tables=['TempModFixed'], attributes=['1TE416(C)', '1TE417(C)', '1TE418(C)', '1TE419(C)']\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# === EJECUCIÓN PRINCIPAL: DESCARGA COMPLETA DE TODOS LOS DATOS ===\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"🚀 INICIANDO DESCARGA COMPLETA DE TODOS LOS DATOS\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "logger.info(f\"📅 Rango general: {START_DATE.strftime('%Y-%m-%d')} a {END_DATE.strftime('%Y-%m-%d')}\")\n",
    "logger.info(f\"📁 Directorio de salida: {OUTPUT_DIR}\")\n",
    "logger.info(f\"📂 RefCells usará fecha específica: 2024-07-23\")\n",
    "\n",
    "# Ejecutar descarga completa\n",
    "results = download_all_data(START_DATE, END_DATE, OUTPUT_DIR)\n",
    "\n",
    "# Resumen final detallado\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"📊 RESUMEN FINAL DE DESCARGAS\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "total_successful = sum(1 for v in results.values() if v)\n",
    "total_processes = len(results)\n",
    "\n",
    "logger.info(f\"✅ Procesos exitosos: {total_successful}/{total_processes}\")\n",
    "\n",
    "# Detalle por proceso\n",
    "status_emoji = lambda x: \"✅\" if x else \"❌\"\n",
    "for process, success in results.items():\n",
    "    process_name = {\n",
    "        'iv600': 'IV600 (Clickhouse)',\n",
    "        'pv_glasses': 'PV Glasses',\n",
    "        'dustiq': 'DustIQ', \n",
    "        'pvstand': 'PVStand',\n",
    "        'refcells': 'RefCells (Descarga)',\n",
    "        'refcells_processed': 'RefCells (Procesamiento)',\n",
    "        'soiling_kit': 'Soiling Kit',\n",
    "        'temp_mod_fixed': 'Temperatura Módulos'\n",
    "    }.get(process, process)\n",
    "    \n",
    "    logger.info(f\"{status_emoji(success)} {process_name}\")\n",
    "\n",
    "if total_successful == total_processes:\n",
    "    logger.info(\"\\n🎉 ¡TODOS LOS PROCESOS COMPLETADOS EXITOSAMENTE!\")\n",
    "    logger.info(\"📂 Archivos generados en: \" + OUTPUT_DIR)\n",
    "    logger.info(\"🔍 RefCells procesado con resample 1min y guardado como refcells_data.csv\")\n",
    "else:\n",
    "    logger.warning(f\"\\n⚠️ {total_processes - total_successful} proceso(s) fallaron\")\n",
    "    logger.info(\"🔧 Revisa los logs anteriores para detalles de errores\")\n",
    "\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(\"🏁 PROCESO COMPLETO FINALIZADO\")\n",
    "logger.info(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Para descargar todo de una vez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 12:12:24 - INFO - ✅ Función download_refcells() definida correctamente\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_all_data_optimized(start_date, end_date, output_dir):\n",
    "    \"\"\"Función principal optimizada para manejo de memoria.\"\"\"\n",
    "    import gc\n",
    "    \n",
    "    logger.info(\"Iniciando proceso de descarga optimizado...\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    results = {}\n",
    "    \n",
    "    try:\n",
    "        # IV600 (Clickhouse)\n",
    "        logger.info(\"\\n🔹 Descargando IV600...\")\n",
    "        gc.collect()\n",
    "        results['iv600'] = download_iv600(start_date, end_date, output_dir)\n",
    "        logger.info(f\"IV600: {'✅' if results['iv600'] else '❌'}\")\n",
    "        gc.collect()\n",
    "        \n",
    "        # InfluxDB Manager - reutilizar conexión pero limpiar datos\n",
    "        influx_manager = InfluxDBManager(INFLUX_CONFIG)\n",
    "        if not influx_manager.connect():\n",
    "            return results\n",
    "            \n",
    "        try:\n",
    "            # PV Glasses\n",
    "            logger.info(\"\\n🔹 Descargando PV Glasses...\")\n",
    "            gc.collect()\n",
    "            results['pv_glasses'] = download_pv_glasses(influx_manager, start_date, end_date, output_dir)\n",
    "            logger.info(f\"PV Glasses: {'✅' if results['pv_glasses'] else '❌'}\")\n",
    "            gc.collect()\n",
    "            \n",
    "            # DustIQ\n",
    "            logger.info(\"\\n🔹 Descargando DustIQ...\")\n",
    "            gc.collect()\n",
    "            results['dustiq'] = download_dustiq(influx_manager, start_date, end_date, output_dir)\n",
    "            logger.info(f\"DustIQ: {'✅' if results['dustiq'] else '❌'}\")\n",
    "            gc.collect()\n",
    "            \n",
    "            # PVStand\n",
    "            logger.info(\"\\n🔹 Descargando PVStand...\")\n",
    "            gc.collect()\n",
    "            results['pvstand'] = download_pvstand(influx_manager, start_date, end_date, output_dir)\n",
    "            logger.info(f\"PVStand: {'✅' if results['pvstand'] else '❌'}\")\n",
    "            gc.collect()\n",
    "            \n",
    "            # RefCells con procesamiento optimizado\n",
    "            logger.info(\"\\n🔹 Descargando RefCells...\")\n",
    "            refcells_start = pd.to_datetime('23/07/2024', dayfirst=True).tz_localize('UTC')\n",
    "            gc.collect()\n",
    "            results['refcells'] = download_refcells(influx_manager, refcells_start, end_date, output_dir)\n",
    "            logger.info(f\"RefCells Descarga: {'✅' if results['refcells'] else '❌'}\")\n",
    "            \n",
    "            # Procesamiento RefCells\n",
    "            if results['refcells']:\n",
    "                logger.info(\"🔹 Procesando RefCells...\")\n",
    "                try:\n",
    "                    input_filepath = os.path.join(output_dir, 'refcells_data.csv')\n",
    "                    \n",
    "                    # Procesar en chunks\n",
    "                    chunks = []\n",
    "                    for chunk in pd.read_csv(input_filepath, chunksize=30000):\n",
    "                        chunk['_time'] = pd.to_datetime(chunk['_time'])\n",
    "                        chunk = chunk.set_index('_time')\n",
    "                        chunks.append(chunk)\n",
    "                    \n",
    "                    df = pd.concat(chunks)\n",
    "                    del chunks\n",
    "                    gc.collect()\n",
    "                    \n",
    "                    df_resampled = df.resample('1min').mean().dropna(how='all')\n",
    "                    del df\n",
    "                    gc.collect()\n",
    "                    \n",
    "                    df_resampled.to_csv(os.path.join(output_dir, 'refcells_data.csv'))\n",
    "                    results['refcells_processed'] = True\n",
    "                    logger.info(f\"RefCells Procesamiento: ✅ ({len(df_resampled)} registros)\")\n",
    "                    del df_resampled\n",
    "                    gc.collect()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error procesando RefCells: {e}\")\n",
    "                    results['refcells_processed'] = False\n",
    "            \n",
    "            # Soiling Kit\n",
    "            logger.info(\"\\n🔹 Descargando Soiling Kit...\")\n",
    "            gc.collect()\n",
    "            results['soiling_kit'] = download_soiling_kit(influx_manager, start_date, end_date, output_dir)\n",
    "            logger.info(f\"Soiling Kit: {'✅' if results['soiling_kit'] else '❌'}\")\n",
    "            gc.collect()\n",
    "            \n",
    "            # Temperatura\n",
    "            logger.info(\"\\n🔹 Descargando Temperatura...\")\n",
    "            gc.collect()\n",
    "            results['temp_mod_fixed'] = download_temp_mod_fixed(influx_manager, start_date, end_date, output_dir)\n",
    "            logger.info(f\"Temperatura: {'✅' if results['temp_mod_fixed'] else '❌'}\")\n",
    "            gc.collect()\n",
    "            \n",
    "        finally:\n",
    "            influx_manager.disconnect()\n",
    "            \n",
    "        # Resumen\n",
    "        successful = sum(1 for v in results.values() if v)\n",
    "        logger.info(f\"\\n🎉 Completado: {successful}/{len(results)} exitosos\")\n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error general: {e}\")\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 12:12:35 - INFO - 🔋 Iniciando descarga de RefCells...\n",
      "2025-08-07 12:12:35 - INFO - Cliente InfluxDB y query_api inicializados.\n",
      "2025-08-07 12:12:35 - INFO - ✅ Conexión establecida\n",
      "2025-08-07 12:12:35 - INFO - 🔋 Iniciando descarga optimizada de datos de celdas de referencia...\n",
      "2025-08-07 12:12:35 - INFO - 🚀 PASO 1: DESCARGANDO DATOS (OPTIMIZADO)...\n",
      "2025-08-07 12:12:35 - INFO - 📊 Consultando InfluxDB: bucket=PSDA, tables=['fixed_plant_atamo_1'], attributes=['1RC410(w.m-2)', '1RC411(w.m-2)', '1RC412(w.m-2)']\n",
      "2025-08-07 12:12:35 - INFO - Consultando InfluxDB: bucket=PSDA, tables=['fixed_plant_atamo_1'], attributes=['1RC410(w.m-2)', '1RC411(w.m-2)', '1RC412(w.m-2)']\n",
      "2025-08-07 12:13:39 - INFO - 📊 Datos obtenidos: 1,062,471 registros\n",
      "2025-08-07 12:13:39 - INFO - 📊 Columnas originales: ['result', 'table', '_start', '_stop', '_time', '_measurement', '1RC410(w.m-2)', '1RC411(w.m-2)', '1RC412(w.m-2)']\n",
      "2025-08-07 12:13:39 - INFO - 🔄 PASO 2: PROCESANDO FORMATO DE DATOS...\n",
      "2025-08-07 12:13:39 - INFO - 🔄 Convirtiendo fechas con manejo robusto...\n",
      "2025-08-07 12:13:39 - INFO - ✅ Fechas convertidas con formato UTC\n",
      "2025-08-07 12:13:39 - INFO - 🔄 Separando datos por fecha con manejo de zona horaria...\n",
      "2025-08-07 12:13:39 - INFO - 📅 Fecha de cambio (con zona horaria): 2024-12-12 00:00:00+00:00\n",
      "2025-08-07 12:13:39 - INFO - 📊 Registros antes del 12-12-2024: 185,939\n",
      "2025-08-07 12:13:39 - INFO - 📊 Registros desde el 12-12-2024: 876,532\n",
      "2025-08-07 12:13:39 - INFO - 🔄 Procesando datos desde 12-12-2024 (formato long real)...\n",
      "2025-08-07 12:13:39 - INFO - 📊 Timestamps únicos desde 12-12-2024: 876,532\n",
      "2025-08-07 12:13:39 - INFO - 📊 Promedio de filas por timestamp: 1.00\n",
      "2025-08-07 12:13:39 - INFO - 📊 Filas con un solo valor (formato long): 908880\n",
      "2025-08-07 12:13:39 - WARNING - ⚠️ Formato long no detectado - usando datos originales\n",
      "2025-08-07 12:13:39 - INFO - 🔄 PASO 3: APLICANDO PROCESAMIENTO ADICIONAL...\n",
      "2025-08-07 12:13:39 - INFO - 🕐 Filtrando por ventana horaria (13:00 - 18:00)...\n",
      "2025-08-07 12:13:40 - INFO - 📊 Registros después del filtro horario: 219,666\n",
      "2025-08-07 12:13:40 - INFO - 📊 Aplicando resample por minuto...\n",
      "2025-08-07 12:13:40 - INFO - 📊 Registros después del resample: 101,147\n",
      "2025-08-07 12:13:40 - INFO - 💾 PASO 4: GUARDANDO ARCHIVO FINAL...\n",
      "2025-08-07 12:13:42 - INFO - 🔍 PASO 5: VERIFICACIÓN FINAL...\n",
      "2025-08-07 12:13:42 - INFO - ✅ Datos de celdas de referencia procesados exitosamente\n",
      "2025-08-07 12:13:42 - INFO - 📂 Archivo: refcells_data.csv\n",
      "2025-08-07 12:13:42 - INFO - 📊 Tamaño: 5.44 MB\n",
      "2025-08-07 12:13:42 - INFO - 📈 Total de registros: 101,147\n",
      "2025-08-07 12:13:42 - INFO - 📊 Columnas: ['timestamp', '1RC410(w.m-2)', '1RC411(w.m-2)', '1RC412(w.m-2)']\n",
      "2025-08-07 12:13:42 - INFO - 📅 Rango de fechas: 2024-07-23 13:00:00+00:00 a 2025-08-06 13:53:00+00:00\n",
      "2025-08-07 12:13:42 - INFO - 📋 Muestra de los primeros datos:\n",
      "2025-08-07 12:13:42 - INFO -                   timestamp  1RC410(w.m-2)  1RC411(w.m-2)  1RC412(w.m-2)\n",
      "0 2024-07-23 13:00:00+00:00         472.63         484.35         477.15\n",
      "1 2024-07-23 13:01:00+00:00         477.36         488.82         480.95\n",
      "2 2024-07-23 13:02:00+00:00         480.63         492.31         486.19\n",
      "2025-08-07 12:13:44 - INFO - 🎉 ¡Descarga completada exitosamente!\n",
      "2025-08-07 12:13:44 - INFO - Conexión a InfluxDB cerrada.\n",
      "2025-08-07 12:13:44 - INFO - 🔌 Conexión cerrada\n",
      "2025-08-07 12:13:44 - INFO - 🏁 Proceso finalizado\n"
     ]
    }
   ],
   "source": [
    "# 🚀 **EJECUTAR DESCARGA DE REFCELLS** - CELDA SIMPLE\n",
    "# ============================================================================\n",
    "# Esta es la ÚNICA celda que debes ejecutar para descargar RefCells\n",
    "\n",
    "logger.info(\"🔋 Iniciando descarga de RefCells...\")\n",
    "\n",
    "# Configurar fechas\n",
    "refcells_start_date = pd.to_datetime('23/07/2024', dayfirst=True).tz_localize('UTC')\n",
    "end_date = pd.to_datetime('31/12/2025', dayfirst=True).tz_localize('UTC')\n",
    "\n",
    "# Inicializar cliente InfluxDB\n",
    "influx_manager = InfluxDBManager(INFLUX_CONFIG)\n",
    "\n",
    "try:\n",
    "    if not influx_manager.connect():\n",
    "        logger.error(\"❌ No se pudo conectar a InfluxDB\")\n",
    "    else:\n",
    "        logger.info(\"✅ Conexión establecida\")\n",
    "        \n",
    "        # 🎯 EJECUTAR LA FUNCIÓN (definida en Celda 17)\n",
    "        success = download_refcells(influx_manager, refcells_start_date, end_date, OUTPUT_DIR)\n",
    "        \n",
    "        if success:\n",
    "            logger.info(\"🎉 ¡Descarga completada exitosamente!\")\n",
    "        else:\n",
    "            logger.error(\"❌ Error en la descarga\")\n",
    "            \n",
    "finally:\n",
    "    influx_manager.disconnect()\n",
    "    logger.info(\"🔌 Conexión cerrada\")\n",
    "\n",
    "logger.info(\"🏁 Proceso finalizado\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 11:58:17 - INFO - 🚀 Iniciando descarga completa optimizada...\n",
      "2025-06-19 11:58:17 - INFO - Iniciando proceso de descarga optimizado...\n",
      "2025-06-19 11:58:17 - INFO - \n",
      "🔹 Descargando IV600...\n",
      "2025-06-19 11:58:17 - INFO - Iniciando descarga de datos IV600...\n",
      "2025-06-19 11:58:17 - INFO - Conectando a Clickhouse...\n",
      "2025-06-19 11:58:17 - INFO - Conexión a Clickhouse establecida\n",
      "2025-06-19 11:58:17 - INFO - Consultando datos IV600...\n",
      "2025-06-19 11:58:18 - INFO - Datos obtenidos: 2029 registros\n",
      "2025-06-19 11:58:18 - INFO - Procesando datos...\n",
      "2025-06-19 11:58:18 - INFO - Creando DataFrame...\n",
      "2025-06-19 11:58:18 - INFO - Rango de fechas en los datos:\n",
      "2025-06-19 11:58:18 - INFO - Fecha más antigua: 2024-09-24 12:16:00+00:00\n",
      "2025-06-19 11:58:18 - INFO - Fecha más reciente: 2025-04-22 12:43:00+00:00\n",
      "2025-06-19 11:58:18 - INFO - Filtrando datos entre 2024-07-01 00:00:00+00:00 y 2025-12-31 00:00:00+00:00...\n",
      "2025-06-19 11:58:18 - INFO - Se encontraron 2029 registros en el rango especificado.\n",
      "2025-06-19 11:58:18 - INFO - Guardando datos en: /home/nicole/SR/SOILING/datos/raw_iv600_data.csv\n",
      "2025-06-19 11:58:18 - INFO - Datos guardados exitosamente. Total de registros: 2029\n",
      "2025-06-19 11:58:18 - INFO - Rango de fechas: 2024-09-24 12:16:00+00:00 a 2025-04-22 12:43:00+00:00\n",
      "2025-06-19 11:58:18 - INFO - Cerrando conexión a Clickhouse...\n",
      "2025-06-19 11:58:18 - INFO - Conexión a Clickhouse cerrada\n",
      "2025-06-19 11:58:18 - INFO - IV600: ✅\n",
      "2025-06-19 11:58:18 - INFO - Cliente InfluxDB y query_api inicializados.\n",
      "2025-06-19 11:58:18 - INFO - \n",
      "🔹 Descargando PV Glasses...\n",
      "2025-06-19 11:58:18 - INFO - Iniciando descarga de datos PV Glasses...\n",
      "2025-06-19 11:58:18 - INFO - Consultando InfluxDB: bucket=meteo_psda, tables=['6852_Ftc'], attributes=['R_FC1_Avg', 'R_FC2_Avg', 'R_FC3_Avg', 'R_FC4_Avg', 'R_FC5_Avg']\n",
      "2025-06-19 11:58:52 - INFO - Datos PV Glasses guardados exitosamente\n",
      "2025-06-19 11:58:52 - INFO - Total de registros: 83167\n",
      "2025-06-19 11:58:52 - INFO - Rango de fechas: 2024-09-05 13:00:00+00:00 a 2025-06-19 15:50:00+00:00\n",
      "2025-06-19 11:58:53 - INFO - PV Glasses: ✅\n",
      "2025-06-19 11:58:53 - INFO - \n",
      "🔹 Descargando DustIQ...\n",
      "2025-06-19 11:58:53 - INFO - Iniciando descarga de datos DustIQ...\n",
      "2025-06-19 11:58:53 - INFO - Consultando InfluxDB: bucket=PSDA, tables=['DustIQ'], attributes=['SR_C11_Avg', 'SR_C12_Avg']\n",
      "2025-06-19 11:59:33 - INFO - Datos DustIQ guardados exitosamente\n",
      "2025-06-19 11:59:33 - INFO - Total de registros: 492075\n",
      "2025-06-19 11:59:33 - INFO - Rango de fechas: 0 a 492074\n",
      "2025-06-19 11:59:33 - INFO - DustIQ: ✅\n",
      "2025-06-19 11:59:34 - INFO - \n",
      "🔹 Descargando PVStand...\n",
      "2025-06-19 11:59:34 - INFO - Iniciando descarga de datos PVStand...\n",
      "2025-06-19 11:59:34 - INFO - Consultando InfluxDB: bucket=PSDA, tables=['PERC1_fixed_1MD43420160719', 'PERC2_fixed_1MD43920160719'], attributes=['Imax', 'Umax', 'Pmax']\n",
      "2025-06-19 11:59:51 - INFO - Datos PVStand guardados exitosamente\n",
      "2025-06-19 11:59:51 - INFO - Total de registros: 199258\n",
      "2025-06-19 11:59:51 - INFO - Rango de fechas: 0 a 199257\n",
      "2025-06-19 11:59:51 - INFO - PVStand: ✅\n",
      "2025-06-19 11:59:51 - INFO - \n",
      "🔹 Descargando RefCells...\n",
      "2025-06-19 11:59:51 - INFO - Iniciando descarga de datos de celdas de referencia...\n",
      "2025-06-19 11:59:51 - INFO - Consultando InfluxDB: bucket=PSDA, tables=['RefCellsFixed'], attributes=['1RC410(w.m-2)', '1RC411(w.m-2)', '1RC412(w.m-2)']\n",
      "2025-06-19 12:01:14 - INFO - Consultando InfluxDB: bucket=PSDA, tables=['fixed_plant_atamo_1'], attributes=['1RC410(w.m-2)', '1RC411(w.m-2)', '1RC412(w.m-2)']\n",
      "2025-06-19 12:01:45 - INFO - Procesando DataFrame 1 con 2303110 registros\n",
      "2025-06-19 12:01:51 - INFO - DataFrame 1 procesado: 476185 registros después del filtrado\n",
      "2025-06-19 12:01:51 - INFO - Procesando DataFrame 2 con 687875 registros\n",
      "2025-06-19 12:01:51 - INFO - DataFrame 2 procesado: 141941 registros después del filtrado\n",
      "2025-06-19 12:02:14 - INFO - Datos de celdas de referencia guardados exitosamente\n",
      "2025-06-19 12:02:14 - INFO - Total de registros: 618126\n",
      "2025-06-19 12:02:14 - INFO - Columnas guardadas: ['_time', '1RC410(w.m-2)', '1RC411(w.m-2)', '1RC412(w.m-2)']\n",
      "2025-06-19 12:02:15 - INFO - Rango de fechas: 2024-07-23 13:00:01+00:00 a 2025-06-19 16:01:03.455138+00:00\n",
      "2025-06-19 12:02:17 - INFO - RefCells Descarga: ✅\n",
      "2025-06-19 12:02:17 - INFO - 🔹 Procesando RefCells...\n",
      "2025-06-19 12:02:32 - INFO - RefCells Procesamiento: ✅ (88476 registros)\n",
      "2025-06-19 12:02:32 - INFO - \n",
      "🔹 Descargando Soiling Kit...\n",
      "2025-06-19 12:02:33 - INFO - Iniciando descarga de datos del Soiling Kit...\n",
      "2025-06-19 12:02:33 - INFO - Consultando InfluxDB: bucket=PSDA, tables=['soilingkit'], attributes=['Isc(e)', 'Isc(p)', 'Te(C)', 'Tp(C)']\n",
      "2025-06-19 12:02:41 - INFO - Datos del Soiling Kit guardados exitosamente\n",
      "2025-06-19 12:02:41 - INFO - Total de registros: 58325\n",
      "2025-06-19 12:02:41 - INFO - Rango de fechas: 0 a 58324\n",
      "2025-06-19 12:02:41 - INFO - Soiling Kit: ✅\n",
      "2025-06-19 12:02:41 - INFO - \n",
      "🔹 Descargando Temperatura...\n",
      "2025-06-19 12:02:41 - INFO - Iniciando descarga de datos de Temperatura de Módulos (PT100)...\n",
      "2025-06-19 12:02:41 - INFO - Consultando datos de temperatura de ['TempModFixed']...\n",
      "2025-06-19 12:02:41 - INFO - Consultando InfluxDB: bucket=PSDA, tables=['TempModFixed'], attributes=['1TE416(C)', '1TE417(C)', '1TE418(C)', '1TE419(C)']\n",
      "2025-06-19 12:04:42 - INFO - Datos de ['TempModFixed'] procesados. Registros: 552142\n",
      "2025-06-19 12:04:42 - INFO - Consultando datos de temperatura de ['fixed_plant_atamo_1']...\n",
      "2025-06-19 12:04:42 - INFO - Consultando InfluxDB: bucket=PSDA, tables=['fixed_plant_atamo_1'], attributes=['1TE416(C)', '1TE417(C)', '1TE418(C)', '1TE419(C)']\n",
      "2025-06-19 12:05:04 - INFO - Datos de ['fixed_plant_atamo_1'] procesados. Registros: 188574\n",
      "2025-06-19 12:05:32 - INFO - Datos de Temperatura de Módulos guardados exitosamente en /home/nicole/SR/SOILING/datos/temp_mod_fixed_data.csv\n",
      "2025-06-19 12:05:32 - INFO - Total de registros combinados: 740716\n",
      "2025-06-19 12:05:32 - INFO - Rango de fechas: 2024-07-01 13:00:02+00:00 a 2025-06-19 16:04:04.638503+00:00\n",
      "2025-06-19 12:05:32 - INFO - Temperatura: ✅\n",
      "2025-06-19 12:05:33 - INFO - Conexión a InfluxDB cerrada.\n",
      "2025-06-19 12:05:33 - INFO - \n",
      "🎉 Completado: 8/8 exitosos\n",
      "2025-06-19 12:05:33 - INFO - \n",
      "==================================================\n",
      "2025-06-19 12:05:33 - INFO - ✅ iv600\n",
      "2025-06-19 12:05:33 - INFO - ✅ pv_glasses\n",
      "2025-06-19 12:05:33 - INFO - ✅ dustiq\n",
      "2025-06-19 12:05:33 - INFO - ✅ pvstand\n",
      "2025-06-19 12:05:33 - INFO - ✅ refcells\n",
      "2025-06-19 12:05:33 - INFO - ✅ refcells_processed\n",
      "2025-06-19 12:05:33 - INFO - ✅ soiling_kit\n",
      "2025-06-19 12:05:33 - INFO - ✅ temp_mod_fixed\n",
      "2025-06-19 12:05:33 - INFO - ==================================================\n"
     ]
    }
   ],
   "source": [
    "# === EJECUCIÓN OPTIMIZADA COMPLETA ===\n",
    "logger.info(\"🚀 Iniciando descarga completa optimizada...\")\n",
    "results = download_all_data_optimized(START_DATE, END_DATE, OUTPUT_DIR)\n",
    "\n",
    "# Resumen final\n",
    "logger.info(\"\\n\" + \"=\"*50)\n",
    "for process, success in results.items():\n",
    "    status = \"✅\" if success else \"❌\"\n",
    "    logger.info(f\"{status} {process}\")\n",
    "logger.info(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 10:17:47 - INFO - \n",
      "================================================================================\n",
      "2025-07-29 10:17:47 - INFO - 🔋 DESCARGA PVSTAND DESDE CLICKHOUSE\n",
      "2025-07-29 10:17:47 - INFO - ================================================================================\n",
      "2025-07-29 10:17:47 - INFO - �� Rango de fechas: 2024-07-01 a 2025-12-31\n",
      "2025-07-29 10:17:47 - INFO - 📁 Directorio de salida: /home/nicole/SR/SOILING/datos\n",
      "2025-07-29 10:17:47 - INFO - 🚀 Descarga desde ClickHouse - PSDA.perc1fixed y PSDA.perc2fixed\n",
      "2025-07-29 10:17:47 - INFO - Iniciando descarga de datos PVStand desde ClickHouse...\n",
      "2025-07-29 10:17:47 - INFO - Conectando a Clickhouse...\n",
      "2025-07-29 10:17:47 - INFO - Conexión a Clickhouse establecida\n",
      "2025-07-29 10:17:47 - INFO - Consultando datos PVStand desde ClickHouse...\n",
      "2025-07-29 10:17:47 - INFO - Ejecutando consulta: \n",
      "        SELECT \n",
      "            timestamp,\n",
      "            'perc1fixed' as module,\n",
      "            pmax,\n",
      "      ...\n",
      "2025-07-29 10:17:50 - INFO - Datos obtenidos: 221608 registros\n",
      "2025-07-29 10:17:50 - INFO - Procesando datos...\n",
      "2025-07-29 10:17:51 - INFO - Ordenando datos por timestamp...\n",
      "2025-07-29 10:17:51 - INFO - Rango de fechas en los datos:\n",
      "2025-07-29 10:17:51 - INFO - Fecha más antigua: 2024-07-01 00:00:00+00:00\n",
      "2025-07-29 10:17:51 - INFO - Fecha más reciente: 2025-07-29 14:05:00+00:00\n",
      "2025-07-29 10:17:51 - INFO - Distribución por módulo:\n",
      "2025-07-29 10:17:51 - INFO -    - perc1fixed: 110804 registros\n",
      "2025-07-29 10:17:51 - INFO -    - perc2fixed: 110804 registros\n",
      "2025-07-29 10:17:51 - INFO - Guardando datos en: /home/nicole/SR/SOILING/datos/raw_pvstand_clickhouse_data.csv\n",
      "2025-07-29 10:17:54 - INFO - Datos PVStand desde ClickHouse guardados exitosamente\n",
      "2025-07-29 10:17:54 - INFO - Total de registros: 221608\n",
      "2025-07-29 10:17:54 - INFO - Rango de fechas: 2024-07-01 00:00:00+00:00 a 2025-07-29 14:05:00+00:00\n",
      "2025-07-29 10:17:54 - INFO - Estadísticas de los datos por módulo:\n",
      "2025-07-29 10:17:54 - INFO - \n",
      "perc1fixed:\n",
      "2025-07-29 10:17:54 - INFO -    pmax - Rango: 0.000 a 439.380\n",
      "2025-07-29 10:17:54 - INFO -    imax - Rango: 0.000 a 13.797\n",
      "2025-07-29 10:17:54 - INFO -    umax - Rango: 0.065 a 48.757\n",
      "2025-07-29 10:17:54 - INFO - \n",
      "perc2fixed:\n",
      "2025-07-29 10:17:54 - INFO -    pmax - Rango: 0.000 a 449.910\n",
      "2025-07-29 10:17:54 - INFO -    imax - Rango: 0.000 a 12.597\n",
      "2025-07-29 10:17:54 - INFO -    umax - Rango: 0.097 a 52.696\n",
      "2025-07-29 10:17:54 - INFO - \n",
      "Estructura de datos del PVStand:\n",
      "2025-07-29 10:17:54 - INFO -    - module: Identificador del módulo (perc1fixed/perc2fixed)\n",
      "2025-07-29 10:17:54 - INFO -    - pmax: Potencia máxima del módulo\n",
      "2025-07-29 10:17:54 - INFO -    - imax: Corriente máxima del módulo\n",
      "2025-07-29 10:17:54 - INFO -    - umax: Voltaje máximo del módulo\n",
      "2025-07-29 10:17:54 - INFO - Cerrando conexión a Clickhouse...\n",
      "2025-07-29 10:17:54 - INFO - Conexión a Clickhouse cerrada\n",
      "2025-07-29 10:17:54 - INFO - \n",
      "============================================================\n",
      "2025-07-29 10:17:54 - INFO - 🎉 ¡DESCARGA PVSTAND DESDE CLICKHOUSE COMPLETADA!\n",
      "2025-07-29 10:17:54 - INFO - 📂 Archivo: raw_pvstand_clickhouse_data.csv\n",
      "2025-07-29 10:17:54 - INFO - �� Tamaño: 11.62 MB\n",
      "2025-07-29 10:17:54 - INFO - �� Total de registros: 221,608\n",
      "2025-07-29 10:17:54 - INFO - 🗂️ Ubicación: /home/nicole/SR/SOILING/datos\n",
      "2025-07-29 10:17:54 - INFO - �� Datos incluyen: pmax, imax, umax\n",
      "2025-07-29 10:17:54 - INFO - �� Período: Julio 2024 - Julio 2025\n",
      "2025-07-29 10:17:54 - INFO - ============================================================\n",
      "2025-07-29 10:17:54 - INFO - \n",
      "📋 Muestra de los datos:\n",
      "2025-07-29 10:17:54 - INFO -    Columnas: ['timestamp', 'module', 'pmax', 'imax', 'umax']\n",
      "2025-07-29 10:17:54 - INFO -    pmax rango: 0.000 - 0.000\n",
      "2025-07-29 10:17:54 - INFO -    imax rango: 0.000 - 0.001\n",
      "2025-07-29 10:17:54 - INFO -    umax rango: 0.448 - 0.655\n",
      "2025-07-29 10:17:54 - INFO - \n",
      "🏁 DESCARGA PVSTAND DESDE CLICKHOUSE COMPLETADA\n"
     ]
    }
   ],
   "source": [
    "# 🚀 EJECUTAR SOLO DESCARGA PVSTAND DESDE CLICKHOUSE\n",
    "# ============================================================================\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"🔋 DESCARGA PVSTAND DESDE CLICKHOUSE\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "logger.info(f\"�� Rango de fechas: {START_DATE.strftime('%Y-%m-%d')} a {END_DATE.strftime('%Y-%m-%d')}\")\n",
    "logger.info(f\"📁 Directorio de salida: {OUTPUT_DIR}\")\n",
    "logger.info(\"🚀 Descarga desde ClickHouse - PSDA.perc1fixed y PSDA.perc2fixed\")\n",
    "\n",
    "# Ejecutar descarga de PVStand desde ClickHouse\n",
    "success = download_pvstand_clickhouse(START_DATE, END_DATE, OUTPUT_DIR)\n",
    "\n",
    "if success:\n",
    "    # Verificar archivo generado\n",
    "    output_file = os.path.join(OUTPUT_DIR, 'raw_pvstand_iv_data.csv')\n",
    "    if os.path.exists(output_file):\n",
    "        file_size_mb = os.path.getsize(output_file) / (1024*1024)\n",
    "        total_lines = sum(1 for line in open(output_file)) - 1\n",
    "        \n",
    "        logger.info(\"\\n\" + \"=\"*60)\n",
    "        logger.info(\"🎉 ¡DESCARGA PVSTAND DESDE CLICKHOUSE COMPLETADA!\")\n",
    "        logger.info(f\"📂 Archivo: raw_pvstand_iv_data.csv\")\n",
    "        logger.info(f\"�� Tamaño: {file_size_mb:.2f} MB\")\n",
    "        logger.info(f\"�� Total de registros: {total_lines:,}\")\n",
    "        logger.info(f\"🗂️ Ubicación: {OUTPUT_DIR}\")\n",
    "        logger.info(\"�� Datos incluyen: pmax, imax, umax\")\n",
    "        logger.info(\"�� Período: Julio 2024 - Julio 2025\")\n",
    "        logger.info(\"=\"*60)\n",
    "        \n",
    "        # Mostrar muestra de los datos\n",
    "        try:\n",
    "            df_sample = pd.read_csv(output_file, nrows=5)\n",
    "            logger.info(\"\\n📋 Muestra de los datos:\")\n",
    "            logger.info(f\"   Columnas: {list(df_sample.columns)}\")\n",
    "            if 'pmax' in df_sample.columns:\n",
    "                logger.info(f\"   pmax rango: {df_sample['pmax'].min():.3f} - {df_sample['pmax'].max():.3f}\")\n",
    "            if 'imax' in df_sample.columns:\n",
    "                logger.info(f\"   imax rango: {df_sample['imax'].min():.3f} - {df_sample['imax'].max():.3f}\")\n",
    "            if 'umax' in df_sample.columns:\n",
    "                logger.info(f\"   umax rango: {df_sample['umax'].min():.3f} - {df_sample['umax'].max():.3f}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"⚠️ Error al mostrar muestra: {e}\")\n",
    "    else:\n",
    "        logger.warning(\"⚠️ Archivo no encontrado después de la descarga\")\n",
    "else:\n",
    "    logger.error(\"❌ Error en la descarga de PVStand desde ClickHouse\")\n",
    "\n",
    "logger.info(\"\\n🏁 DESCARGA PVSTAND DESDE CLICKHOUSE COMPLETADA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 09:19:02 - INFO - \n",
      "================================================================================\n",
      "2025-07-29 09:19:02 - INFO - 🔋 INICIANDO DESCARGA ESPECÍFICA: DATOS DE PVSTAND\n",
      "2025-07-29 09:19:02 - INFO - ================================================================================\n",
      "2025-07-29 09:19:02 - INFO - 📅 Rango de fechas: 2024-07-01 a 2025-12-31\n",
      "2025-07-29 09:19:02 - INFO - 📁 Directorio de salida: /home/nicole/SR/SOILING/datos\n",
      "2025-07-29 09:19:02 - INFO - 🚀 Descarga optimizada solo para PVStand\n",
      "2025-07-29 09:19:02 - INFO - Cliente InfluxDB y query_api inicializados.\n",
      "2025-07-29 09:19:02 - INFO - ✅ Conexión a InfluxDB establecida\n",
      "2025-07-29 09:19:02 - INFO - \n",
      "🔋 Iniciando descarga de PVStand...\n",
      "2025-07-29 09:19:02 - ERROR - ❌ Error general en el proceso: name 'download_pvstand' is not defined\n",
      "2025-07-29 09:19:02 - ERROR - 🔍 Detalles del error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3612/427214555.py\", line 24, in <module>\n",
      "    success = download_pvstand(influx_manager, START_DATE, END_DATE, OUTPUT_DIR)\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "NameError: name 'download_pvstand' is not defined. Did you mean: 'download_dustiq'?\n",
      "\n",
      "2025-07-29 09:19:02 - INFO - Conexión a InfluxDB cerrada.\n",
      "2025-07-29 09:19:02 - INFO - 🔌 Conexión a InfluxDB cerrada\n",
      "2025-07-29 09:19:02 - INFO - \n",
      "🏁 PROCESO DE DESCARGA ESPECÍFICA DE PVSTAND FINALIZADO\n"
     ]
    }
   ],
   "source": [
    "# 🔋 DESCARGA ESPECÍFICA: SOLO DATOS DE PVSTAND ACTUALIZADOS INFLUXDB\n",
    "# ============================================================================\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"🔋 INICIANDO DESCARGA ESPECÍFICA: DATOS DE PVSTAND\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "logger.info(f\"📅 Rango de fechas: {START_DATE.strftime('%Y-%m-%d')} a {END_DATE.strftime('%Y-%m-%d')}\")\n",
    "logger.info(f\"📁 Directorio de salida: {OUTPUT_DIR}\")\n",
    "logger.info(\"🚀 Descarga optimizada solo para PVStand\")\n",
    "\n",
    "# Inicializar cliente InfluxDB\n",
    "influx_manager = InfluxDBManager(INFLUX_CONFIG)\n",
    "\n",
    "try:\n",
    "    if not influx_manager.connect():\n",
    "        logger.error(\"❌ No se pudo establecer conexión con InfluxDB\")\n",
    "    else:\n",
    "        logger.info(\"✅ Conexión a InfluxDB establecida\")\n",
    "        \n",
    "        # Ejecutar descarga específica de PVStand\n",
    "        logger.info(\"\\n🔋 Iniciando descarga de PVStand...\")\n",
    "        success = download_pvstand(influx_manager, START_DATE, END_DATE, OUTPUT_DIR)\n",
    "        \n",
    "        # Verificar resultado y mostrar información del archivo\n",
    "        if success:\n",
    "            output_file = os.path.join(OUTPUT_DIR, 'raw_pvstand_iv_data.csv')\n",
    "            if os.path.exists(output_file):\n",
    "                file_size_mb = os.path.getsize(output_file) / (1024*1024)\n",
    "                \n",
    "                # Leer primeras líneas para verificar datos\n",
    "                try:\n",
    "                    df_check = pd.read_csv(output_file, nrows=5)\n",
    "                    logger.info(f\"📊 Columnas en el archivo: {list(df_check.columns)}\")\n",
    "                    logger.info(f\"📈 Primeros registros: {len(df_check)}\")\n",
    "                    \n",
    "                    # Contar total de registros\n",
    "                    total_lines = sum(1 for line in open(output_file)) - 1  # -1 para header\n",
    "                    \n",
    "                    logger.info(\"\\n\" + \"=\"*60)\n",
    "                    logger.info(\"🎉 ¡DESCARGA DE PVSTAND COMPLETADA!\")\n",
    "                    logger.info(f\"📂 Archivo: raw_pvstand_iv_data.csv\")\n",
    "                    logger.info(f\"📊 Tamaño: {file_size_mb:.2f} MB\")\n",
    "                    logger.info(f\"📈 Total de registros: {total_lines:,}\")\n",
    "                    logger.info(f\"🗂️ Ubicación: {OUTPUT_DIR}\")\n",
    "                    logger.info(\"=\"*60)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"⚠️ Error al verificar contenido del archivo: {e}\")\n",
    "                    logger.info(\"✅ Archivo generado exitosamente\")\n",
    "            else:\n",
    "                logger.warning(\"⚠️ Archivo no encontrado después de la descarga\")\n",
    "        else:\n",
    "            logger.error(\"\\n\" + \"=\"*50)\n",
    "            logger.error(\"❌ ERROR EN LA DESCARGA DE PVSTAND\")\n",
    "            logger.error(\"🔍 Revisa los logs anteriores para más detalles\")\n",
    "            logger.error(\"🔧 Verifica la conexión y configuración de InfluxDB\")\n",
    "            logger.error(\"=\"*50)\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"❌ Error general en el proceso: {e}\")\n",
    "    import traceback\n",
    "    logger.error(f\"🔍 Detalles del error:\\n{traceback.format_exc()}\")\n",
    "    \n",
    "finally:\n",
    "    # Cerrar conexión\n",
    "    influx_manager.disconnect()\n",
    "    logger.info(\"🔌 Conexión a InfluxDB cerrada\")\n",
    "\n",
    "logger.info(\"\\n🏁 PROCESO DE DESCARGA ESPECÍFICA DE PVSTAND FINALIZADO\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 12:54:18 - INFO - \n",
      "================================================================================\n",
      "2025-07-24 12:54:18 - INFO - 📊 INICIANDO DESCARGA ESPECÍFICA: DATOS DE DUSTIQ\n",
      "2025-07-24 12:54:18 - INFO - ================================================================================\n",
      "2025-07-24 12:54:18 - INFO - 📅 Rango de fechas: 2024-07-01 a 2025-12-31\n",
      "2025-07-24 12:54:18 - INFO - 📁 Directorio de salida: /home/nicole/SR/SOILING/datos\n",
      "2025-07-24 12:54:18 - INFO - 🚀 Descarga optimizada solo para DustIQ\n",
      "2025-07-24 12:54:18 - INFO - Cliente InfluxDB y query_api inicializados.\n",
      "2025-07-24 12:54:18 - INFO - ✅ Conexión a InfluxDB establecida\n",
      "2025-07-24 12:54:18 - INFO - \n",
      "📊 Iniciando descarga de DustIQ...\n",
      "2025-07-24 12:54:18 - INFO - Iniciando descarga de datos DustIQ...\n",
      "2025-07-24 12:54:18 - INFO - Consultando InfluxDB: bucket=PSDA, tables=['DustIQ'], attributes=['SR_C11_Avg', 'SR_C12_Avg']\n",
      "2025-07-24 12:54:57 - INFO - Datos DustIQ guardados exitosamente\n",
      "2025-07-24 12:54:57 - INFO - Total de registros: 537765\n",
      "2025-07-24 12:54:57 - INFO - Rango de fechas: 0 a 537764\n",
      "2025-07-24 12:54:57 - INFO - 📊 Columnas en el archivo: ['Unnamed: 0', 'result', 'table', '_start', '_stop', '_time', '_measurement', 'SR_C11_Avg', 'SR_C12_Avg']\n",
      "2025-07-24 12:54:57 - INFO - 📈 Primeros registros: 10\n",
      "2025-07-24 12:54:57 - INFO - 🔍 Datos del DustIQ detectados:\n",
      "2025-07-24 12:54:57 - INFO -    - SR_C11_Avg - Sensor 1: ✅\n",
      "2025-07-24 12:54:57 - INFO -    - SR_C12_Avg - Sensor 2: ✅\n",
      "2025-07-24 12:54:57 - INFO -    - Datos válidos SR_C11_Avg: 10/10\n",
      "2025-07-24 12:54:57 - INFO -    - Datos válidos SR_C12_Avg: 10/10\n",
      "2025-07-24 12:54:57 - INFO - \n",
      "============================================================\n",
      "2025-07-24 12:54:57 - INFO - 🎉 ¡DESCARGA DE DUSTIQ COMPLETADA!\n",
      "2025-07-24 12:54:57 - INFO - 📂 Archivo: raw_dustiq_data.csv\n",
      "2025-07-24 12:54:57 - INFO - 📊 Tamaño: 57.79 MB\n",
      "2025-07-24 12:54:57 - INFO - 📈 Total de registros: 537,765\n",
      "2025-07-24 12:54:57 - INFO - 🗂️ Ubicación: /home/nicole/SR/SOILING/datos\n",
      "2025-07-24 12:54:57 - INFO - 📊 Datos incluyen: SR_C11_Avg, SR_C12_Avg\n",
      "2025-07-24 12:54:57 - INFO - 📡 Sensores de irradiancia para análisis de soiling\n",
      "2025-07-24 12:54:57 - INFO - ============================================================\n",
      "2025-07-24 12:54:57 - INFO - Conexión a InfluxDB cerrada.\n",
      "2025-07-24 12:54:57 - INFO - 🔌 Conexión a InfluxDB cerrada\n",
      "2025-07-24 12:54:57 - INFO - \n",
      "🏁 PROCESO DE DESCARGA ESPECÍFICA DE DUSTIQ FINALIZADO\n"
     ]
    }
   ],
   "source": [
    "# 📊 DESCARGA ESPECÍFICA: SOLO DATOS DE DUSTIQ ACTUALIZADOS DESDE INFLUXDB\n",
    "# ============================================================================\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"📊 INICIANDO DESCARGA ESPECÍFICA: DATOS DE DUSTIQ\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "logger.info(f\"📅 Rango de fechas: {START_DATE.strftime('%Y-%m-%d')} a {END_DATE.strftime('%Y-%m-%d')}\")\n",
    "logger.info(f\"📁 Directorio de salida: {OUTPUT_DIR}\")\n",
    "logger.info(\"🚀 Descarga optimizada solo para DustIQ\")\n",
    "\n",
    "# Inicializar cliente InfluxDB\n",
    "influx_manager = InfluxDBManager(INFLUX_CONFIG)\n",
    "\n",
    "try:\n",
    "    if not influx_manager.connect():\n",
    "        logger.error(\"❌ No se pudo establecer conexión con InfluxDB\")\n",
    "    else:\n",
    "        logger.info(\"✅ Conexión a InfluxDB establecida\")\n",
    "        \n",
    "        # Ejecutar descarga específica de DustIQ\n",
    "        logger.info(\"\\n📊 Iniciando descarga de DustIQ...\")\n",
    "        success = download_dustiq(influx_manager, START_DATE, END_DATE, OUTPUT_DIR)\n",
    "        \n",
    "        # Verificar resultado y mostrar información del archivo\n",
    "        if success:\n",
    "            output_file = os.path.join(OUTPUT_DIR, 'raw_dustiq_data.csv')\n",
    "            if os.path.exists(output_file):\n",
    "                file_size_mb = os.path.getsize(output_file) / (1024*1024)\n",
    "                \n",
    "                # Leer primeras líneas para verificar datos\n",
    "                try:\n",
    "                    df_check = pd.read_csv(output_file, nrows=10)\n",
    "                    logger.info(f\"📊 Columnas en el archivo: {list(df_check.columns)}\")\n",
    "                    logger.info(f\"📈 Primeros registros: {len(df_check)}\")\n",
    "                    \n",
    "                    # Contar total de registros\n",
    "                    total_lines = sum(1 for line in open(output_file)) - 1  # -1 para header\n",
    "                    \n",
    "                    # Mostrar información específica del DustIQ\n",
    "                    if 'SR_C11_Avg' in df_check.columns and 'SR_C12_Avg' in df_check.columns:\n",
    "                        logger.info(\"🔍 Datos del DustIQ detectados:\")\n",
    "                        logger.info(f\"   - SR_C11_Avg - Sensor 1: ✅\")\n",
    "                        logger.info(f\"   - SR_C12_Avg - Sensor 2: ✅\")\n",
    "                        \n",
    "                        # Verificar si hay datos válidos (no NaN)\n",
    "                        valid_c11 = df_check['SR_C11_Avg'].notna().sum()\n",
    "                        valid_c12 = df_check['SR_C12_Avg'].notna().sum()\n",
    "                        logger.info(f\"   - Datos válidos SR_C11_Avg: {valid_c11}/{len(df_check)}\")\n",
    "                        logger.info(f\"   - Datos válidos SR_C12_Avg: {valid_c12}/{len(df_check)}\")\n",
    "                    \n",
    "                    logger.info(\"\\n\" + \"=\"*60)\n",
    "                    logger.info(\"🎉 ¡DESCARGA DE DUSTIQ COMPLETADA!\")\n",
    "                    logger.info(f\"📂 Archivo: raw_dustiq_data.csv\")\n",
    "                    logger.info(f\"📊 Tamaño: {file_size_mb:.2f} MB\")\n",
    "                    logger.info(f\"📈 Total de registros: {total_lines:,}\")\n",
    "                    logger.info(f\"🗂️ Ubicación: {OUTPUT_DIR}\")\n",
    "                    logger.info(\"📊 Datos incluyen: SR_C11_Avg, SR_C12_Avg\")\n",
    "                    logger.info(\"📡 Sensores de irradiancia para análisis de soiling\")\n",
    "                    logger.info(\"=\"*60)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"⚠️ Error al verificar contenido del archivo: {e}\")\n",
    "                    logger.info(\"✅ Archivo generado exitosamente\")\n",
    "            else:\n",
    "                logger.warning(\"⚠️ Archivo no encontrado después de la descarga\")\n",
    "        else:\n",
    "            logger.error(\"\\n\" + \"=\"*50)\n",
    "            logger.error(\"❌ ERROR EN LA DESCARGA DE DUSTIQ\")\n",
    "            logger.error(\"🔍 Revisa los logs anteriores para más detalles\")\n",
    "            logger.error(\"🔧 Verifica la conexión y configuración de InfluxDB\")\n",
    "            logger.error(\"🔗 Bucket: PSDA, Table: DustIQ\")\n",
    "            logger.error(\"📊 Atributos: SR_C11_Avg, SR_C12_Avg\")\n",
    "            logger.error(\"=\"*50)\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"❌ Error general en el proceso: {e}\")\n",
    "    import traceback\n",
    "    logger.error(f\"🔍 Detalles del error:\\n{traceback.format_exc()}\")\n",
    "    \n",
    "finally:\n",
    "    # Cerrar conexión\n",
    "    influx_manager.disconnect()\n",
    "    logger.info(\"🔌 Conexión a InfluxDB cerrada\")\n",
    "\n",
    "logger.info(\"\\n🏁 PROCESO DE DESCARGA ESPECÍFICA DE DUSTIQ FINALIZADO\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 12:57:14 - INFO - Iniciando descarga de datos DustIQ desde ClickHouse...\n",
      "2025-08-06 12:57:14 - INFO - Conectando a Clickhouse...\n",
      "2025-08-06 12:57:15 - INFO - Conexión a Clickhouse establecida\n",
      "2025-08-06 12:57:15 - INFO - Consultando datos DustIQ desde ClickHouse...\n",
      "2025-08-06 12:57:15 - INFO - Ejecutando consulta: \n",
      "        SELECT \n",
      "            Stamptime,\n",
      "            Attribute,\n",
      "            Measure\n",
      "        FROM PSDA...\n",
      "2025-08-06 12:57:26 - INFO - Datos obtenidos: 1182090 registros\n",
      "2025-08-06 12:57:26 - INFO - Procesando datos...\n",
      "2025-08-06 12:57:33 - INFO - Pivotando datos de long format a wide format...\n",
      "2025-08-06 12:57:33 - INFO - Manejando duplicados agrupando por promedio...\n",
      "2025-08-06 12:57:34 - INFO - Rango de fechas en los datos:\n",
      "2025-08-06 12:57:34 - INFO - Fecha más antigua: 2024-07-01 00:00:00+00:00\n",
      "2025-08-06 12:57:34 - INFO - Fecha más reciente: 2025-08-06 00:00:00+00:00\n",
      "2025-08-06 12:57:34 - INFO - Guardando datos en: /home/nicole/SR/SOILING/datos/raw_dustiq_data.csv\n",
      "2025-08-06 12:57:44 - INFO - Datos DustIQ desde ClickHouse guardados exitosamente\n",
      "2025-08-06 12:57:44 - INFO - Total de registros: 556485\n",
      "2025-08-06 12:57:45 - INFO - Rango de fechas: 2024-07-01 00:00:00+00:00 a 2025-08-06 00:00:00+00:00\n",
      "2025-08-06 12:57:45 - INFO - Estadísticas de los datos:\n",
      "2025-08-06 12:57:45 - INFO - SR_C11_Avg - Rango: 0.000 a 100.000\n",
      "2025-08-06 12:57:45 - INFO - SR_C12_Avg - Rango: 0.000 a 101.000\n",
      "2025-08-06 12:57:45 - INFO - Cerrando conexión a Clickhouse...\n",
      "2025-08-06 12:57:45 - INFO - Conexión a Clickhouse cerrada\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_dustiq_clickhouse(START_DATE, END_DATE, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 12:48:23 - INFO - \n",
      "================================================================================\n",
      "2025-08-06 12:48:23 - INFO - 🌪️ DESCARGA SOILING KIT DESDE CLICKHOUSE\n",
      "2025-08-06 12:48:23 - INFO - ================================================================================\n",
      "2025-08-06 12:48:23 - INFO - �� Rango de fechas: 2024-07-01 a 2025-12-31\n",
      "2025-08-06 12:48:23 - INFO - 📁 Directorio de salida: /home/nicole/SR/SOILING/datos\n",
      "2025-08-06 12:48:23 - INFO - 🚀 Descarga desde ClickHouse - PSDA.soilingkit\n",
      "2025-08-06 12:48:23 - INFO - Iniciando descarga de datos del Soiling Kit desde ClickHouse...\n",
      "2025-08-06 12:48:23 - INFO - Conectando a Clickhouse...\n",
      "2025-08-06 12:48:24 - INFO - Conexión a Clickhouse establecida\n",
      "2025-08-06 12:48:24 - INFO - Consultando datos del Soiling Kit desde ClickHouse...\n",
      "2025-08-06 12:48:24 - INFO - Ejecutando consulta: \n",
      "        SELECT \n",
      "            Stamptime,\n",
      "            Attribute,\n",
      "            Measure\n",
      "        FROM PSDA...\n",
      "2025-08-06 12:48:31 - INFO - Datos obtenidos: 1075004 registros\n",
      "2025-08-06 12:48:31 - INFO - Procesando datos...\n",
      "2025-08-06 12:48:36 - INFO - Pivotando datos de long format a wide format...\n",
      "2025-08-06 12:48:36 - INFO - Manejando duplicados agrupando por promedio...\n",
      "2025-08-06 12:48:37 - INFO - Rango de fechas en los datos:\n",
      "2025-08-06 12:48:37 - INFO - Fecha más antigua: 2024-07-01 12:02:15+00:00\n",
      "2025-08-06 12:48:37 - INFO - Fecha más reciente: 2025-08-05 16:07:08+00:00\n",
      "2025-08-06 12:48:37 - INFO - Guardando datos en: /home/nicole/SR/SOILING/datos/soiling_kit_raw_data.csv\n",
      "2025-08-06 12:48:38 - INFO - Datos del Soiling Kit desde ClickHouse guardados exitosamente\n",
      "2025-08-06 12:48:38 - INFO - Total de registros: 70223\n",
      "2025-08-06 12:48:38 - INFO - Rango de fechas: 2024-07-01 12:02:15+00:00 a 2025-08-05 16:07:08+00:00\n",
      "2025-08-06 12:48:38 - INFO - Estadísticas de los datos:\n",
      "2025-08-06 12:48:38 - INFO - Isc(e) - Rango: 0.247 a 1.952\n",
      "2025-08-06 12:48:38 - INFO - Isc(p) - Rango: 0.063 a 1.977\n",
      "2025-08-06 12:48:38 - INFO - Te(C) - Rango: 0.0 a 312.5\n",
      "2025-08-06 12:48:38 - INFO - Tp(C) - Rango: 0.0 a 624.5\n",
      "2025-08-06 12:48:38 - INFO - Estructura de datos del Soiling Kit:\n",
      "2025-08-06 12:48:38 - INFO -    - Isc(e): Corriente de cortocircuito de la celda limpia (referencia)\n",
      "2025-08-06 12:48:38 - INFO -    - Isc(p): Corriente de cortocircuito de la celda sucia (panel)\n",
      "2025-08-06 12:48:38 - INFO -    - Te(C): Temperatura de la celda limpia en Celsius\n",
      "2025-08-06 12:48:38 - INFO -    - Tp(C): Temperatura de la celda sucia en Celsius\n",
      "2025-08-06 12:48:38 - INFO - Cerrando conexión a Clickhouse...\n",
      "2025-08-06 12:48:38 - INFO - Conexión a Clickhouse cerrada\n",
      "2025-08-06 12:48:39 - INFO - \n",
      "============================================================\n",
      "2025-08-06 12:48:39 - INFO - 🎉 ¡DESCARGA SOILING KIT DESDE CLICKHOUSE COMPLETADA!\n",
      "2025-08-06 12:48:39 - INFO - 📂 Archivo: soiling_kit_raw_data.csv\n",
      "2025-08-06 12:48:39 - INFO - �� Tamaño: 6.31 MB\n",
      "2025-08-06 12:48:39 - INFO - �� Total de registros: 70,223\n",
      "2025-08-06 12:48:39 - INFO - 🗂️ Ubicación: /home/nicole/SR/SOILING/datos\n",
      "2025-08-06 12:48:39 - INFO - ��️ Datos incluyen: Isc(e), Isc(p), Te(C), Tp(C)\n",
      "2025-08-06 12:48:39 - INFO - �� Período: Julio 2024 - Julio 2025\n",
      "2025-08-06 12:48:39 - INFO - ============================================================\n",
      "2025-08-06 12:48:39 - INFO - \n",
      "📋 Muestra de los datos:\n",
      "2025-08-06 12:48:39 - INFO -    Columnas: ['timestamp', 'Isc(e)', 'Isc(p)', 'Te(C)', 'Tp(C)']\n",
      "2025-08-06 12:48:39 - INFO -    Isc(e) rango: 1.029 - 1.032\n",
      "2025-08-06 12:48:39 - INFO -    Isc(p) rango: 1.078 - 1.080\n",
      "2025-08-06 12:48:39 - INFO - \n",
      "�� DESCARGA SOILING KIT DESDE CLICKHOUSE COMPLETADA\n"
     ]
    }
   ],
   "source": [
    "# 🚀 EJECUCIÓN: DESCARGA SOILING KIT DESDE CLICKHOUSE\n",
    "# ============================================================================\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"🌪️ DESCARGA SOILING KIT DESDE CLICKHOUSE\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "logger.info(f\"�� Rango de fechas: {START_DATE.strftime('%Y-%m-%d')} a {END_DATE.strftime('%Y-%m-%d')}\")\n",
    "logger.info(f\"📁 Directorio de salida: {OUTPUT_DIR}\")\n",
    "logger.info(\"🚀 Descarga desde ClickHouse - PSDA.soilingkit\")\n",
    "\n",
    "# Ejecutar descarga del Soiling Kit desde ClickHouse\n",
    "success = download_soiling_kit_clickhouse(START_DATE, END_DATE, OUTPUT_DIR)\n",
    "\n",
    "if success:\n",
    "    # Verificar archivo generado\n",
    "    output_file = os.path.join(OUTPUT_DIR, 'soiling_kit_raw_data.csv')\n",
    "    if os.path.exists(output_file):\n",
    "        file_size_mb = os.path.getsize(output_file) / (1024*1024)\n",
    "        total_lines = sum(1 for line in open(output_file)) - 1\n",
    "        \n",
    "        logger.info(\"\\n\" + \"=\"*60)\n",
    "        logger.info(\"🎉 ¡DESCARGA SOILING KIT DESDE CLICKHOUSE COMPLETADA!\")\n",
    "        logger.info(f\"📂 Archivo: soiling_kit_raw_data.csv\")\n",
    "        logger.info(f\"�� Tamaño: {file_size_mb:.2f} MB\")\n",
    "        logger.info(f\"�� Total de registros: {total_lines:,}\")\n",
    "        logger.info(f\"🗂️ Ubicación: {OUTPUT_DIR}\")\n",
    "        logger.info(\"��️ Datos incluyen: Isc(e), Isc(p), Te(C), Tp(C)\")\n",
    "        logger.info(\"�� Período: Julio 2024 - Julio 2025\")\n",
    "        logger.info(\"=\"*60)\n",
    "        \n",
    "        # Mostrar muestra de los datos\n",
    "        try:\n",
    "            df_sample = pd.read_csv(output_file, nrows=5)\n",
    "            logger.info(\"\\n📋 Muestra de los datos:\")\n",
    "            logger.info(f\"   Columnas: {list(df_sample.columns)}\")\n",
    "            if 'Isc(e)' in df_sample.columns:\n",
    "                logger.info(f\"   Isc(e) rango: {df_sample['Isc(e)'].min():.3f} - {df_sample['Isc(e)'].max():.3f}\")\n",
    "            if 'Isc(p)' in df_sample.columns:\n",
    "                logger.info(f\"   Isc(p) rango: {df_sample['Isc(p)'].min():.3f} - {df_sample['Isc(p)'].max():.3f}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"⚠️ Error al mostrar muestra: {e}\")\n",
    "    else:\n",
    "        logger.warning(\"⚠️ Archivo no encontrado después de la descarga\")\n",
    "else:\n",
    "    logger.error(\"❌ Error en la descarga del Soiling Kit desde ClickHouse\")\n",
    "\n",
    "logger.info(\"\\n�� DESCARGA SOILING KIT DESDE CLICKHOUSE COMPLETADA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 12:58:10 - INFO - \n",
      "================================================================================\n",
      "2025-07-29 12:58:10 - INFO - 🔋 DESCARGA ESPECÍFICA: SOLO DATOS IV600\n",
      "2025-07-29 12:58:11 - INFO - ================================================================================\n",
      "2025-07-29 12:58:11 - INFO - �� Rango de fechas: 2024-07-01 a 2025-12-31\n",
      "2025-07-29 12:58:11 - INFO - 📁 Directorio de salida: /home/nicole/SR/SOILING/datos\n",
      "2025-07-29 12:58:11 - INFO - 🚀 Descarga desde ClickHouse - ref_data.iv_curves_trazador_manual\n",
      "2025-07-29 12:58:11 - INFO - Iniciando descarga de datos IV600...\n",
      "2025-07-29 12:58:11 - INFO - Conectando a Clickhouse...\n",
      "2025-07-29 12:58:13 - INFO - Conexión a Clickhouse establecida\n",
      "2025-07-29 12:58:13 - INFO - Consultando datos IV600...\n",
      "2025-07-29 12:58:16 - INFO - Datos obtenidos: 2406 registros\n",
      "2025-07-29 12:58:16 - INFO - Procesando datos...\n",
      "2025-07-29 12:58:16 - INFO - Creando DataFrame...\n",
      "2025-07-29 12:58:18 - INFO - Rango de fechas en los datos:\n",
      "2025-07-29 12:58:18 - INFO - Fecha más antigua: 2024-09-24 12:16:00+00:00\n",
      "2025-07-29 12:58:18 - INFO - Fecha más reciente: 2025-05-26 14:45:00+00:00\n",
      "2025-07-29 12:58:18 - INFO - Filtrando datos entre 2024-07-01 00:00:00+00:00 y 2025-12-31 00:00:00+00:00...\n",
      "2025-07-29 12:58:18 - INFO - Se encontraron 2406 registros en el rango especificado.\n",
      "2025-07-29 12:58:18 - INFO - Guardando datos en: /home/nicole/SR/SOILING/datos/raw_iv600_data.csv\n",
      "2025-07-29 12:58:19 - INFO - Datos guardados exitosamente. Total de registros: 2406\n",
      "2025-07-29 12:58:19 - INFO - Rango de fechas: 2024-09-24 12:16:00+00:00 a 2025-05-26 14:45:00+00:00\n",
      "2025-07-29 12:58:19 - INFO - Cerrando conexión a Clickhouse...\n",
      "2025-07-29 12:58:19 - INFO - Conexión a Clickhouse cerrada\n",
      "2025-07-29 12:58:19 - INFO - \n",
      "============================================================\n",
      "2025-07-29 12:58:19 - INFO - 🎉 ¡DESCARGA IV600 COMPLETADA!\n",
      "2025-07-29 12:58:19 - INFO - �� Archivo: raw_iv600_data.csv\n",
      "2025-07-29 12:58:19 - INFO - �� Tamaño: 0.29 MB\n",
      "2025-07-29 12:58:19 - INFO - �� Total de registros: 2,406\n",
      "2025-07-29 12:58:19 - INFO - 🗂️ Ubicación: /home/nicole/SR/SOILING/datos\n",
      "2025-07-29 12:58:19 - INFO - 🔋 Datos incluyen: timestamp, module, pmp, isc, voc, imp, vmp\n",
      "2025-07-29 12:58:19 - INFO - 📊 Curvas IV procesadas desde ClickHouse\n",
      "2025-07-29 12:58:19 - INFO - ============================================================\n",
      "2025-07-29 12:58:19 - INFO - \n",
      "📋 Muestra de los datos:\n",
      "2025-07-29 12:58:19 - INFO -    Columnas: ['timestamp', 'module', 'pmp', 'isc', 'voc', 'imp', 'vmp']\n",
      "2025-07-29 12:58:19 - INFO -    pmp rango: 281.224 - 302.016\n",
      "2025-07-29 12:58:19 - INFO -    isc rango: 8.987 - 9.630\n",
      "2025-07-29 12:58:19 - INFO -    voc rango: 42.269 - 43.020\n",
      "2025-07-29 12:58:19 - INFO - \n",
      "🏁 DESCARGA IV600 COMPLETADA\n"
     ]
    }
   ],
   "source": [
    "# 🔋 DESCARGA ESPECÍFICA: SOLO IV600\n",
    "# ============================================================================\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"🔋 DESCARGA ESPECÍFICA: SOLO DATOS IV600\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "logger.info(f\"�� Rango de fechas: {START_DATE.strftime('%Y-%m-%d')} a {END_DATE.strftime('%Y-%m-%d')}\")\n",
    "logger.info(f\"📁 Directorio de salida: {OUTPUT_DIR}\")\n",
    "logger.info(\"🚀 Descarga desde ClickHouse - ref_data.iv_curves_trazador_manual\")\n",
    "\n",
    "# Ejecutar descarga específica de IV600\n",
    "success = download_iv600(START_DATE, END_DATE, OUTPUT_DIR)\n",
    "\n",
    "if success:\n",
    "    # Verificar archivo generado\n",
    "    output_file = os.path.join(OUTPUT_DIR, 'raw_iv600_data.csv')\n",
    "    if os.path.exists(output_file):\n",
    "        file_size_mb = os.path.getsize(output_file) / (1024*1024)\n",
    "        total_lines = sum(1 for line in open(output_file)) - 1\n",
    "        \n",
    "        logger.info(\"\\n\" + \"=\"*60)\n",
    "        logger.info(\"🎉 ¡DESCARGA IV600 COMPLETADA!\")\n",
    "        logger.info(f\"�� Archivo: raw_iv600_data.csv\")\n",
    "        logger.info(f\"�� Tamaño: {file_size_mb:.2f} MB\")\n",
    "        logger.info(f\"�� Total de registros: {total_lines:,}\")\n",
    "        logger.info(f\"🗂️ Ubicación: {OUTPUT_DIR}\")\n",
    "        logger.info(\"🔋 Datos incluyen: timestamp, module, pmp, isc, voc, imp, vmp\")\n",
    "        logger.info(\"📊 Curvas IV procesadas desde ClickHouse\")\n",
    "        logger.info(\"=\"*60)\n",
    "        \n",
    "        # Mostrar muestra de los datos\n",
    "        try:\n",
    "            df_sample = pd.read_csv(output_file, nrows=5)\n",
    "            logger.info(\"\\n📋 Muestra de los datos:\")\n",
    "            logger.info(f\"   Columnas: {list(df_sample.columns)}\")\n",
    "            if 'pmp' in df_sample.columns:\n",
    "                logger.info(f\"   pmp rango: {df_sample['pmp'].min():.3f} - {df_sample['pmp'].max():.3f}\")\n",
    "            if 'isc' in df_sample.columns:\n",
    "                logger.info(f\"   isc rango: {df_sample['isc'].min():.3f} - {df_sample['isc'].max():.3f}\")\n",
    "            if 'voc' in df_sample.columns:\n",
    "                logger.info(f\"   voc rango: {df_sample['voc'].min():.3f} - {df_sample['voc'].max():.3f}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"⚠️ Error al mostrar muestra: {e}\")\n",
    "    else:\n",
    "        logger.warning(\"⚠️ Archivo no encontrado después de la descarga\")\n",
    "else:\n",
    "    logger.error(\"❌ Error en la descarga de IV600\")\n",
    "\n",
    "logger.info(\"\\n🏁 DESCARGA IV600 COMPLETADA\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
