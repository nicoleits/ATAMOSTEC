{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python path: /home/nicole/SR/SOILING/.venv/bin/python\n",
      "Python version: 3.12.3 (main, Jun 18 2025, 17:59:45) [GCC 13.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(f\"Python path: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 10:17:04 - INFO - Rango de fechas: 2024-07-01 a 2025-12-31\n",
      "2025-07-29 10:17:04 - INFO - Directorio de salida: /home/nicole/SR/SOILING/datos\n"
     ]
    }
   ],
   "source": [
    "# Importar librerías necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import clickhouse_connect\n",
    "from influxdb_client import InfluxDBClient\n",
    "import gc\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuración de InfluxDB\n",
    "INFLUX_CONFIG = {\n",
    "    'url': \"http://146.83.153.212:27017\", #\"http://172.24.61.95:27017\"\n",
    "    'token': \"piDbFR_bfRWO5Epu1IS96WbkNpSZZCYgwZZR29PcwUsxXwKdIyLMhVAhU4-5ohWeXIsX7Dp_X-WiPIDx0beafg==\",\n",
    "    'org': \"atamostec\",\n",
    "    'timeout': 300000\n",
    "}\n",
    "\n",
    "# Configuración de Clickhouse\n",
    "CLICKHOUSE_CONFIG = {\n",
    "    'host': \"146.83.153.212\", #\"172.24.61.95\"\n",
    "    'port': \"30091\",\n",
    "    'user': \"default\",\n",
    "    'password': \"Psda2020\"\n",
    "}\n",
    "\n",
    "# Configuración de fechas\n",
    "START_DATE = pd.to_datetime('01/07/2024', dayfirst=True).tz_localize('UTC')\n",
    "END_DATE = pd.to_datetime('31/12/2025', dayfirst=True).tz_localize('UTC')\n",
    "\n",
    "# Directorio de salida - ruta absoluta desde el notebook en download/\n",
    "OUTPUT_DIR = \"/home/nicole/SR/SOILING/datos\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Mostrar configuración\n",
    "logger.info(f\"Rango de fechas: {START_DATE.strftime('%Y-%m-%d')} a {END_DATE.strftime('%Y-%m-%d')}\")\n",
    "logger.info(f\"Directorio de salida: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfluxDBManager:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.client = None\n",
    "        self.query_api = None\n",
    "        \n",
    "    def connect(self):\n",
    "        try:\n",
    "            self.client = InfluxDBClient(\n",
    "                url=self.config['url'],\n",
    "                token=self.config['token'],\n",
    "                org=self.config['org'],\n",
    "                timeout=self.config['timeout']\n",
    "            )\n",
    "            self.query_api = self.client.query_api()\n",
    "            logger.info(\"Cliente InfluxDB y query_api inicializados.\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error al conectar con InfluxDB: {e}\")\n",
    "            return False\n",
    "            \n",
    "    def disconnect(self):\n",
    "        if self.client:\n",
    "            self.client.close()\n",
    "            logger.info(\"Conexión a InfluxDB cerrada.\")\n",
    "            \n",
    "    def query_influxdb(self, bucket, tables, attributes, start_date, stop_date):\n",
    "        try:\n",
    "            # Convertir fechas al formato correcto para InfluxDB\n",
    "            start_str = start_date.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "            stop_str = stop_date.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "            \n",
    "            # Construir la lista de atributos en formato correcto\n",
    "            attributes_str = \" or \".join([f'r[\"_field\"] == \"{attr}\"' for attr in attributes])\n",
    "\n",
    "            query = f'''\n",
    "            from(bucket: \"{bucket}\")\n",
    "                |> range(start: {start_str}, stop: {stop_str})\n",
    "                |> filter(fn: (r) => {\" or \".join([f'r[\"_measurement\"] == \"{table}\"' for table in tables])})\n",
    "                |> filter(fn: (r) => {attributes_str})\n",
    "                |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n",
    "            '''\n",
    "            \n",
    "            logger.info(f\"Consultando InfluxDB: bucket={bucket}, tables={tables}, attributes={attributes}\")\n",
    "            \n",
    "            result = self.query_api.query_data_frame(query)\n",
    "            \n",
    "            if result.empty:\n",
    "                logger.warning(\"No se encontraron datos en la consulta.\")\n",
    "                return None\n",
    "                \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error en la consulta a InfluxDB: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔋 DESCARGA IV600 \n",
    "def download_iv600(start_date, end_date, output_dir):\n",
    "    \"\"\"Descarga y procesa datos de IV600 desde Clickhouse.\"\"\"\n",
    "    logger.info(\"Iniciando descarga de datos IV600...\")\n",
    "    client = None\n",
    "    \n",
    "    try:\n",
    "        # Conectar a Clickhouse\n",
    "        logger.info(\"Conectando a Clickhouse...\")\n",
    "        client = clickhouse_connect.get_client(\n",
    "            host=CLICKHOUSE_CONFIG['host'],\n",
    "            port=CLICKHOUSE_CONFIG['port'],\n",
    "            username=CLICKHOUSE_CONFIG['user'],\n",
    "            password=CLICKHOUSE_CONFIG['password']\n",
    "        )\n",
    "        logger.info(\"Conexión a Clickhouse establecida\")\n",
    "        \n",
    "        # Consultar datos\n",
    "        logger.info(\"Consultando datos IV600...\")\n",
    "        query = \"SELECT * FROM ref_data.iv_curves_trazador_manual\"\n",
    "        data_iv_curves = client.query(query)\n",
    "        logger.info(f\"Datos obtenidos: {len(data_iv_curves.result_set)} registros\")\n",
    "        \n",
    "        # Procesar datos\n",
    "        logger.info(\"Procesando datos...\")\n",
    "        curves_list = []\n",
    "        for curve in data_iv_curves.result_set:\n",
    "            currents = curve[4]\n",
    "            voltages = curve[3]\n",
    "            powers = [currents[i] * voltages[i] for i in range(len(currents))]\n",
    "            timestamp = curve[0]\n",
    "            module = curve[2]\n",
    "            pmp = max(powers)\n",
    "            isc = max(currents)\n",
    "            voc = max(voltages)\n",
    "            imp = currents[np.argmax(powers)]\n",
    "            vmp = voltages[np.argmax(powers)]\n",
    "            curves_list.append([timestamp, module, pmp, isc, voc, imp, vmp])\n",
    "\n",
    "        # Crear DataFrame\n",
    "        logger.info(\"Creando DataFrame...\")\n",
    "        column_names = [\"timestamp\", \"module\", \"pmp\", \"isc\", \"voc\", \"imp\", \"vmp\"]\n",
    "        df_curves = pd.DataFrame(curves_list, columns=column_names)\n",
    "        \n",
    "        # Convertir timestamp a datetime y asegurar que esté en UTC\n",
    "        df_curves['timestamp'] = pd.to_datetime(df_curves['timestamp'])\n",
    "        if df_curves['timestamp'].dt.tz is None:\n",
    "            df_curves['timestamp'] = df_curves['timestamp'].dt.tz_localize('UTC')\n",
    "        else:\n",
    "            df_curves['timestamp'] = df_curves['timestamp'].dt.tz_convert('UTC')\n",
    "        \n",
    "        df_curves.set_index('timestamp', inplace=True)\n",
    "        \n",
    "        # Mostrar información sobre el rango de fechas en los datos\n",
    "        logger.info(f\"Rango de fechas en los datos:\")\n",
    "        logger.info(f\"Fecha más antigua: {df_curves.index.min()}\")\n",
    "        logger.info(f\"Fecha más reciente: {df_curves.index.max()}\")\n",
    "        \n",
    "        # Filtrar por fecha usando query para mayor flexibilidad\n",
    "        logger.info(f\"Filtrando datos entre {start_date} y {end_date}...\")\n",
    "        df_curves = df_curves.query('@start_date <= index <= @end_date')\n",
    "        \n",
    "        if len(df_curves) == 0:\n",
    "            logger.warning(\"No se encontraron datos en el rango de fechas especificado.\")\n",
    "            logger.info(\"Ajustando el rango de fechas al rango disponible en los datos...\")\n",
    "            df_curves = df_curves.sort_index()\n",
    "        else:\n",
    "            logger.info(f\"Se encontraron {len(df_curves)} registros en el rango especificado.\")\n",
    "\n",
    "        # Guardar datos\n",
    "        output_filepath = os.path.join(output_dir, 'raw_iv600_data.csv')\n",
    "        logger.info(f\"Guardando datos en: {output_filepath}\")\n",
    "        df_curves.to_csv(output_filepath)\n",
    "        logger.info(f\"Datos guardados exitosamente. Total de registros: {len(df_curves)}\")\n",
    "        logger.info(f\"Rango de fechas: {df_curves.index.min()} a {df_curves.index.max()}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error en la descarga de datos IV600: {e}\")\n",
    "        import traceback\n",
    "        logger.error(f\"Detalles del error:\\n{traceback.format_exc()}\")\n",
    "        return False\n",
    "    finally:\n",
    "        if client:\n",
    "            logger.info(\"Cerrando conexión a Clickhouse...\")\n",
    "            client.close()\n",
    "            logger.info(\"Conexión a Clickhouse cerrada\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔋 DESCARGA PV GLASSES\n",
    "def download_pv_glasses(influx_client, start_date, end_date, output_dir):\n",
    "    \"\"\"Descarga y procesa datos de PV Glasses.\"\"\"\n",
    "    logger.info(\"Iniciando descarga de datos PV Glasses...\")\n",
    "    \n",
    "    try:\n",
    "        # Configuración de la consulta\n",
    "        bucket = \"meteo_psda\"\n",
    "        tables = [\"6852_Ftc\"]\n",
    "        attributes = [\"R_FC1_Avg\", \"R_FC2_Avg\", \"R_FC3_Avg\", \"R_FC4_Avg\", \"R_FC5_Avg\"]\n",
    "        \n",
    "        # Obtener datos\n",
    "        df_glasses = influx_client.query_influxdb(bucket, tables, attributes, start_date, end_date)\n",
    "        \n",
    "        if df_glasses is None or df_glasses.empty:\n",
    "            logger.warning(\"No se obtuvieron datos de PV Glasses\")\n",
    "            return False\n",
    "            \n",
    "        # Asegurar que el índice sea DatetimeIndex\n",
    "        if '_time' in df_glasses.columns:\n",
    "            df_glasses.set_index('_time', inplace=True)\n",
    "        elif 'time' in df_glasses.columns:\n",
    "            df_glasses.set_index('time', inplace=True)\n",
    "            \n",
    "        # Convertir el índice a DatetimeIndex si no lo es\n",
    "        if not isinstance(df_glasses.index, pd.DatetimeIndex):\n",
    "            df_glasses.index = pd.to_datetime(df_glasses.index)\n",
    "            \n",
    "        # Filtrar por horario (13:00 a 18:00)\n",
    "        df_glasses = df_glasses.between_time('13:00', '18:00')\n",
    "        \n",
    "        # Seleccionar solo las columnas numéricas para el cálculo\n",
    "        numeric_columns = df_glasses.select_dtypes(include=[np.number]).columns\n",
    "        df_glasses_numeric = df_glasses[numeric_columns]\n",
    "        \n",
    "        # Calcular referencia (promedio de R_FC1_Avg)\n",
    "        if 'R_FC1_Avg' in df_glasses_numeric.columns:\n",
    "            df_glasses['Ref'] = df_glasses_numeric['R_FC1_Avg'].mean()\n",
    "        \n",
    "        # Calcular datos diarios solo para columnas numéricas\n",
    "        df_glasses_daily = df_glasses_numeric.resample('1d').sum().div(60000)\n",
    "        \n",
    "        # Guardar datos\n",
    "        output_filepath = os.path.join(output_dir, 'raw_pv_glasses_data.csv')\n",
    "        daily_output_filepath = os.path.join(output_dir, 'raw_pv_glasses_daily_data.csv')\n",
    "        \n",
    "        df_glasses.to_csv(output_filepath)\n",
    "        df_glasses_daily.to_csv(daily_output_filepath)\n",
    "        \n",
    "        logger.info(f\"Datos PV Glasses guardados exitosamente\")\n",
    "        logger.info(f\"Total de registros: {len(df_glasses)}\")\n",
    "        logger.info(f\"Rango de fechas: {df_glasses.index.min()} a {df_glasses.index.max()}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error en la descarga de datos PV Glasses: {e}\")\n",
    "        import traceback\n",
    "        logger.error(f\"Detalles del error:\\n{traceback.format_exc()}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔋 DESCARGA DUSTIQ DESDE INFLUXDB\n",
    "def download_dustiq(influx_client, start_date, end_date, output_dir):\n",
    "    \"\"\"Descarga y procesa datos de DustIQ.\"\"\"\n",
    "    logger.info(\"Iniciando descarga de datos DustIQ...\")\n",
    "    \n",
    "    try:\n",
    "        # Configuración de la consulta\n",
    "        bucket = \"PSDA\"\n",
    "        tables = [\"DustIQ\"]\n",
    "        attributes = [\"SR_C11_Avg\", \"SR_C12_Avg\"]\n",
    "        \n",
    "        # Obtener datos\n",
    "        df_dustiq = influx_client.query_influxdb(bucket, tables, attributes, start_date, end_date)\n",
    "        \n",
    "        if df_dustiq is None or df_dustiq.empty:\n",
    "            logger.warning(\"No se obtuvieron datos de DustIQ\")\n",
    "            return False\n",
    "            \n",
    "        # Guardar datos\n",
    "        output_filepath = os.path.join(output_dir, 'raw_dustiq_data.csv')\n",
    "        df_dustiq.to_csv(output_filepath)\n",
    "        \n",
    "        logger.info(f\"Datos DustIQ guardados exitosamente\")\n",
    "        logger.info(f\"Total de registros: {len(df_dustiq)}\")\n",
    "        logger.info(f\"Rango de fechas: {df_dustiq.index.min()} a {df_dustiq.index.max()}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error en la descarga de datos DustIQ: {e}\")\n",
    "        import traceback\n",
    "        logger.error(f\"Detalles del error:\\n{traceback.format_exc()}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔋 DESCARGA DUSTIQ DESDE CLICKHOUSE\n",
    "def download_dustiq_clickhouse(start_date, end_date, output_dir):\n",
    "    \"\"\"Descarga y procesa datos de DustIQ desde ClickHouse.\"\"\"\n",
    "    logger.info(\"Iniciando descarga de datos DustIQ desde ClickHouse...\")\n",
    "    client = None\n",
    "    \n",
    "    try:\n",
    "        # Conectar a Clickhouse\n",
    "        logger.info(\"Conectando a Clickhouse...\")\n",
    "        client = clickhouse_connect.get_client(\n",
    "            host=CLICKHOUSE_CONFIG['host'],\n",
    "            port=CLICKHOUSE_CONFIG['port'],\n",
    "            username=CLICKHOUSE_CONFIG['user'],\n",
    "            password=CLICKHOUSE_CONFIG['password']\n",
    "        )\n",
    "        logger.info(\"Conexión a Clickhouse establecida\")\n",
    "        \n",
    "        # Convertir fechas al formato correcto para ClickHouse\n",
    "        start_str = start_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        end_str = end_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        # Consultar datos de dustiq desde el bucket PSDA\n",
    "        logger.info(\"Consultando datos DustIQ desde ClickHouse...\")\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            Stamptime,\n",
    "            Attribute,\n",
    "            Measure\n",
    "        FROM PSDA.dustiq \n",
    "        WHERE Stamptime >= '{start_str}' AND Stamptime <= '{end_str}'\n",
    "        AND Attribute IN ('SR_C11_Avg', 'SR_C12_Avg')\n",
    "        ORDER BY Stamptime, Attribute\n",
    "        \"\"\"\n",
    "        \n",
    "        logger.info(f\"Ejecutando consulta: {query[:100]}...\")\n",
    "        result = client.query(query)\n",
    "        \n",
    "        if not result.result_set:\n",
    "            logger.warning(\"No se encontraron datos de DustIQ en ClickHouse\")\n",
    "            return False\n",
    "            \n",
    "        logger.info(f\"Datos obtenidos: {len(result.result_set)} registros\")\n",
    "        \n",
    "        # Convertir a DataFrame\n",
    "        logger.info(\"Procesando datos...\")\n",
    "        df_dustiq = pd.DataFrame(result.result_set, columns=['Stamptime', 'Attribute', 'Measure'])        \n",
    "        # Convertir Stamptime a datetime y asegurar que esté en UTC\n",
    "        df_dustiq['Stamptime'] = pd.to_datetime(df_dustiq['Stamptime'])\n",
    "        if df_dustiq['Stamptime'].dt.tz is None:\n",
    "            df_dustiq['Stamptime'] = df_dustiq['Stamptime'].dt.tz_localize('UTC')\n",
    "        else:\n",
    "            df_dustiq['Stamptime'] = df_dustiq['Stamptime'].dt.tz_convert('UTC')\n",
    "\n",
    "        # Pivotar los datos para convertir de long format a wide format\n",
    "        logger.info(\"Pivotando datos de long format a wide format...\")\n",
    "\n",
    "        # Primero, manejar duplicados agregando por promedio\n",
    "        logger.info(\"Manejando duplicados agrupando por promedio...\")\n",
    "        df_dustiq_grouped = df_dustiq.groupby(['Stamptime', 'Attribute'])['Measure'].mean().reset_index()\n",
    "\n",
    "        # Ahora hacer el pivot sin duplicados\n",
    "        df_dustiq_pivot = df_dustiq_grouped.pivot(index='Stamptime', columns='Attribute', values='Measure')\n",
    "\n",
    "        # Renombrar el índice\n",
    "        df_dustiq_pivot.index.name = 'timestamp'\n",
    "        # Mostrar información sobre el rango de fechas en los datos\n",
    "        logger.info(f\"Rango de fechas en los datos:\")\n",
    "        logger.info(f\"Fecha más antigua: {df_dustiq_pivot.index.min()}\")\n",
    "        logger.info(f\"Fecha más reciente: {df_dustiq_pivot.index.max()}\")\n",
    "\n",
    "        # Verificar que hay datos en el rango especificado\n",
    "        if len(df_dustiq_pivot) == 0:\n",
    "            logger.warning(\"No se encontraron datos en el rango de fechas especificado.\")\n",
    "            return False\n",
    "\n",
    "        # Guardar datos\n",
    "        output_filepath = os.path.join(output_dir, 'raw_dustiq_data.csv')\n",
    "        logger.info(f\"Guardando datos en: {output_filepath}\")\n",
    "        df_dustiq_pivot.to_csv(output_filepath)\n",
    "\n",
    "        logger.info(f\"Datos DustIQ desde ClickHouse guardados exitosamente\")\n",
    "        logger.info(f\"Total de registros: {len(df_dustiq_pivot)}\")\n",
    "        logger.info(f\"Rango de fechas: {df_dustiq_pivot.index.min()} a {df_dustiq_pivot.index.max()}\")\n",
    "\n",
    "        # Mostrar estadísticas básicas\n",
    "        logger.info(\"Estadísticas de los datos:\")\n",
    "        if 'SR_C11_Avg' in df_dustiq_pivot.columns:\n",
    "            logger.info(f\"SR_C11_Avg - Rango: {df_dustiq_pivot['SR_C11_Avg'].min():.3f} a {df_dustiq_pivot['SR_C11_Avg'].max():.3f}\")\n",
    "        if 'SR_C12_Avg' in df_dustiq_pivot.columns:\n",
    "            logger.info(f\"SR_C12_Avg - Rango: {df_dustiq_pivot['SR_C12_Avg'].min():.3f} a {df_dustiq_pivot['SR_C12_Avg'].max():.3f}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error en la descarga de datos DustIQ desde ClickHouse: {e}\")\n",
    "        import traceback\n",
    "        logger.error(f\"Detalles del error:\\n{traceback.format_exc()}\")\n",
    "        return False\n",
    "    finally:\n",
    "        if client:\n",
    "            logger.info(\"Cerrando conexión a Clickhouse...\")\n",
    "            client.close()\n",
    "            logger.info(\"Conexión a Clickhouse cerrada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 🔋 DESCARGA PVSTAND DESDE CLICKHOUSE (CON IDENTIFICACIÓN DE MÓDULO)\n",
    "# ============================================================================\n",
    "\n",
    "def download_pvstand_clickhouse(start_date, end_date, output_dir):\n",
    "    \"\"\"Descarga y procesa datos de PVStand desde ClickHouse.\"\"\"\n",
    "    logger.info(\"Iniciando descarga de datos PVStand desde ClickHouse...\")\n",
    "    client = None\n",
    "    \n",
    "    try:\n",
    "        # Conectar a Clickhouse\n",
    "        logger.info(\"Conectando a Clickhouse...\")\n",
    "        client = clickhouse_connect.get_client(\n",
    "            host=CLICKHOUSE_CONFIG['host'],\n",
    "            port=CLICKHOUSE_CONFIG['port'],\n",
    "            username=CLICKHOUSE_CONFIG['user'],\n",
    "            password=CLICKHOUSE_CONFIG['password']\n",
    "        )\n",
    "        logger.info(\"Conexión a Clickhouse establecida\")\n",
    "        \n",
    "        # Convertir fechas al formato correcto para ClickHouse\n",
    "        start_str = start_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        end_str = end_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        # Consultar datos de PVStand desde las tablas perc1fixed y perc2fixed\n",
    "        logger.info(\"Consultando datos PVStand desde ClickHouse...\")\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            timestamp,\n",
    "            'perc1fixed' as module,\n",
    "            pmax,\n",
    "            imax,\n",
    "            umax\n",
    "        FROM PSDA.perc1fixed \n",
    "        WHERE timestamp >= '{start_str}' AND timestamp <= '{end_str}'\n",
    "        \n",
    "        UNION ALL\n",
    "        \n",
    "        SELECT \n",
    "            timestamp,\n",
    "            'perc2fixed' as module,\n",
    "            pmax,\n",
    "            imax,\n",
    "            umax\n",
    "        FROM PSDA.perc2fixed \n",
    "        WHERE timestamp >= '{start_str}' AND timestamp <= '{end_str}'\n",
    "        \n",
    "        ORDER BY timestamp\n",
    "        \"\"\"\n",
    "        \n",
    "        logger.info(f\"Ejecutando consulta: {query[:100]}...\")\n",
    "        result = client.query(query)\n",
    "        \n",
    "        if not result.result_set:\n",
    "            logger.warning(\"No se encontraron datos de PVStand en ClickHouse\")\n",
    "            return False\n",
    "            \n",
    "        logger.info(f\"Datos obtenidos: {len(result.result_set)} registros\")\n",
    "        \n",
    "        # Convertir a DataFrame\n",
    "        logger.info(\"Procesando datos...\")\n",
    "        df_pvstand = pd.DataFrame(result.result_set, columns=['timestamp', 'module', 'pmax', 'imax', 'umax'])\n",
    "        \n",
    "        # Convertir timestamp a datetime y asegurar que esté en UTC\n",
    "        df_pvstand['timestamp'] = pd.to_datetime(df_pvstand['timestamp'])\n",
    "        if df_pvstand['timestamp'].dt.tz is None:\n",
    "            df_pvstand['timestamp'] = df_pvstand['timestamp'].dt.tz_localize('UTC')\n",
    "        else:\n",
    "            df_pvstand['timestamp'] = df_pvstand['timestamp'].dt.tz_convert('UTC')\n",
    "\n",
    "        # Establecer timestamp como índice\n",
    "        df_pvstand.set_index('timestamp', inplace=True)\n",
    "        \n",
    "        # Ordenar por timestamp (importante para series temporales)\n",
    "        logger.info(\"Ordenando datos por timestamp...\")\n",
    "        df_pvstand = df_pvstand.sort_index()\n",
    "        \n",
    "        # Mostrar información sobre el rango de fechas en los datos\n",
    "        logger.info(f\"Rango de fechas en los datos:\")\n",
    "        logger.info(f\"Fecha más antigua: {df_pvstand.index.min()}\")\n",
    "        logger.info(f\"Fecha más reciente: {df_pvstand.index.max()}\")\n",
    "\n",
    "        # Verificar que hay datos en el rango especificado\n",
    "        if len(df_pvstand) == 0:\n",
    "            logger.warning(\"No se encontraron datos en el rango de fechas especificado.\")\n",
    "            return False\n",
    "\n",
    "        # Mostrar distribución por módulo\n",
    "        module_counts = df_pvstand['module'].value_counts()\n",
    "        logger.info(\"Distribución por módulo:\")\n",
    "        for module, count in module_counts.items():\n",
    "            logger.info(f\"   - {module}: {count} registros\")\n",
    "\n",
    "        # Guardar datos\n",
    "        output_filepath = os.path.join(output_dir, 'raw_pvstand_clickhouse_data.csv')\n",
    "        logger.info(f\"Guardando datos en: {output_filepath}\")\n",
    "        df_pvstand.to_csv(output_filepath)\n",
    "\n",
    "        logger.info(f\"Datos PVStand desde ClickHouse guardados exitosamente\")\n",
    "        logger.info(f\"Total de registros: {len(df_pvstand)}\")\n",
    "        logger.info(f\"Rango de fechas: {df_pvstand.index.min()} a {df_pvstand.index.max()}\")\n",
    "\n",
    "        # Mostrar estadísticas básicas por módulo\n",
    "        logger.info(\"Estadísticas de los datos por módulo:\")\n",
    "        for module in ['perc1fixed', 'perc2fixed']:\n",
    "            if module in df_pvstand['module'].values:\n",
    "                module_data = df_pvstand[df_pvstand['module'] == module]\n",
    "                logger.info(f\"\\n{module}:\")\n",
    "                logger.info(f\"   pmax - Rango: {module_data['pmax'].min():.3f} a {module_data['pmax'].max():.3f}\")\n",
    "                logger.info(f\"   imax - Rango: {module_data['imax'].min():.3f} a {module_data['imax'].max():.3f}\")\n",
    "                logger.info(f\"   umax - Rango: {module_data['umax'].min():.3f} a {module_data['umax'].max():.3f}\")\n",
    "        \n",
    "        # Mostrar información sobre la estructura de datos\n",
    "        logger.info(\"\\nEstructura de datos del PVStand:\")\n",
    "        logger.info(f\"   - module: Identificador del módulo (perc1fixed/perc2fixed)\")\n",
    "        logger.info(f\"   - pmax: Potencia máxima del módulo\")\n",
    "        logger.info(f\"   - imax: Corriente máxima del módulo\")\n",
    "        logger.info(f\"   - umax: Voltaje máximo del módulo\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error en la descarga de datos PVStand desde ClickHouse: {e}\")\n",
    "        import traceback\n",
    "        logger.error(f\"Detalles del error:\\n{traceback.format_exc()}\")\n",
    "        return False\n",
    "    finally:\n",
    "        if client:\n",
    "            logger.info(\"Cerrando conexión a Clickhouse...\")\n",
    "            client.close()\n",
    "            logger.info(\"Conexión a Clickhouse cerrada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pvstand(influx_client, start_date, end_date, output_dir):\n",
    "    \"\"\"Descarga y procesa datos de PVStand.\"\"\"\n",
    "    logger.info(\"Iniciando descarga de datos PVStand...\")\n",
    "    \n",
    "    try:\n",
    "        # Configuración de la consulta\n",
    "        bucket = \"PSDA\"\n",
    "        tables = [\"PERC1_fixed_1MD43420160719\", \"PERC2_fixed_1MD43920160719\"]\n",
    "        attributes = [\"Imax\", \"Umax\", \"Pmax\"]\n",
    "        \n",
    "        # Obtener datos\n",
    "        df_pvstand = influx_client.query_influxdb(bucket, tables, attributes, start_date, end_date)\n",
    "        \n",
    "        if df_pvstand is None or df_pvstand.empty:\n",
    "            logger.warning(\"No se obtuvieron datos de PVStand\")\n",
    "            return False\n",
    "            \n",
    "        # Guardar datos\n",
    "        output_filepath = os.path.join(output_dir, 'raw_pvstand_iv_data.csv')\n",
    "        df_pvstand.to_csv(output_filepath)\n",
    "        \n",
    "        logger.info(f\"Datos PVStand guardados exitosamente\")\n",
    "        logger.info(f\"Total de registros: {len(df_pvstand)}\")\n",
    "        logger.info(f\"Rango de fechas: {df_pvstand.index.min()} a {df_pvstand.index.max()}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error en la descarga de datos PVStand: {e}\")\n",
    "        import traceback\n",
    "        logger.error(f\"Detalles del error:\\n{traceback.format_exc()}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_soiling_kit(influx_client, start_date, end_date, output_dir):\n",
    "    \"\"\"Descarga y procesa datos del Soiling Kit.\"\"\"\n",
    "    logger.info(\"Iniciando descarga de datos del Soiling Kit...\")\n",
    "    \n",
    "    try:\n",
    "        # Configuración de la consulta\n",
    "        bucket = \"PSDA\"\n",
    "        tables = [\"soilingkit\"]\n",
    "        attributes = [\"Isc(e)\", \"Isc(p)\", \"Te(C)\", \"Tp(C)\"]\n",
    "        \n",
    "        # Obtener datos\n",
    "        df_sk = influx_client.query_influxdb(bucket, tables, attributes, start_date, end_date)\n",
    "        \n",
    "        if df_sk is None or df_sk.empty:\n",
    "            logger.warning(\"No se obtuvieron datos del Soiling Kit\")\n",
    "            return False\n",
    "            \n",
    "        # Guardar datos\n",
    "        output_filepath = os.path.join(output_dir, 'soiling_kit_raw_data.csv')\n",
    "        df_sk.to_csv(output_filepath)\n",
    "        \n",
    "        logger.info(f\"Datos del Soiling Kit guardados exitosamente\")\n",
    "        logger.info(f\"Total de registros: {len(df_sk)}\")\n",
    "        logger.info(f\"Rango de fechas: {df_sk.index.min()} a {df_sk.index.max()}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error en la descarga de datos del Soiling Kit: {e}\")\n",
    "        import traceback\n",
    "        logger.error(f\"Detalles del error:\\n{traceback.format_exc()}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 🌪️ DESCARGA SOILING KIT DESDE CLICKHOUSE\n",
    "# ============================================================================\n",
    "\n",
    "def download_soiling_kit_clickhouse(start_date, end_date, output_dir):\n",
    "    \"\"\"Descarga y procesa datos del Soiling Kit desde ClickHouse.\"\"\"\n",
    "    logger.info(\"Iniciando descarga de datos del Soiling Kit desde ClickHouse...\")\n",
    "    client = None\n",
    "    \n",
    "    try:\n",
    "        # Conectar a Clickhouse\n",
    "        logger.info(\"Conectando a Clickhouse...\")\n",
    "        client = clickhouse_connect.get_client(\n",
    "            host=CLICKHOUSE_CONFIG['host'],\n",
    "            port=CLICKHOUSE_CONFIG['port'],\n",
    "            username=CLICKHOUSE_CONFIG['user'],\n",
    "            password=CLICKHOUSE_CONFIG['password']\n",
    "        )\n",
    "        logger.info(\"Conexión a Clickhouse establecida\")\n",
    "        \n",
    "        # Convertir fechas al formato correcto para ClickHouse\n",
    "        start_str = start_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        end_str = end_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        # Consultar datos del Soiling Kit desde PSDA.soilingkit\n",
    "        logger.info(\"Consultando datos del Soiling Kit desde ClickHouse...\")\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            Stamptime,\n",
    "            Attribute,\n",
    "            Measure\n",
    "        FROM PSDA.soilingkit \n",
    "        WHERE Stamptime >= '{start_str}' AND Stamptime <= '{end_str}'\n",
    "        AND Attribute IN ('Isc(e)', 'Isc(p)', 'Te(C)', 'Tp(C)')\n",
    "        ORDER BY Stamptime, Attribute\n",
    "        \"\"\"\n",
    "        \n",
    "        logger.info(f\"Ejecutando consulta: {query[:100]}...\")\n",
    "        result = client.query(query)\n",
    "        \n",
    "        if not result.result_set:\n",
    "            logger.warning(\"No se encontraron datos del Soiling Kit en ClickHouse\")\n",
    "            return False\n",
    "            \n",
    "        logger.info(f\"Datos obtenidos: {len(result.result_set)} registros\")\n",
    "        \n",
    "        # Convertir a DataFrame\n",
    "        logger.info(\"Procesando datos...\")\n",
    "        df_soilingkit = pd.DataFrame(result.result_set, columns=['Stamptime', 'Attribute', 'Measure'])\n",
    "        \n",
    "        # Convertir Stamptime a datetime y asegurar que esté en UTC\n",
    "        df_soilingkit['Stamptime'] = pd.to_datetime(df_soilingkit['Stamptime'])\n",
    "        if df_soilingkit['Stamptime'].dt.tz is None:\n",
    "            df_soilingkit['Stamptime'] = df_soilingkit['Stamptime'].dt.tz_localize('UTC')\n",
    "        else:\n",
    "            df_soilingkit['Stamptime'] = df_soilingkit['Stamptime'].dt.tz_convert('UTC')\n",
    "\n",
    "        # Pivotar los datos para convertir de long format a wide format\n",
    "        logger.info(\"Pivotando datos de long format a wide format...\")\n",
    "\n",
    "        # Primero, manejar duplicados agregando por promedio\n",
    "        logger.info(\"Manejando duplicados agrupando por promedio...\")\n",
    "        df_soilingkit_grouped = df_soilingkit.groupby(['Stamptime', 'Attribute'])['Measure'].mean().reset_index()\n",
    "\n",
    "        # Ahora hacer el pivot sin duplicados\n",
    "        df_soilingkit_pivot = df_soilingkit_grouped.pivot(index='Stamptime', columns='Attribute', values='Measure')\n",
    "\n",
    "        # Renombrar el índice\n",
    "        df_soilingkit_pivot.index.name = 'timestamp'\n",
    "        \n",
    "        # Mostrar información sobre el rango de fechas en los datos\n",
    "        logger.info(f\"Rango de fechas en los datos:\")\n",
    "        logger.info(f\"Fecha más antigua: {df_soilingkit_pivot.index.min()}\")\n",
    "        logger.info(f\"Fecha más reciente: {df_soilingkit_pivot.index.max()}\")\n",
    "\n",
    "        # Verificar que hay datos en el rango especificado\n",
    "        if len(df_soilingkit_pivot) == 0:\n",
    "            logger.warning(\"No se encontraron datos en el rango de fechas especificado.\")\n",
    "            return False\n",
    "\n",
    "        # Guardar datos\n",
    "        output_filepath = os.path.join(output_dir, 'soiling_kit_clickhouse_data.csv')\n",
    "        logger.info(f\"Guardando datos en: {output_filepath}\")\n",
    "        df_soilingkit_pivot.to_csv(output_filepath)\n",
    "\n",
    "        logger.info(f\"Datos del Soiling Kit desde ClickHouse guardados exitosamente\")\n",
    "        logger.info(f\"Total de registros: {len(df_soilingkit_pivot)}\")\n",
    "        logger.info(f\"Rango de fechas: {df_soilingkit_pivot.index.min()} a {df_soilingkit_pivot.index.max()}\")\n",
    "\n",
    "        # Mostrar estadísticas básicas\n",
    "        logger.info(\"Estadísticas de los datos:\")\n",
    "        if 'Isc(e)' in df_soilingkit_pivot.columns:\n",
    "            logger.info(f\"Isc(e) - Rango: {df_soilingkit_pivot['Isc(e)'].min():.3f} a {df_soilingkit_pivot['Isc(e)'].max():.3f}\")\n",
    "        if 'Isc(p)' in df_soilingkit_pivot.columns:\n",
    "            logger.info(f\"Isc(p) - Rango: {df_soilingkit_pivot['Isc(p)'].min():.3f} a {df_soilingkit_pivot['Isc(p)'].max():.3f}\")\n",
    "        if 'Te(C)' in df_soilingkit_pivot.columns:\n",
    "            logger.info(f\"Te(C) - Rango: {df_soilingkit_pivot['Te(C)'].min():.1f} a {df_soilingkit_pivot['Te(C)'].max():.1f}\")\n",
    "        if 'Tp(C)' in df_soilingkit_pivot.columns:\n",
    "            logger.info(f\"Tp(C) - Rango: {df_soilingkit_pivot['Tp(C)'].min():.1f} a {df_soilingkit_pivot['Tp(C)'].max():.1f}\")\n",
    "        \n",
    "        # Mostrar información sobre la estructura de datos\n",
    "        logger.info(\"Estructura de datos del Soiling Kit:\")\n",
    "        logger.info(f\"   - Isc(e): Corriente de cortocircuito de la celda limpia (referencia)\")\n",
    "        logger.info(f\"   - Isc(p): Corriente de cortocircuito de la celda sucia (panel)\")\n",
    "        logger.info(f\"   - Te(C): Temperatura de la celda limpia en Celsius\")\n",
    "        logger.info(f\"   - Tp(C): Temperatura de la celda sucia en Celsius\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error en la descarga de datos del Soiling Kit desde ClickHouse: {e}\")\n",
    "        import traceback\n",
    "        logger.error(f\"Detalles del error:\\n{traceback.format_exc()}\")\n",
    "        return False\n",
    "    finally:\n",
    "        if client:\n",
    "            logger.info(\"Cerrando conexión a Clickhouse...\")\n",
    "            client.close()\n",
    "            logger.info(\"Conexión a Clickhouse cerrada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_temp_mod_fixed(influx_client, start_date, end_date, output_dir):\n",
    "    \"\"\"Descarga y procesa datos de temperatura de módulos (PT100).\"\"\"\n",
    "    logger.info(\"Iniciando descarga de datos de Temperatura de Módulos (PT100)...\")\n",
    "    \n",
    "    try:\n",
    "        bucket = \"PSDA\"\n",
    "        attributes = [\"1TE416(C)\", \"1TE417(C)\", \"1TE418(C)\", \"1TE419(C)\"] # Sensores PT100\n",
    "        \n",
    "        # Fuente 1: Tabla TempModFixed\n",
    "        tables_source1 = [\"TempModFixed\"]\n",
    "        logger.info(f\"Consultando datos de temperatura de {tables_source1}...\")\n",
    "        df_temp_source1 = influx_client.query_influxdb(bucket, tables_source1, attributes, start_date, end_date)\n",
    "        \n",
    "        if df_temp_source1 is not None and not df_temp_source1.empty:\n",
    "            if '_time' in df_temp_source1.columns:\n",
    "                df_temp_source1.set_index('_time', inplace=True)\n",
    "            elif 'time' in df_temp_source1.columns: # Por si acaso el nombre de la columna de tiempo varía\n",
    "                df_temp_source1.set_index('time', inplace=True)\n",
    "\n",
    "            if not isinstance(df_temp_source1.index, pd.DatetimeIndex):\n",
    "                df_temp_source1.index = pd.to_datetime(df_temp_source1.index)\n",
    "            \n",
    "            df_temp_source1 = df_temp_source1.between_time('13:00', '18:00') # Filtro horario como en soiling_intercomparison.py\n",
    "            logger.info(f\"Datos de {tables_source1} procesados. Registros: {len(df_temp_source1)}\")\n",
    "        else:\n",
    "            logger.warning(f\"No se obtuvieron datos de temperatura de {tables_source1} o el DataFrame está vacío.\")\n",
    "            df_temp_source1 = pd.DataFrame() # Asegurar que sea un DF vacío si no hay datos\n",
    "\n",
    "        # Fuente 2: Tabla fixed_plant_atamo_1 (como en soiling_intercomparison.py)\n",
    "        # El script original usa una fecha de inicio fija para esta fuente: pd.to_datetime('05/12/2024', dayfirst=True)\n",
    "        # Usaremos la 'start_date' global, pero puedes ajustarla si necesitas la fecha fija.\n",
    "        # Para replicar exactamente, podrías usar:\n",
    "        # date_s_fixed_plant = pd.to_datetime('05/12/2024', dayfirst=True).tz_localize('UTC')\n",
    "        # Y luego pasar date_s_fixed_plant a query_influxdb para esta fuente.\n",
    "        # Por ahora, usaremos start_date y end_date globales.\n",
    "        tables_source2 = [\"fixed_plant_atamo_1\"]\n",
    "        logger.info(f\"Consultando datos de temperatura de {tables_source2}...\")\n",
    "        df_temp_source2 = influx_client.query_influxdb(bucket, tables_source2, attributes, start_date, end_date)\n",
    "\n",
    "        if df_temp_source2 is not None and not df_temp_source2.empty:\n",
    "            if '_time' in df_temp_source2.columns:\n",
    "                df_temp_source2.set_index('_time', inplace=True)\n",
    "            elif 'time' in df_temp_source2.columns:\n",
    "                df_temp_source2.set_index('time', inplace=True)\n",
    "\n",
    "            if not isinstance(df_temp_source2.index, pd.DatetimeIndex):\n",
    "                df_temp_source2.index = pd.to_datetime(df_temp_source2.index)\n",
    "                \n",
    "            df_temp_source2 = df_temp_source2.between_time('13:00', '18:00')\n",
    "            logger.info(f\"Datos de {tables_source2} procesados. Registros: {len(df_temp_source2)}\")\n",
    "        else:\n",
    "            logger.warning(f\"No se obtuvieron datos de temperatura de {tables_source2} o el DataFrame está vacío.\")\n",
    "            df_temp_source2 = pd.DataFrame()\n",
    "\n",
    "        # Concatenar datos de ambas fuentes\n",
    "        dataframes_to_concat = []\n",
    "        if not df_temp_source1.empty:\n",
    "            dataframes_to_concat.append(df_temp_source1)\n",
    "        if not df_temp_source2.empty:\n",
    "            dataframes_to_concat.append(df_temp_source2)\n",
    "\n",
    "        if not dataframes_to_concat:\n",
    "            logger.warning(\"No hay datos de temperatura de ninguna fuente para concatenar.\")\n",
    "            return False\n",
    "            \n",
    "        df_temp_combined = pd.concat(dataframes_to_concat)\n",
    "        df_temp_combined = df_temp_combined.sort_index() # Ordenar por tiempo\n",
    "        \n",
    "        # Eliminar columnas que pueden ser resultado de la consulta y no son atributos (result, table)\n",
    "        cols_to_drop = [col for col in ['result', 'table'] if col in df_temp_combined.columns]\n",
    "        if cols_to_drop:\n",
    "            df_temp_combined.drop(columns=cols_to_drop, inplace=True)\n",
    "            \n",
    "        # Guardar datos\n",
    "        output_filepath = os.path.join(output_dir, 'temp_mod_fixed_data.csv')\n",
    "        df_temp_combined.to_csv(output_filepath)\n",
    "        \n",
    "        logger.info(f\"Datos de Temperatura de Módulos guardados exitosamente en {output_filepath}\")\n",
    "        logger.info(f\"Total de registros combinados: {len(df_temp_combined)}\")\n",
    "        if not df_temp_combined.empty:\n",
    "            logger.info(f\"Rango de fechas: {df_temp_combined.index.min()} a {df_temp_combined.index.max()}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error en la descarga de datos de Temperatura de Módulos: {e}\")\n",
    "        import traceback\n",
    "        logger.error(f\"Detalles del error:\\\\n{traceback.format_exc()}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_refcells(influx_client, start_date, end_date, output_dir):\n",
    "    \"\"\"Descarga y procesa datos de celdas de referencia.\"\"\"\n",
    "    logger.info(\"Iniciando descarga de datos de celdas de referencia...\")\n",
    "    \n",
    "    try:\n",
    "        # Configuración de la consulta\n",
    "        bucket = \"PSDA\"\n",
    "        tables1 = [\"RefCellsFixed\"]\n",
    "        tables2 = [\"fixed_plant_atamo_1\"]\n",
    "        attributes = [\"1RC410(w.m-2)\", \"1RC411(w.m-2)\", \"1RC412(w.m-2)\"]\n",
    "        \n",
    "        # Obtener datos de ambas fuentes\n",
    "        df1 = influx_client.query_influxdb(bucket, tables1, attributes, start_date, end_date)\n",
    "        df2 = influx_client.query_influxdb(bucket, tables2, attributes, start_date, end_date)\n",
    "        \n",
    "        if df1 is None and df2 is None:\n",
    "            logger.warning(\"No se obtuvieron datos de celdas de referencia\")\n",
    "            return False\n",
    "        \n",
    "        # Procesar DataFrames individualmente\n",
    "        processed_dfs = []\n",
    "        \n",
    "        for i, df in enumerate([df1, df2], 1):\n",
    "            if df is not None and not df.empty:\n",
    "                logger.info(f\"Procesando DataFrame {i} con {len(df)} registros\")\n",
    "                \n",
    "                # Asegurar que el índice sea DatetimeIndex\n",
    "                if '_time' in df.columns:\n",
    "                    df = df.set_index('_time')\n",
    "                elif 'time' in df.columns:\n",
    "                    df = df.set_index('time')\n",
    "                    \n",
    "                # Convertir el índice a DatetimeIndex si no lo es\n",
    "                if not isinstance(df.index, pd.DatetimeIndex):\n",
    "                    df.index = pd.to_datetime(df.index, format='mixed')\n",
    "                    \n",
    "                # Filtrar por ventana horaria (13:00 a 18:00)\n",
    "                df = df.between_time('13:00', '18:00')\n",
    "                \n",
    "                # Eliminar columnas innecesarias que pueden venir de InfluxDB\n",
    "                cols_to_drop = [col for col in ['result', 'table', '_start', '_stop', '_measurement'] if col in df.columns]\n",
    "                if cols_to_drop:\n",
    "                    df = df.drop(columns=cols_to_drop)\n",
    "                \n",
    "                processed_dfs.append(df)\n",
    "                logger.info(f\"DataFrame {i} procesado: {len(df)} registros después del filtrado\")\n",
    "            \n",
    "        if not processed_dfs:\n",
    "            logger.warning(\"No hay datos válidos para concatenar\")\n",
    "            return False\n",
    "            \n",
    "        # Concatenar datos procesados\n",
    "        df_combined = pd.concat(processed_dfs)\n",
    "        df_combined = df_combined.sort_index()\n",
    "        \n",
    "        # Resetear el índice para convertir el timestamp en una columna llamada '_time'\n",
    "        df_combined_reset = df_combined.reset_index()\n",
    "        df_combined_reset = df_combined_reset.rename(columns={'index': '_time'})\n",
    "        \n",
    "        # Formatear la columna _time a formato estándar\n",
    "        df_combined_reset['_time'] = pd.to_datetime(df_combined_reset['_time']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        # Guardar datos con _time como columna normal\n",
    "        output_filepath = os.path.join(output_dir, 'refcells_data.csv')\n",
    "        df_combined_reset.to_csv(output_filepath, index=False)\n",
    "        \n",
    "        logger.info(f\"Datos de celdas de referencia guardados exitosamente\")\n",
    "        logger.info(f\"Total de registros: {len(df_combined_reset)}\")\n",
    "        logger.info(f\"Columnas guardadas: {list(df_combined_reset.columns)}\")\n",
    "        logger.info(f\"Rango de fechas: {df_combined.index.min()} a {df_combined.index.max()}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error en la descarga de datos de celdas de referencia: {e}\")\n",
    "        import traceback\n",
    "        logger.error(f\"Detalles del error:\\n{traceback.format_exc()}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_all_data(start_date, end_date, output_dir):\n",
    "    \"\"\"Función principal que coordina la descarga de todos los datos.\"\"\"\n",
    "    logger.info(\"Iniciando proceso de descarga de datos...\")\n",
    "    \n",
    "    # Crear directorio de salida si no existe\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Inicializar resultados\n",
    "    results = {}\n",
    "    \n",
    "    try:\n",
    "        # Primero descargar datos de Clickhouse\n",
    "        logger.info(\"\\nIniciando descarga de datos desde Clickhouse...\")\n",
    "        results['iv600'] = download_iv600(start_date, end_date, output_dir)\n",
    "        logger.info(f\"Descarga IV600: {'Exitosa' if results['iv600'] else 'Fallida'}\")\n",
    "        \n",
    "        # Liberar memoria\n",
    "        gc.collect()\n",
    "        \n",
    "        # Luego descargar datos de InfluxDB\n",
    "        logger.info(\"\\nIniciando descargas desde InfluxDB...\")\n",
    "        \n",
    "        # Inicializar cliente InfluxDB\n",
    "        influx_manager = InfluxDBManager(INFLUX_CONFIG)\n",
    "        if not influx_manager.connect():\n",
    "            logger.error(\"No se pudo establecer conexión con InfluxDB\")\n",
    "            return results\n",
    "            \n",
    "        try:\n",
    "            # Descargar datos de PV Glasses\n",
    "            logger.info(\"\\nProcesando PV Glasses...\")\n",
    "            results['pv_glasses'] = download_pv_glasses(influx_manager, start_date, end_date, output_dir)\n",
    "            logger.info(f\"Descarga PV Glasses: {'Exitosa' if results['pv_glasses'] else 'Fallida'}\")\n",
    "            \n",
    "            # Descargar datos de DustIQ\n",
    "            logger.info(\"\\nProcesando DustIQ...\")\n",
    "            results['dustiq'] = download_dustiq(influx_manager, start_date, end_date, output_dir)\n",
    "            logger.info(f\"Descarga DustIQ: {'Exitosa' if results['dustiq'] else 'Fallida'}\")\n",
    "            \n",
    "            # Descargar datos de PVStand\n",
    "            logger.info(\"\\nProcesando PVStand...\")\n",
    "            results['pvstand'] = download_pvstand(influx_manager, start_date, end_date, output_dir)\n",
    "            logger.info(f\"Descarga PVStand: {'Exitosa' if results['pvstand'] else 'Fallida'}\")\n",
    "            \n",
    "            # Descargar datos de celdas de referencia (CON FECHA ESPECÍFICA Y PROCESAMIENTO)\n",
    "            logger.info(\"\\nProcesando celdas de referencia...\")\n",
    "            # Fecha específica para refcells: 23 de julio de 2024\n",
    "            refcells_start_date = pd.to_datetime('23/07/2024', dayfirst=True).tz_localize('UTC')\n",
    "            logger.info(f\"Usando fecha específica para RefCells: {refcells_start_date}\")\n",
    "            \n",
    "            results['refcells'] = download_refcells(influx_manager, refcells_start_date, end_date, output_dir)\n",
    "            logger.info(f\"Descarga celdas de referencia: {'Exitosa' if results['refcells'] else 'Fallida'}\")\n",
    "            \n",
    "            # PROCESAMIENTO ADICIONAL DE REFCELLS\n",
    "            if results['refcells']:\n",
    "                logger.info(\"Aplicando procesamiento adicional a RefCells...\")\n",
    "                try:\n",
    "                    # Leer el archivo generado\n",
    "                    input_filepath = os.path.join(output_dir, 'refcells_data.csv')\n",
    "                    df = pd.read_csv(input_filepath)\n",
    "                    \n",
    "                    # Establecer _time como índice\n",
    "                    df['_time'] = pd.to_datetime(df['_time'])\n",
    "                    df = df.set_index('_time')\n",
    "                    \n",
    "                    # Resample por minuto (promedio)\n",
    "                    logger.info(\"Aplicando resample por minuto a RefCells...\")\n",
    "                    df_resampled = df.resample('1min').mean()\n",
    "                    df_resampled = df_resampled.dropna(how='all')\n",
    "                    \n",
    "                    # Sobrescribir el archivo original con los datos procesados\n",
    "                    final_output_path = os.path.join(output_dir, 'refcells_data.csv')\n",
    "                    df_resampled.to_csv(final_output_path)\n",
    "                    \n",
    "                    logger.info(f\"RefCells procesadas: {len(df)} → {len(df_resampled)} registros\")\n",
    "                    results['refcells_processed'] = True\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error en procesamiento de RefCells: {e}\")\n",
    "                    results['refcells_processed'] = False\n",
    "            \n",
    "            # Descargar datos del Soiling Kit\n",
    "            logger.info(\"\\nProcesando Soiling Kit...\")\n",
    "            results['soiling_kit'] = download_soiling_kit(influx_manager, start_date, end_date, output_dir)\n",
    "            logger.info(f\"Descarga Soiling Kit: {'Exitosa' if results['soiling_kit'] else 'Fallida'}\")\n",
    "\n",
    "            # Descargar datos de Temperatura de Módulos\n",
    "            logger.info(\"\\nProcesando Temperatura de Módulos (PT100)...\")\n",
    "            results['temp_mod_fixed'] = download_temp_mod_fixed(influx_manager, start_date, end_date, output_dir)\n",
    "            logger.info(f\"Descarga Temperatura de Módulos: {'Exitosa' if results['temp_mod_fixed'] else 'Fallida'}\")\n",
    "\n",
    "        finally:\n",
    "            # Cerrar conexión con InfluxDB\n",
    "            influx_manager.disconnect()\n",
    "            \n",
    "        # Resumen de resultados\n",
    "        logger.info(\"\\nResumen de descargas:\")\n",
    "        for key, value in results.items():\n",
    "            logger.info(f\"{key}: {'Exitosa' if value else 'Fallida'}\")\n",
    "            \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error en el proceso de descarga: {e}\")\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 13:50:57 - INFO - \n",
      "================================================================================\n",
      "2025-07-25 13:50:57 - INFO - 🌪️ DESCARGA ACTUALIZADA: SOILING KIT\n",
      "2025-07-25 13:50:57 - INFO - ================================================================================\n",
      "2025-07-25 13:50:57 - INFO - 📅 Rango actualizado: 2024-07-01 a 2025-07-31\n",
      "2025-07-25 13:50:57 - INFO - 📁 Directorio de salida: /home/nicole/SR/SOILING/datos\n",
      "2025-07-25 13:50:57 - INFO - Cliente InfluxDB y query_api inicializados.\n",
      "2025-07-25 13:50:57 - INFO - ✅ Conexión a InfluxDB establecida\n",
      "2025-07-25 13:50:57 - INFO - \n",
      "🌪️ Descargando datos actualizados del Soiling Kit...\n",
      "2025-07-25 13:50:57 - INFO - Iniciando descarga de datos del Soiling Kit...\n",
      "2025-07-25 13:50:57 - INFO - Consultando InfluxDB: bucket=PSDA, tables=['soilingkit'], attributes=['Isc(e)', 'Isc(p)', 'Te(C)', 'Tp(C)']\n",
      "2025-07-25 13:51:04 - INFO - Datos del Soiling Kit guardados exitosamente\n",
      "2025-07-25 13:51:04 - INFO - Total de registros: 68282\n",
      "2025-07-25 13:51:04 - INFO - Rango de fechas: 0 a 68281\n",
      "2025-07-25 13:51:04 - INFO - \n",
      "============================================================\n",
      "2025-07-25 13:51:04 - INFO - 🎉 ¡DESCARGA ACTUALIZADA COMPLETADA!\n",
      "2025-07-25 13:51:04 - INFO - 📂 Archivo: soiling_kit_raw_data.csv\n",
      "2025-07-25 13:51:04 - INFO - 📊 Tamaño: 9.09 MB\n",
      "2025-07-25 13:51:04 - INFO - 📈 Total de registros: 68,282\n",
      "2025-07-25 13:51:04 - INFO - 🗂️ Ubicación: /home/nicole/SR/SOILING/datos\n",
      "2025-07-25 13:51:04 - INFO - 🌪️ Datos incluyen: Isc(e), Isc(p), Te(C), Tp(C)\n",
      "2025-07-25 13:51:04 - INFO - 📅 Período: Julio 2024 - Julio 2025\n",
      "2025-07-25 13:51:04 - INFO - ============================================================\n",
      "2025-07-25 13:51:04 - INFO - \n",
      "📋 Muestra de los datos:\n",
      "2025-07-25 13:51:04 - INFO -    Columnas: ['Unnamed: 0', 'result', 'table', '_start', '_stop', '_time', '_measurement', 'device', 'Isc(e)', 'Isc(p)', 'Te(C)', 'Tp(C)']\n",
      "2025-07-25 13:51:04 - INFO -    Isc(e) rango: 1.029 - 1.032\n",
      "2025-07-25 13:51:04 - INFO -    Isc(p) rango: 1.078 - 1.080\n",
      "2025-07-25 13:51:04 - INFO - Conexión a InfluxDB cerrada.\n",
      "2025-07-25 13:51:04 - INFO - 🔌 Conexión cerrada\n",
      "2025-07-25 13:51:04 - INFO - \n",
      "🏁 DESCARGA ACTUALIZADA COMPLETADA\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 🌪️ DESCARGA ACTUALIZADA: SOILING KIT DESDE JULIO 2024 HASTA ACTUALIDAD\n",
    "# ============================================================================\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"🌪️ DESCARGA ACTUALIZADA: SOILING KIT\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "# Configurar fechas: desde julio 2024 hasta la actualidad\n",
    "START_DATE_SK = pd.to_datetime('01/07/2024', dayfirst=True).tz_localize('UTC')  # Desde julio 2024\n",
    "END_DATE_SK = pd.to_datetime('31/07/2025', dayfirst=True).tz_localize('UTC')    # Hasta julio 2025 (actualidad)\n",
    "\n",
    "logger.info(f\"📅 Rango actualizado: {START_DATE_SK.strftime('%Y-%m-%d')} a {END_DATE_SK.strftime('%Y-%m-%d')}\")\n",
    "logger.info(f\"📁 Directorio de salida: {OUTPUT_DIR}\")\n",
    "\n",
    "# Inicializar cliente InfluxDB\n",
    "influx_manager = InfluxDBManager(INFLUX_CONFIG)\n",
    "\n",
    "try:\n",
    "    if not influx_manager.connect():\n",
    "        logger.error(\"❌ No se pudo establecer conexión con InfluxDB\")\n",
    "    else:\n",
    "        logger.info(\"✅ Conexión a InfluxDB establecida\")\n",
    "        \n",
    "        # Ejecutar descarga del Soiling Kit\n",
    "        logger.info(\"\\n🌪️ Descargando datos actualizados del Soiling Kit...\")\n",
    "        success = download_soiling_kit(influx_manager, START_DATE_SK, END_DATE_SK, OUTPUT_DIR)\n",
    "        \n",
    "        if success:\n",
    "            # Verificar archivo generado\n",
    "            output_file = os.path.join(OUTPUT_DIR, 'soiling_kit_raw_data.csv')\n",
    "            if os.path.exists(output_file):\n",
    "                file_size_mb = os.path.getsize(output_file) / (1024*1024)\n",
    "                total_lines = sum(1 for line in open(output_file)) - 1\n",
    "                \n",
    "                logger.info(\"\\n\" + \"=\"*60)\n",
    "                logger.info(\"🎉 ¡DESCARGA ACTUALIZADA COMPLETADA!\")\n",
    "                logger.info(f\"📂 Archivo: soiling_kit_raw_data.csv\")\n",
    "                logger.info(f\"📊 Tamaño: {file_size_mb:.2f} MB\")\n",
    "                logger.info(f\"📈 Total de registros: {total_lines:,}\")\n",
    "                logger.info(f\"🗂️ Ubicación: {OUTPUT_DIR}\")\n",
    "                logger.info(\"🌪️ Datos incluyen: Isc(e), Isc(p), Te(C), Tp(C)\")\n",
    "                logger.info(\"📅 Período: Julio 2024 - Julio 2025\")\n",
    "                logger.info(\"=\"*60)\n",
    "                \n",
    "                # Mostrar muestra de los datos\n",
    "                try:\n",
    "                    df_sample = pd.read_csv(output_file, nrows=5)\n",
    "                    logger.info(\"\\n📋 Muestra de los datos:\")\n",
    "                    logger.info(f\"   Columnas: {list(df_sample.columns)}\")\n",
    "                    if 'Isc(e)' in df_sample.columns:\n",
    "                        logger.info(f\"   Isc(e) rango: {df_sample['Isc(e)'].min():.3f} - {df_sample['Isc(e)'].max():.3f}\")\n",
    "                    if 'Isc(p)' in df_sample.columns:\n",
    "                        logger.info(f\"   Isc(p) rango: {df_sample['Isc(p)'].min():.3f} - {df_sample['Isc(p)'].max():.3f}\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"⚠️ Error al mostrar muestra: {e}\")\n",
    "            else:\n",
    "                logger.warning(\"⚠️ Archivo no encontrado después de la descarga\")\n",
    "        else:\n",
    "            logger.error(\"❌ Error en la descarga del Soiling Kit\")\n",
    "            \n",
    "except Exception as e:\n",
    "    logger.error(f\"❌ Error general: {e}\")\n",
    "    import traceback\n",
    "    logger.error(f\"🔍 Detalles:\\\\n{traceback.format_exc()}\")\n",
    "    \n",
    "finally:\n",
    "    influx_manager.disconnect()\n",
    "    logger.info(\"🔌 Conexión cerrada\")\n",
    "\n",
    "logger.info(\"\\n🏁 DESCARGA ACTUALIZADA COMPLETADA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 11:47:24 - INFO - \n",
      "================================================================================\n",
      "2025-06-19 11:47:24 - INFO - 🚀 INICIANDO DESCARGA COMPLETA DE TODOS LOS DATOS\n",
      "2025-06-19 11:47:24 - INFO - ================================================================================\n",
      "2025-06-19 11:47:24 - INFO - 📅 Rango general: 2024-07-01 a 2025-12-31\n",
      "2025-06-19 11:47:24 - INFO - 📁 Directorio de salida: /home/nicole/SR/SOILING/datos\n",
      "2025-06-19 11:47:24 - INFO - 📂 RefCells usará fecha específica: 2024-07-23\n",
      "2025-06-19 11:47:24 - INFO - Iniciando proceso de descarga de datos...\n",
      "2025-06-19 11:47:24 - INFO - \n",
      "Iniciando descarga de datos desde Clickhouse...\n",
      "2025-06-19 11:47:24 - INFO - Iniciando descarga de datos IV600...\n",
      "2025-06-19 11:47:24 - INFO - Conectando a Clickhouse...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 11:47:25 - INFO - Conexión a Clickhouse establecida\n",
      "2025-06-19 11:47:25 - INFO - Consultando datos IV600...\n",
      "2025-06-19 11:47:26 - INFO - Datos obtenidos: 2029 registros\n",
      "2025-06-19 11:47:26 - INFO - Procesando datos...\n",
      "2025-06-19 11:47:26 - INFO - Creando DataFrame...\n",
      "2025-06-19 11:47:26 - INFO - Rango de fechas en los datos:\n",
      "2025-06-19 11:47:26 - INFO - Fecha más antigua: 2024-09-24 12:16:00+00:00\n",
      "2025-06-19 11:47:26 - INFO - Fecha más reciente: 2025-04-22 12:43:00+00:00\n",
      "2025-06-19 11:47:26 - INFO - Filtrando datos entre 2024-07-01 00:00:00+00:00 y 2025-12-31 00:00:00+00:00...\n",
      "2025-06-19 11:47:26 - INFO - Se encontraron 2029 registros en el rango especificado.\n",
      "2025-06-19 11:47:26 - INFO - Guardando datos en: /home/nicole/SR/SOILING/datos/raw_iv600_data.csv\n",
      "2025-06-19 11:47:26 - INFO - Datos guardados exitosamente. Total de registros: 2029\n",
      "2025-06-19 11:47:26 - INFO - Rango de fechas: 2024-09-24 12:16:00+00:00 a 2025-04-22 12:43:00+00:00\n",
      "2025-06-19 11:47:26 - INFO - Cerrando conexión a Clickhouse...\n",
      "2025-06-19 11:47:26 - INFO - Conexión a Clickhouse cerrada\n",
      "2025-06-19 11:47:26 - INFO - Descarga IV600: Exitosa\n",
      "2025-06-19 11:47:26 - INFO - \n",
      "Iniciando descargas desde InfluxDB...\n",
      "2025-06-19 11:47:26 - INFO - Cliente InfluxDB y query_api inicializados.\n",
      "2025-06-19 11:47:26 - INFO - \n",
      "Procesando PV Glasses...\n",
      "2025-06-19 11:47:26 - INFO - Iniciando descarga de datos PV Glasses...\n",
      "2025-06-19 11:47:26 - INFO - Consultando InfluxDB: bucket=meteo_psda, tables=['6852_Ftc'], attributes=['R_FC1_Avg', 'R_FC2_Avg', 'R_FC3_Avg', 'R_FC4_Avg', 'R_FC5_Avg']\n",
      "2025-06-19 11:47:48 - INFO - Datos PV Glasses guardados exitosamente\n",
      "2025-06-19 11:47:48 - INFO - Total de registros: 83157\n",
      "2025-06-19 11:47:48 - INFO - Rango de fechas: 2024-09-05 13:00:00+00:00 a 2025-06-19 15:40:00+00:00\n",
      "2025-06-19 11:47:48 - INFO - Descarga PV Glasses: Exitosa\n",
      "2025-06-19 11:47:48 - INFO - \n",
      "Procesando DustIQ...\n",
      "2025-06-19 11:47:48 - INFO - Iniciando descarga de datos DustIQ...\n",
      "2025-06-19 11:47:48 - INFO - Consultando InfluxDB: bucket=PSDA, tables=['DustIQ'], attributes=['SR_C11_Avg', 'SR_C12_Avg']\n",
      "2025-06-19 11:48:19 - INFO - Datos DustIQ guardados exitosamente\n",
      "2025-06-19 11:48:19 - INFO - Total de registros: 492075\n",
      "2025-06-19 11:48:19 - INFO - Rango de fechas: 0 a 492074\n",
      "2025-06-19 11:48:19 - INFO - Descarga DustIQ: Exitosa\n",
      "2025-06-19 11:48:19 - INFO - \n",
      "Procesando PVStand...\n",
      "2025-06-19 11:48:19 - INFO - Iniciando descarga de datos PVStand...\n",
      "2025-06-19 11:48:19 - INFO - Consultando InfluxDB: bucket=PSDA, tables=['PERC1_fixed_1MD43420160719', 'PERC2_fixed_1MD43920160719'], attributes=['Imax', 'Umax', 'Pmax']\n",
      "2025-06-19 11:48:31 - INFO - Datos PVStand guardados exitosamente\n",
      "2025-06-19 11:48:31 - INFO - Total de registros: 199254\n",
      "2025-06-19 11:48:31 - INFO - Rango de fechas: 0 a 199253\n",
      "2025-06-19 11:48:31 - INFO - Descarga PVStand: Exitosa\n",
      "2025-06-19 11:48:31 - INFO - \n",
      "Procesando celdas de referencia...\n",
      "2025-06-19 11:48:31 - INFO - Usando fecha específica para RefCells: 2024-07-23 00:00:00+00:00\n",
      "2025-06-19 11:48:31 - INFO - Iniciando descarga de datos de celdas de referencia...\n",
      "2025-06-19 11:48:31 - INFO - Consultando InfluxDB: bucket=PSDA, tables=['RefCellsFixed'], attributes=['1RC410(w.m-2)', '1RC411(w.m-2)', '1RC412(w.m-2)']\n",
      "2025-06-19 11:49:42 - INFO - Consultando InfluxDB: bucket=PSDA, tables=['fixed_plant_atamo_1'], attributes=['1RC410(w.m-2)', '1RC411(w.m-2)', '1RC412(w.m-2)']\n",
      "2025-06-19 11:49:58 - INFO - Procesando DataFrame 1 con 2303110 registros\n",
      "2025-06-19 11:49:59 - INFO - DataFrame 1 procesado: 476185 registros después del filtrado\n",
      "2025-06-19 11:49:59 - INFO - Procesando DataFrame 2 con 687839 registros\n",
      "2025-06-19 11:49:59 - INFO - DataFrame 2 procesado: 141905 registros después del filtrado\n",
      "2025-06-19 11:50:17 - INFO - Datos de celdas de referencia guardados exitosamente\n",
      "2025-06-19 11:50:17 - INFO - Total de registros: 618090\n",
      "2025-06-19 11:50:17 - INFO - Columnas guardadas: ['_time', '1RC410(w.m-2)', '1RC411(w.m-2)', '1RC412(w.m-2)']\n",
      "2025-06-19 11:50:17 - INFO - Rango de fechas: 2024-07-23 13:00:01+00:00 a 2025-06-19 15:49:04.432230+00:00\n",
      "2025-06-19 11:50:18 - INFO - Descarga celdas de referencia: Exitosa\n",
      "2025-06-19 11:50:18 - INFO - Aplicando procesamiento adicional a RefCells...\n",
      "2025-06-19 11:50:20 - INFO - Aplicando resample por minuto a RefCells...\n",
      "2025-06-19 11:50:24 - INFO - RefCells procesadas: 618090 → 88464 registros\n",
      "2025-06-19 11:50:24 - INFO - \n",
      "Procesando Soiling Kit...\n",
      "2025-06-19 11:50:24 - INFO - Iniciando descarga de datos del Soiling Kit...\n",
      "2025-06-19 11:50:24 - INFO - Consultando InfluxDB: bucket=PSDA, tables=['soilingkit'], attributes=['Isc(e)', 'Isc(p)', 'Te(C)', 'Tp(C)']\n",
      "2025-06-19 11:50:32 - INFO - Datos del Soiling Kit guardados exitosamente\n",
      "2025-06-19 11:50:32 - INFO - Total de registros: 58325\n",
      "2025-06-19 11:50:32 - INFO - Rango de fechas: 0 a 58324\n",
      "2025-06-19 11:50:32 - INFO - Descarga Soiling Kit: Exitosa\n",
      "2025-06-19 11:50:32 - INFO - \n",
      "Procesando Temperatura de Módulos (PT100)...\n",
      "2025-06-19 11:50:32 - INFO - Iniciando descarga de datos de Temperatura de Módulos (PT100)...\n",
      "2025-06-19 11:50:32 - INFO - Consultando datos de temperatura de ['TempModFixed']...\n",
      "2025-06-19 11:50:32 - INFO - Consultando InfluxDB: bucket=PSDA, tables=['TempModFixed'], attributes=['1TE416(C)', '1TE417(C)', '1TE418(C)', '1TE419(C)']\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# === EJECUCIÓN PRINCIPAL: DESCARGA COMPLETA DE TODOS LOS DATOS ===\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"🚀 INICIANDO DESCARGA COMPLETA DE TODOS LOS DATOS\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "logger.info(f\"📅 Rango general: {START_DATE.strftime('%Y-%m-%d')} a {END_DATE.strftime('%Y-%m-%d')}\")\n",
    "logger.info(f\"📁 Directorio de salida: {OUTPUT_DIR}\")\n",
    "logger.info(f\"📂 RefCells usará fecha específica: 2024-07-23\")\n",
    "\n",
    "# Ejecutar descarga completa\n",
    "results = download_all_data(START_DATE, END_DATE, OUTPUT_DIR)\n",
    "\n",
    "# Resumen final detallado\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"📊 RESUMEN FINAL DE DESCARGAS\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "total_successful = sum(1 for v in results.values() if v)\n",
    "total_processes = len(results)\n",
    "\n",
    "logger.info(f\"✅ Procesos exitosos: {total_successful}/{total_processes}\")\n",
    "\n",
    "# Detalle por proceso\n",
    "status_emoji = lambda x: \"✅\" if x else \"❌\"\n",
    "for process, success in results.items():\n",
    "    process_name = {\n",
    "        'iv600': 'IV600 (Clickhouse)',\n",
    "        'pv_glasses': 'PV Glasses',\n",
    "        'dustiq': 'DustIQ', \n",
    "        'pvstand': 'PVStand',\n",
    "        'refcells': 'RefCells (Descarga)',\n",
    "        'refcells_processed': 'RefCells (Procesamiento)',\n",
    "        'soiling_kit': 'Soiling Kit',\n",
    "        'temp_mod_fixed': 'Temperatura Módulos'\n",
    "    }.get(process, process)\n",
    "    \n",
    "    logger.info(f\"{status_emoji(success)} {process_name}\")\n",
    "\n",
    "if total_successful == total_processes:\n",
    "    logger.info(\"\\n🎉 ¡TODOS LOS PROCESOS COMPLETADOS EXITOSAMENTE!\")\n",
    "    logger.info(\"📂 Archivos generados en: \" + OUTPUT_DIR)\n",
    "    logger.info(\"🔍 RefCells procesado con resample 1min y guardado como refcells_data.csv\")\n",
    "else:\n",
    "    logger.warning(f\"\\n⚠️ {total_processes - total_successful} proceso(s) fallaron\")\n",
    "    logger.info(\"🔧 Revisa los logs anteriores para detalles de errores\")\n",
    "\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(\"🏁 PROCESO COMPLETO FINALIZADO\")\n",
    "logger.info(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Para descargar todo de una vez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_all_data_optimized(start_date, end_date, output_dir):\n",
    "    \"\"\"Función principal optimizada para manejo de memoria.\"\"\"\n",
    "    import gc\n",
    "    \n",
    "    logger.info(\"Iniciando proceso de descarga optimizado...\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    results = {}\n",
    "    \n",
    "    try:\n",
    "        # IV600 (Clickhouse)\n",
    "        logger.info(\"\\n🔹 Descargando IV600...\")\n",
    "        gc.collect()\n",
    "        results['iv600'] = download_iv600(start_date, end_date, output_dir)\n",
    "        logger.info(f\"IV600: {'✅' if results['iv600'] else '❌'}\")\n",
    "        gc.collect()\n",
    "        \n",
    "        # InfluxDB Manager - reutilizar conexión pero limpiar datos\n",
    "        influx_manager = InfluxDBManager(INFLUX_CONFIG)\n",
    "        if not influx_manager.connect():\n",
    "            return results\n",
    "            \n",
    "        try:\n",
    "            # PV Glasses\n",
    "            logger.info(\"\\n🔹 Descargando PV Glasses...\")\n",
    "            gc.collect()\n",
    "            results['pv_glasses'] = download_pv_glasses(influx_manager, start_date, end_date, output_dir)\n",
    "            logger.info(f\"PV Glasses: {'✅' if results['pv_glasses'] else '❌'}\")\n",
    "            gc.collect()\n",
    "            \n",
    "            # DustIQ\n",
    "            logger.info(\"\\n🔹 Descargando DustIQ...\")\n",
    "            gc.collect()\n",
    "            results['dustiq'] = download_dustiq(influx_manager, start_date, end_date, output_dir)\n",
    "            logger.info(f\"DustIQ: {'✅' if results['dustiq'] else '❌'}\")\n",
    "            gc.collect()\n",
    "            \n",
    "            # PVStand\n",
    "            logger.info(\"\\n🔹 Descargando PVStand...\")\n",
    "            gc.collect()\n",
    "            results['pvstand'] = download_pvstand(influx_manager, start_date, end_date, output_dir)\n",
    "            logger.info(f\"PVStand: {'✅' if results['pvstand'] else '❌'}\")\n",
    "            gc.collect()\n",
    "            \n",
    "            # RefCells con procesamiento optimizado\n",
    "            logger.info(\"\\n🔹 Descargando RefCells...\")\n",
    "            refcells_start = pd.to_datetime('23/07/2024', dayfirst=True).tz_localize('UTC')\n",
    "            gc.collect()\n",
    "            results['refcells'] = download_refcells(influx_manager, refcells_start, end_date, output_dir)\n",
    "            logger.info(f\"RefCells Descarga: {'✅' if results['refcells'] else '❌'}\")\n",
    "            \n",
    "            # Procesamiento RefCells\n",
    "            if results['refcells']:\n",
    "                logger.info(\"🔹 Procesando RefCells...\")\n",
    "                try:\n",
    "                    input_filepath = os.path.join(output_dir, 'refcells_data.csv')\n",
    "                    \n",
    "                    # Procesar en chunks\n",
    "                    chunks = []\n",
    "                    for chunk in pd.read_csv(input_filepath, chunksize=30000):\n",
    "                        chunk['_time'] = pd.to_datetime(chunk['_time'])\n",
    "                        chunk = chunk.set_index('_time')\n",
    "                        chunks.append(chunk)\n",
    "                    \n",
    "                    df = pd.concat(chunks)\n",
    "                    del chunks\n",
    "                    gc.collect()\n",
    "                    \n",
    "                    df_resampled = df.resample('1min').mean().dropna(how='all')\n",
    "                    del df\n",
    "                    gc.collect()\n",
    "                    \n",
    "                    df_resampled.to_csv(os.path.join(output_dir, 'refcells_data.csv'))\n",
    "                    results['refcells_processed'] = True\n",
    "                    logger.info(f\"RefCells Procesamiento: ✅ ({len(df_resampled)} registros)\")\n",
    "                    del df_resampled\n",
    "                    gc.collect()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error procesando RefCells: {e}\")\n",
    "                    results['refcells_processed'] = False\n",
    "            \n",
    "            # Soiling Kit\n",
    "            logger.info(\"\\n🔹 Descargando Soiling Kit...\")\n",
    "            gc.collect()\n",
    "            results['soiling_kit'] = download_soiling_kit(influx_manager, start_date, end_date, output_dir)\n",
    "            logger.info(f\"Soiling Kit: {'✅' if results['soiling_kit'] else '❌'}\")\n",
    "            gc.collect()\n",
    "            \n",
    "            # Temperatura\n",
    "            logger.info(\"\\n🔹 Descargando Temperatura...\")\n",
    "            gc.collect()\n",
    "            results['temp_mod_fixed'] = download_temp_mod_fixed(influx_manager, start_date, end_date, output_dir)\n",
    "            logger.info(f\"Temperatura: {'✅' if results['temp_mod_fixed'] else '❌'}\")\n",
    "            gc.collect()\n",
    "            \n",
    "        finally:\n",
    "            influx_manager.disconnect()\n",
    "            \n",
    "        # Resumen\n",
    "        successful = sum(1 for v in results.values() if v)\n",
    "        logger.info(f\"\\n🎉 Completado: {successful}/{len(results)} exitosos\")\n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error general: {e}\")\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 11:58:17 - INFO - 🚀 Iniciando descarga completa optimizada...\n",
      "2025-06-19 11:58:17 - INFO - Iniciando proceso de descarga optimizado...\n",
      "2025-06-19 11:58:17 - INFO - \n",
      "🔹 Descargando IV600...\n",
      "2025-06-19 11:58:17 - INFO - Iniciando descarga de datos IV600...\n",
      "2025-06-19 11:58:17 - INFO - Conectando a Clickhouse...\n",
      "2025-06-19 11:58:17 - INFO - Conexión a Clickhouse establecida\n",
      "2025-06-19 11:58:17 - INFO - Consultando datos IV600...\n",
      "2025-06-19 11:58:18 - INFO - Datos obtenidos: 2029 registros\n",
      "2025-06-19 11:58:18 - INFO - Procesando datos...\n",
      "2025-06-19 11:58:18 - INFO - Creando DataFrame...\n",
      "2025-06-19 11:58:18 - INFO - Rango de fechas en los datos:\n",
      "2025-06-19 11:58:18 - INFO - Fecha más antigua: 2024-09-24 12:16:00+00:00\n",
      "2025-06-19 11:58:18 - INFO - Fecha más reciente: 2025-04-22 12:43:00+00:00\n",
      "2025-06-19 11:58:18 - INFO - Filtrando datos entre 2024-07-01 00:00:00+00:00 y 2025-12-31 00:00:00+00:00...\n",
      "2025-06-19 11:58:18 - INFO - Se encontraron 2029 registros en el rango especificado.\n",
      "2025-06-19 11:58:18 - INFO - Guardando datos en: /home/nicole/SR/SOILING/datos/raw_iv600_data.csv\n",
      "2025-06-19 11:58:18 - INFO - Datos guardados exitosamente. Total de registros: 2029\n",
      "2025-06-19 11:58:18 - INFO - Rango de fechas: 2024-09-24 12:16:00+00:00 a 2025-04-22 12:43:00+00:00\n",
      "2025-06-19 11:58:18 - INFO - Cerrando conexión a Clickhouse...\n",
      "2025-06-19 11:58:18 - INFO - Conexión a Clickhouse cerrada\n",
      "2025-06-19 11:58:18 - INFO - IV600: ✅\n",
      "2025-06-19 11:58:18 - INFO - Cliente InfluxDB y query_api inicializados.\n",
      "2025-06-19 11:58:18 - INFO - \n",
      "🔹 Descargando PV Glasses...\n",
      "2025-06-19 11:58:18 - INFO - Iniciando descarga de datos PV Glasses...\n",
      "2025-06-19 11:58:18 - INFO - Consultando InfluxDB: bucket=meteo_psda, tables=['6852_Ftc'], attributes=['R_FC1_Avg', 'R_FC2_Avg', 'R_FC3_Avg', 'R_FC4_Avg', 'R_FC5_Avg']\n",
      "2025-06-19 11:58:52 - INFO - Datos PV Glasses guardados exitosamente\n",
      "2025-06-19 11:58:52 - INFO - Total de registros: 83167\n",
      "2025-06-19 11:58:52 - INFO - Rango de fechas: 2024-09-05 13:00:00+00:00 a 2025-06-19 15:50:00+00:00\n",
      "2025-06-19 11:58:53 - INFO - PV Glasses: ✅\n",
      "2025-06-19 11:58:53 - INFO - \n",
      "🔹 Descargando DustIQ...\n",
      "2025-06-19 11:58:53 - INFO - Iniciando descarga de datos DustIQ...\n",
      "2025-06-19 11:58:53 - INFO - Consultando InfluxDB: bucket=PSDA, tables=['DustIQ'], attributes=['SR_C11_Avg', 'SR_C12_Avg']\n",
      "2025-06-19 11:59:33 - INFO - Datos DustIQ guardados exitosamente\n",
      "2025-06-19 11:59:33 - INFO - Total de registros: 492075\n",
      "2025-06-19 11:59:33 - INFO - Rango de fechas: 0 a 492074\n",
      "2025-06-19 11:59:33 - INFO - DustIQ: ✅\n",
      "2025-06-19 11:59:34 - INFO - \n",
      "🔹 Descargando PVStand...\n",
      "2025-06-19 11:59:34 - INFO - Iniciando descarga de datos PVStand...\n",
      "2025-06-19 11:59:34 - INFO - Consultando InfluxDB: bucket=PSDA, tables=['PERC1_fixed_1MD43420160719', 'PERC2_fixed_1MD43920160719'], attributes=['Imax', 'Umax', 'Pmax']\n",
      "2025-06-19 11:59:51 - INFO - Datos PVStand guardados exitosamente\n",
      "2025-06-19 11:59:51 - INFO - Total de registros: 199258\n",
      "2025-06-19 11:59:51 - INFO - Rango de fechas: 0 a 199257\n",
      "2025-06-19 11:59:51 - INFO - PVStand: ✅\n",
      "2025-06-19 11:59:51 - INFO - \n",
      "🔹 Descargando RefCells...\n",
      "2025-06-19 11:59:51 - INFO - Iniciando descarga de datos de celdas de referencia...\n",
      "2025-06-19 11:59:51 - INFO - Consultando InfluxDB: bucket=PSDA, tables=['RefCellsFixed'], attributes=['1RC410(w.m-2)', '1RC411(w.m-2)', '1RC412(w.m-2)']\n",
      "2025-06-19 12:01:14 - INFO - Consultando InfluxDB: bucket=PSDA, tables=['fixed_plant_atamo_1'], attributes=['1RC410(w.m-2)', '1RC411(w.m-2)', '1RC412(w.m-2)']\n",
      "2025-06-19 12:01:45 - INFO - Procesando DataFrame 1 con 2303110 registros\n",
      "2025-06-19 12:01:51 - INFO - DataFrame 1 procesado: 476185 registros después del filtrado\n",
      "2025-06-19 12:01:51 - INFO - Procesando DataFrame 2 con 687875 registros\n",
      "2025-06-19 12:01:51 - INFO - DataFrame 2 procesado: 141941 registros después del filtrado\n",
      "2025-06-19 12:02:14 - INFO - Datos de celdas de referencia guardados exitosamente\n",
      "2025-06-19 12:02:14 - INFO - Total de registros: 618126\n",
      "2025-06-19 12:02:14 - INFO - Columnas guardadas: ['_time', '1RC410(w.m-2)', '1RC411(w.m-2)', '1RC412(w.m-2)']\n",
      "2025-06-19 12:02:15 - INFO - Rango de fechas: 2024-07-23 13:00:01+00:00 a 2025-06-19 16:01:03.455138+00:00\n",
      "2025-06-19 12:02:17 - INFO - RefCells Descarga: ✅\n",
      "2025-06-19 12:02:17 - INFO - 🔹 Procesando RefCells...\n",
      "2025-06-19 12:02:32 - INFO - RefCells Procesamiento: ✅ (88476 registros)\n",
      "2025-06-19 12:02:32 - INFO - \n",
      "🔹 Descargando Soiling Kit...\n",
      "2025-06-19 12:02:33 - INFO - Iniciando descarga de datos del Soiling Kit...\n",
      "2025-06-19 12:02:33 - INFO - Consultando InfluxDB: bucket=PSDA, tables=['soilingkit'], attributes=['Isc(e)', 'Isc(p)', 'Te(C)', 'Tp(C)']\n",
      "2025-06-19 12:02:41 - INFO - Datos del Soiling Kit guardados exitosamente\n",
      "2025-06-19 12:02:41 - INFO - Total de registros: 58325\n",
      "2025-06-19 12:02:41 - INFO - Rango de fechas: 0 a 58324\n",
      "2025-06-19 12:02:41 - INFO - Soiling Kit: ✅\n",
      "2025-06-19 12:02:41 - INFO - \n",
      "🔹 Descargando Temperatura...\n",
      "2025-06-19 12:02:41 - INFO - Iniciando descarga de datos de Temperatura de Módulos (PT100)...\n",
      "2025-06-19 12:02:41 - INFO - Consultando datos de temperatura de ['TempModFixed']...\n",
      "2025-06-19 12:02:41 - INFO - Consultando InfluxDB: bucket=PSDA, tables=['TempModFixed'], attributes=['1TE416(C)', '1TE417(C)', '1TE418(C)', '1TE419(C)']\n",
      "2025-06-19 12:04:42 - INFO - Datos de ['TempModFixed'] procesados. Registros: 552142\n",
      "2025-06-19 12:04:42 - INFO - Consultando datos de temperatura de ['fixed_plant_atamo_1']...\n",
      "2025-06-19 12:04:42 - INFO - Consultando InfluxDB: bucket=PSDA, tables=['fixed_plant_atamo_1'], attributes=['1TE416(C)', '1TE417(C)', '1TE418(C)', '1TE419(C)']\n",
      "2025-06-19 12:05:04 - INFO - Datos de ['fixed_plant_atamo_1'] procesados. Registros: 188574\n",
      "2025-06-19 12:05:32 - INFO - Datos de Temperatura de Módulos guardados exitosamente en /home/nicole/SR/SOILING/datos/temp_mod_fixed_data.csv\n",
      "2025-06-19 12:05:32 - INFO - Total de registros combinados: 740716\n",
      "2025-06-19 12:05:32 - INFO - Rango de fechas: 2024-07-01 13:00:02+00:00 a 2025-06-19 16:04:04.638503+00:00\n",
      "2025-06-19 12:05:32 - INFO - Temperatura: ✅\n",
      "2025-06-19 12:05:33 - INFO - Conexión a InfluxDB cerrada.\n",
      "2025-06-19 12:05:33 - INFO - \n",
      "🎉 Completado: 8/8 exitosos\n",
      "2025-06-19 12:05:33 - INFO - \n",
      "==================================================\n",
      "2025-06-19 12:05:33 - INFO - ✅ iv600\n",
      "2025-06-19 12:05:33 - INFO - ✅ pv_glasses\n",
      "2025-06-19 12:05:33 - INFO - ✅ dustiq\n",
      "2025-06-19 12:05:33 - INFO - ✅ pvstand\n",
      "2025-06-19 12:05:33 - INFO - ✅ refcells\n",
      "2025-06-19 12:05:33 - INFO - ✅ refcells_processed\n",
      "2025-06-19 12:05:33 - INFO - ✅ soiling_kit\n",
      "2025-06-19 12:05:33 - INFO - ✅ temp_mod_fixed\n",
      "2025-06-19 12:05:33 - INFO - ==================================================\n"
     ]
    }
   ],
   "source": [
    "# === EJECUCIÓN OPTIMIZADA COMPLETA ===\n",
    "logger.info(\"🚀 Iniciando descarga completa optimizada...\")\n",
    "results = download_all_data_optimized(START_DATE, END_DATE, OUTPUT_DIR)\n",
    "\n",
    "# Resumen final\n",
    "logger.info(\"\\n\" + \"=\"*50)\n",
    "for process, success in results.items():\n",
    "    status = \"✅\" if success else \"❌\"\n",
    "    logger.info(f\"{status} {process}\")\n",
    "logger.info(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-07 08:13:18 - INFO - \n",
      "================================================================================\n",
      "2025-07-07 08:13:18 - INFO - 🌪️ INICIANDO DESCARGA ESPECÍFICA: DATOS DE SOILING KIT\n",
      "2025-07-07 08:13:18 - INFO - ================================================================================\n",
      "2025-07-07 08:13:18 - INFO - 📅 Rango de fechas: 2024-07-01 a 2025-12-31\n",
      "2025-07-07 08:13:18 - INFO - 📁 Directorio de salida: /home/nicole/SR/SOILING/datos\n",
      "2025-07-07 08:13:18 - INFO - 🚀 Descarga optimizada solo para Soiling Kit\n",
      "2025-07-07 08:13:18 - INFO - Cliente InfluxDB y query_api inicializados.\n",
      "2025-07-07 08:13:18 - INFO - ✅ Conexión a InfluxDB establecida\n",
      "2025-07-07 08:13:18 - INFO - \n",
      "🌪️ Iniciando descarga de Soiling Kit...\n",
      "2025-07-07 08:13:18 - INFO - Iniciando descarga de datos del Soiling Kit...\n",
      "2025-07-07 08:13:18 - INFO - Consultando InfluxDB: bucket=PSDA, tables=['soilingkit'], attributes=['Isc(e)', 'Isc(p)', 'Te(C)', 'Tp(C)']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-07 08:13:33 - INFO - Datos del Soiling Kit guardados exitosamente\n",
      "2025-07-07 08:13:33 - INFO - Total de registros: 65004\n",
      "2025-07-07 08:13:33 - INFO - Rango de fechas: 0 a 65003\n",
      "2025-07-07 08:13:33 - INFO - 📊 Columnas en el archivo: ['Unnamed: 0', 'result', 'table', '_start', '_stop', '_time', '_measurement', 'device', 'Isc(e)', 'Isc(p)', 'Te(C)', 'Tp(C)']\n",
      "2025-07-07 08:13:33 - INFO - 📈 Primeros registros: 10\n",
      "2025-07-07 08:13:33 - INFO - 🔍 Datos del Soiling Kit detectados:\n",
      "2025-07-07 08:13:33 - INFO -    - Isc(e) - Celda limpia: ✅\n",
      "2025-07-07 08:13:33 - INFO -    - Isc(p) - Celda sucia: ✅\n",
      "2025-07-07 08:13:33 - INFO -    - Temperaturas Te(C) y Tp(C): ✅\n",
      "2025-07-07 08:13:33 - INFO - \n",
      "============================================================\n",
      "2025-07-07 08:13:33 - INFO - 🎉 ¡DESCARGA DE SOILING KIT COMPLETADA!\n",
      "2025-07-07 08:13:33 - INFO - 📂 Archivo: soiling_kit_raw_data.csv\n",
      "2025-07-07 08:13:33 - INFO - 📊 Tamaño: 8.65 MB\n",
      "2025-07-07 08:13:33 - INFO - 📈 Total de registros: 65,004\n",
      "2025-07-07 08:13:33 - INFO - 🗂️ Ubicación: /home/nicole/SR/SOILING/datos\n",
      "2025-07-07 08:13:33 - INFO - 🌪️ Datos incluyen: Isc(e), Isc(p), Te(C), Tp(C)\n",
      "2025-07-07 08:13:33 - INFO - ============================================================\n",
      "2025-07-07 08:13:33 - INFO - Conexión a InfluxDB cerrada.\n",
      "2025-07-07 08:13:33 - INFO - 🔌 Conexión a InfluxDB cerrada\n",
      "2025-07-07 08:13:33 - INFO - \n",
      "🏁 PROCESO DE DESCARGA ESPECÍFICA DE SOILING KIT FINALIZADO\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 🌪️ DESCARGA ESPECÍFICA: SOLO DATOS DE SOILING KIT ACTUALIZADOS\n",
    "# ============================================================================\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"🌪️ INICIANDO DESCARGA ESPECÍFICA: DATOS DE SOILING KIT\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "logger.info(f\"📅 Rango de fechas: {START_DATE.strftime('%Y-%m-%d')} a {END_DATE.strftime('%Y-%m-%d')}\")\n",
    "logger.info(f\"📁 Directorio de salida: {OUTPUT_DIR}\")\n",
    "logger.info(\"🚀 Descarga optimizada solo para Soiling Kit\")\n",
    "\n",
    "# Inicializar cliente InfluxDB\n",
    "influx_manager = InfluxDBManager(INFLUX_CONFIG)\n",
    "\n",
    "try:\n",
    "    if not influx_manager.connect():\n",
    "        logger.error(\"❌ No se pudo establecer conexión con InfluxDB\")\n",
    "    else:\n",
    "        logger.info(\"✅ Conexión a InfluxDB establecida\")\n",
    "        \n",
    "        # Ejecutar descarga específica de Soiling Kit\n",
    "        logger.info(\"\\n🌪️ Iniciando descarga de Soiling Kit...\")\n",
    "        success = download_soiling_kit(influx_manager, START_DATE, END_DATE, OUTPUT_DIR)\n",
    "        \n",
    "        # Verificar resultado y mostrar información del archivo\n",
    "        if success:\n",
    "            output_file = os.path.join(OUTPUT_DIR, 'soiling_kit_raw_data.csv')\n",
    "            if os.path.exists(output_file):\n",
    "                file_size_mb = os.path.getsize(output_file) / (1024*1024)\n",
    "                \n",
    "                # Leer primeras líneas para verificar datos\n",
    "                try:\n",
    "                    df_check = pd.read_csv(output_file, nrows=10)\n",
    "                    logger.info(f\"📊 Columnas en el archivo: {list(df_check.columns)}\")\n",
    "                    logger.info(f\"📈 Primeros registros: {len(df_check)}\")\n",
    "                    \n",
    "                    # Contar total de registros\n",
    "                    total_lines = sum(1 for line in open(output_file)) - 1  # -1 para header\n",
    "                    \n",
    "                    # Mostrar información específica del Soiling Kit\n",
    "                    if 'Isc(e)' in df_check.columns and 'Isc(p)' in df_check.columns:\n",
    "                        logger.info(\"🔍 Datos del Soiling Kit detectados:\")\n",
    "                        logger.info(f\"   - Isc(e) - Celda limpia: ✅\")\n",
    "                        logger.info(f\"   - Isc(p) - Celda sucia: ✅\")\n",
    "                        if 'Te(C)' in df_check.columns and 'Tp(C)' in df_check.columns:\n",
    "                            logger.info(f\"   - Temperaturas Te(C) y Tp(C): ✅\")\n",
    "                    \n",
    "                    logger.info(\"\\n\" + \"=\"*60)\n",
    "                    logger.info(\"🎉 ¡DESCARGA DE SOILING KIT COMPLETADA!\")\n",
    "                    logger.info(f\"📂 Archivo: soiling_kit_raw_data.csv\")\n",
    "                    logger.info(f\"📊 Tamaño: {file_size_mb:.2f} MB\")\n",
    "                    logger.info(f\"📈 Total de registros: {total_lines:,}\")\n",
    "                    logger.info(f\"🗂️ Ubicación: {OUTPUT_DIR}\")\n",
    "                    logger.info(\"🌪️ Datos incluyen: Isc(e), Isc(p), Te(C), Tp(C)\")\n",
    "                    logger.info(\"=\"*60)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"⚠️ Error al verificar contenido del archivo: {e}\")\n",
    "                    logger.info(\"✅ Archivo generado exitosamente\")\n",
    "            else:\n",
    "                logger.warning(\"⚠️ Archivo no encontrado después de la descarga\")\n",
    "        else:\n",
    "            logger.error(\"\\n\" + \"=\"*50)\n",
    "            logger.error(\"❌ ERROR EN LA DESCARGA DE SOILING KIT\")\n",
    "            logger.error(\"🔍 Revisa los logs anteriores para más detalles\")\n",
    "            logger.error(\"🔧 Verifica la conexión y configuración de InfluxDB\")\n",
    "            logger.error(\"🔗 Bucket: PSDA, Table: soilingkit\")\n",
    "            logger.error(\"📊 Atributos: Isc(e), Isc(p), Te(C), Tp(C)\")\n",
    "            logger.error(\"=\"*50)\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"❌ Error general en el proceso: {e}\")\n",
    "    import traceback\n",
    "    logger.error(f\"🔍 Detalles del error:\\n{traceback.format_exc()}\")\n",
    "    \n",
    "finally:\n",
    "    # Cerrar conexión\n",
    "    influx_manager.disconnect()\n",
    "    logger.info(\"🔌 Conexión a InfluxDB cerrada\")\n",
    "\n",
    "logger.info(\"\\n🏁 PROCESO DE DESCARGA ESPECÍFICA DE SOILING KIT FINALIZADO\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 15:24:49 - INFO - \n",
      "================================================================================\n",
      "2025-07-02 15:24:49 - INFO - 🔍 DIAGNÓSTICO: VERIFICANDO DATOS DISPONIBLES EN SOILING KIT\n",
      "2025-07-02 15:24:49 - INFO - ================================================================================\n",
      "2025-07-02 15:24:49 - INFO - Cliente InfluxDB y query_api inicializados.\n",
      "2025-07-02 15:24:49 - INFO - ✅ Conexión a InfluxDB establecida para diagnóstico\n",
      "2025-07-02 15:24:49 - INFO - \n",
      "🔍 Consultando rango completo de fechas disponibles...\n",
      "/home/nicole/SR/SOILING/.venv/lib/python3.12/site-packages/influxdb_client/client/warnings.py:31: MissingPivotFunction: The query doesn't contains the pivot() function.\n",
      "\n",
      "The result will not be shaped to optimal processing by pandas.DataFrame. Use the pivot() function by:\n",
      "\n",
      "    \n",
      "        from(bucket: \"PSDA\")\n",
      "            |> range(start: 2024-01-01T00:00:00Z, stop: 2025-07-31T23:59:59Z)\n",
      "            |> filter(fn: (r) => r[\"_measurement\"] == \"soilingkit\")\n",
      "            |> filter(fn: (r) => r[\"_field\"] == \"Isc(e)\" or r[\"_field\"] == \"Isc(p)\" or r[\"_field\"] == \"Te(C)\" or r[\"_field\"] == \"Tp(C)\")\n",
      "            |> keep(columns: [\"_time\", \"_field\", \"_value\"])\n",
      "            |> sort(columns: [\"_time\"])\n",
      "         |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n",
      "\n",
      "You can disable this warning by:\n",
      "    import warnings\n",
      "    from influxdb_client.client.warnings import MissingPivotFunction\n",
      "\n",
      "    warnings.simplefilter(\"ignore\", MissingPivotFunction)\n",
      "\n",
      "For more info see:\n",
      "    - https://docs.influxdata.com/resources/videos/pivots-in-flux/\n",
      "    - https://docs.influxdata.com/flux/latest/stdlib/universe/pivot/\n",
      "    - https://docs.influxdata.com/flux/latest/stdlib/influxdata/influxdb/schema/fieldsascols/\n",
      "\n",
      "  warnings.warn(message, MissingPivotFunction)\n",
      "2025-07-02 15:24:59 - INFO - 📊 Total de registros encontrados (2024-julio 2025): 348152\n",
      "2025-07-02 15:24:59 - INFO - 📅 Fecha más antigua: 2024-01-22 17:02:14+00:00\n",
      "2025-07-02 15:24:59 - INFO - 📅 Fecha más reciente: 2025-05-25 16:07:09+00:00\n",
      "/tmp/ipykernel_19580/372962380.py:70: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  result['year_month'] = result['_time'].dt.to_period('M')\n",
      "2025-07-02 15:24:59 - INFO - \n",
      "📈 Registros por mes:\n",
      "2025-07-02 15:24:59 - INFO -    - 2024-01: 6764 registros\n",
      "2025-07-02 15:24:59 - INFO -    - 2024-02: 20536 registros\n",
      "2025-07-02 15:24:59 - INFO -    - 2024-03: 21604 registros\n",
      "2025-07-02 15:24:59 - INFO -    - 2024-04: 21480 registros\n",
      "2025-07-02 15:24:59 - INFO -    - 2024-05: 22572 registros\n",
      "2025-07-02 15:24:59 - INFO -    - 2024-06: 21896 registros\n",
      "2025-07-02 15:24:59 - INFO -    - 2024-07: 22336 registros\n",
      "2025-07-02 15:24:59 - INFO -    - 2024-08: 22332 registros\n",
      "2025-07-02 15:24:59 - INFO -    - 2024-09: 20992 registros\n",
      "2025-07-02 15:24:59 - INFO -    - 2024-10: 21728 registros\n",
      "2025-07-02 15:24:59 - INFO -    - 2024-11: 21348 registros\n",
      "2025-07-02 15:24:59 - INFO -    - 2024-12: 21600 registros\n",
      "2025-07-02 15:24:59 - INFO -    - 2025-01: 21820 registros\n",
      "2025-07-02 15:24:59 - INFO -    - 2025-02: 19608 registros\n",
      "2025-07-02 15:24:59 - INFO -    - 2025-03: 22484 registros\n",
      "2025-07-02 15:24:59 - INFO -    - 2025-04: 21796 registros\n",
      "2025-07-02 15:24:59 - INFO -    - 2025-05: 17256 registros\n",
      "2025-07-02 15:25:00 - INFO - \n",
      "🔍 Análisis específico 2024:\n",
      "2025-07-02 15:25:00 - INFO -    - Mayo 2024: 22572 registros\n",
      "2025-07-02 15:25:00 - INFO -      Último registro Mayo 2024: 2024-05-31 16:07:14+00:00\n",
      "2025-07-02 15:25:00 - INFO -    - Junio 2024: 21896 registros\n",
      "2025-07-02 15:25:00 - INFO -      Último registro Junio 2024: 2024-06-30 16:07:14+00:00\n",
      "2025-07-02 15:25:00 - INFO -    - Julio 2024: 22336 registros\n",
      "2025-07-02 15:25:00 - INFO -      Último registro Julio 2024: 2024-07-31 16:07:14+00:00\n",
      "2025-07-02 15:25:00 - INFO - \n",
      "🔍 Análisis específico 2025:\n",
      "2025-07-02 15:25:00 - INFO -    - Mayo 2025: 17256 registros\n",
      "2025-07-02 15:25:00 - INFO -      Último registro Mayo 2025: 2025-05-25 16:07:09+00:00\n",
      "2025-07-02 15:25:00 - INFO -    - Junio 2025: 0 registros\n",
      "2025-07-02 15:25:00 - INFO -    - Julio 2025: 0 registros\n",
      "2025-07-02 15:25:00 - WARNING - ⚠️ No se encontraron datos en julio 2025\n",
      "2025-07-02 15:25:00 - WARNING - ⚠️ No se encontraron datos en junio 2025\n",
      "2025-07-02 15:25:00 - INFO - \n",
      "🔍 Verificando mediciones disponibles en bucket PSDA...\n",
      "2025-07-02 15:25:01 - INFO - 📊 Mediciones disponibles en bucket PSDA:\n",
      "2025-07-02 15:25:01 - INFO -    📊 ACpowerNorth\n",
      "2025-07-02 15:25:01 - INFO -    📊 ACpowerSouth\n",
      "2025-07-02 15:25:01 - INFO -    📊 CACTUS_transmittance_losses\n",
      "2025-07-02 15:25:01 - INFO -    📊 CDAQ1_MiniModules\n",
      "2025-07-02 15:25:01 - INFO -    📊 CDAQ1_RefCellsFixed\n",
      "2025-07-02 15:25:01 - INFO -    📊 CDAQ1_TempModFixed\n",
      "2025-07-02 15:25:01 - INFO -    📊 CDAQ1_WindFixed\n",
      "2025-07-02 15:25:01 - INFO -    📊 CDAQ2_ACpower\n",
      "2025-07-02 15:25:01 - INFO -    📊 CDAQ2_DCpower\n",
      "2025-07-02 15:25:01 - INFO -    📊 CDAQ2_RefCellsFixed\n",
      "2025-07-02 15:25:01 - INFO -    📊 CDAQ2_RefCellsHSAT\n",
      "2025-07-02 15:25:01 - INFO -    📊 CDAQ2_TempModFixed\n",
      "2025-07-02 15:25:01 - INFO -    📊 CDAQ2_TempModHSAT\n",
      "2025-07-02 15:25:01 - INFO -    📊 DCpowerNorth\n",
      "2025-07-02 15:25:01 - INFO -    📊 DCpowerSouth\n",
      "2025-07-02 15:25:01 - INFO -    📊 DustIQ\n",
      "2025-07-02 15:25:01 - INFO -    📊 HET1_fixed_1MD41020160719\n",
      "2025-07-02 15:25:01 - INFO -    📊 HET1_tracker_1MD61120160719\n",
      "2025-07-02 15:25:01 - INFO -    📊 HET2_fixed_1MD41520160719\n",
      "2025-07-02 15:25:01 - INFO -    📊 HET2_tracker_1MD61220160719\n",
      "2025-07-02 15:25:01 - INFO -    📊 MiniModules\n",
      "2025-07-02 15:25:01 - INFO -    📊 PERC1_fixed_1MD43420160719\n",
      "2025-07-02 15:25:01 - INFO -    📊 PERC1_tracker_1MD62720160719\n",
      "2025-07-02 15:25:01 - INFO -    📊 PERC2_fixed_1MD43920160719\n",
      "2025-07-02 15:25:01 - INFO -    📊 PERC2_tracker_1MD62820160719\n",
      "2025-07-02 15:25:01 - INFO -    📊 PERCplus1_fixed_1MD41820160719\n",
      "2025-07-02 15:25:01 - INFO -    📊 PERCplus1_tracker_1MD61920160719\n",
      "2025-07-02 15:25:01 - INFO -    📊 PERCplus2_fixed_1MD42320160719\n",
      "2025-07-02 15:25:01 - INFO -    📊 PERCplus2_tracker_1MD62020160719\n",
      "2025-07-02 15:25:01 - INFO -    📊 RefCellsFixed\n",
      "2025-07-02 15:25:01 - INFO -    📊 RefCellsHSAT\n",
      "2025-07-02 15:25:01 - INFO -    📊 RefCellsVertical\n",
      "2025-07-02 15:25:01 - INFO -    📊 SR_IscFit\n",
      "2025-07-02 15:25:01 - INFO -    📊 SR_Mpp\n",
      "2025-07-02 15:25:01 - INFO -    📊 TempModFixed\n",
      "2025-07-02 15:25:01 - INFO -    📊 TempModHSAT\n",
      "2025-07-02 15:25:01 - INFO -    📊 TempModVert\n",
      "2025-07-02 15:25:01 - INFO -    📊 TempModVertical\n",
      "2025-07-02 15:25:01 - INFO -    📊 Vertical TEST\n",
      "2025-07-02 15:25:01 - INFO -    📊 WindFixed\n",
      "2025-07-02 15:25:01 - INFO -    📊 WindHSAT\n",
      "2025-07-02 15:25:01 - INFO -    📊 WindVertical\n",
      "2025-07-02 15:25:01 - INFO -    📊 atamo2_inverters\n",
      "2025-07-02 15:25:01 - INFO -    📊 banco_pruebas\n",
      "2025-07-02 15:25:01 - INFO -    📊 banco_pruebas_psda\n",
      "2025-07-02 15:25:01 - INFO -    📊 dew_point_vaisala\n",
      "2025-07-02 15:25:01 - INFO -    📊 diris_digiware_2HMI801\n",
      "2025-07-02 15:25:01 - INFO -    📊 diris_i35\n",
      "2025-07-02 15:25:01 - INFO -    📊 fixed_plant_atamo_1\n",
      "2025-07-02 15:25:01 - INFO -    📊 fixed_plant_atamo_2\n",
      "2025-07-02 15:25:01 - INFO -    📊 hsat_plant_atamo_1\n",
      "2025-07-02 15:25:01 - INFO -    📊 hsat_plant_atamo_2\n",
      "2025-07-02 15:25:01 - INFO -    📊 huawei_solar_data_acquisition_atamo_2\n",
      "2025-07-02 15:25:01 - INFO -    📊 huawei_solar_data_acquisition_banco_pruebas\n",
      "2025-07-02 15:25:01 - INFO -    📊 huawei_solar_system\n",
      "2025-07-02 15:25:01 - INFO -    📊 mediciones_sensor\n",
      "2025-07-02 15:25:01 - INFO -    📊 minimodules\n",
      "2025-07-02 15:25:01 - INFO -    📊 nPERT1_fixed_1MD42620160719\n",
      "2025-07-02 15:25:01 - INFO -    📊 nPERT1_tracker_1MD63520160719\n",
      "2025-07-02 15:25:01 - INFO -    📊 nPERT2_fixed_1MD43120160719\n",
      "2025-07-02 15:25:01 - INFO -    📊 nPERT2_tracker_1MD63620160719\n",
      "2025-07-02 15:25:01 - INFO -    📊 operUV\n",
      "2025-07-02 15:25:01 - INFO -    🌪️ soilingkit (relacionado con soiling)\n",
      "2025-07-02 15:25:01 - INFO -    🌪️ soilingkit_sr (relacionado con soiling)\n",
      "2025-07-02 15:25:01 - INFO -    📊 tracker_soltec_atamo_1\n",
      "2025-07-02 15:25:01 - INFO -    📊 tracker_soltec_atamo_2\n",
      "2025-07-02 15:25:01 - INFO -    📊 trackers_atamo1\n",
      "2025-07-02 15:25:01 - INFO -    📊 trackers_atamo2\n",
      "2025-07-02 15:25:01 - INFO -    📊 vaisala\n",
      "2025-07-02 15:25:01 - INFO -    📊 vertical_plant\n",
      "2025-07-02 15:25:01 - INFO -    📊 wind_speed_atamo1\n",
      "2025-07-02 15:25:01 - INFO -    📊 wind_speed_atamo2\n",
      "2025-07-02 15:25:01 - INFO - Conexión a InfluxDB cerrada.\n",
      "2025-07-02 15:25:01 - INFO - 🔌 Conexión de diagnóstico cerrada\n",
      "2025-07-02 15:25:01 - INFO - \n",
      "🏁 DIAGNÓSTICO DE SOILING KIT COMPLETADO\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 🔍 DIAGNÓSTICO: VERIFICAR RANGO DE FECHAS DISPONIBLES EN SOILING KIT\n",
    "# ============================================================================\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"🔍 DIAGNÓSTICO: VERIFICANDO DATOS DISPONIBLES EN SOILING KIT\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "# Inicializar cliente InfluxDB para diagnóstico\n",
    "influx_manager = InfluxDBManager(INFLUX_CONFIG)\n",
    "\n",
    "try:\n",
    "    if not influx_manager.connect():\n",
    "        logger.error(\"❌ No se pudo establecer conexión con InfluxDB\")\n",
    "    else:\n",
    "        logger.info(\"✅ Conexión a InfluxDB establecida para diagnóstico\")\n",
    "        \n",
    "        # Crear consulta para verificar rango de fechas disponibles\n",
    "        logger.info(\"\\n🔍 Consultando rango completo de fechas disponibles...\")\n",
    "        \n",
    "        # Consulta para obtener las fechas más recientes y más antiguas (2024-2025)\n",
    "        query_range = f'''\n",
    "        from(bucket: \"PSDA\")\n",
    "            |> range(start: 2024-01-01T00:00:00Z, stop: 2025-07-31T23:59:59Z)\n",
    "            |> filter(fn: (r) => r[\"_measurement\"] == \"soilingkit\")\n",
    "            |> filter(fn: (r) => r[\"_field\"] == \"Isc(e)\" or r[\"_field\"] == \"Isc(p)\" or r[\"_field\"] == \"Te(C)\" or r[\"_field\"] == \"Tp(C)\")\n",
    "            |> keep(columns: [\"_time\", \"_field\", \"_value\"])\n",
    "            |> sort(columns: [\"_time\"])\n",
    "        '''\n",
    "        \n",
    "        try:\n",
    "            result = influx_manager.query_api.query_data_frame(query_range)\n",
    "            \n",
    "            if result.empty:\n",
    "                logger.warning(\"⚠️ No se encontraron datos de Soiling Kit en 2024-2025\")\n",
    "                \n",
    "                # Intentar con un rango más amplio\n",
    "                logger.info(\"🔍 Probando con rango más amplio (2023-2025)...\")\n",
    "                query_wide = f'''\n",
    "                from(bucket: \"PSDA\")\n",
    "                    |> range(start: 2023-01-01T00:00:00Z, stop: 2025-12-31T23:59:59Z)\n",
    "                    |> filter(fn: (r) => r[\"_measurement\"] == \"soilingkit\")\n",
    "                    |> keep(columns: [\"_time\", \"_measurement\", \"_field\"])\n",
    "                    |> group()\n",
    "                    |> sort(columns: [\"_time\"])\n",
    "                '''\n",
    "                \n",
    "                result_wide = influx_manager.query_api.query_data_frame(query_wide)\n",
    "                \n",
    "                if not result_wide.empty:\n",
    "                    logger.info(f\"📊 Total de registros encontrados: {len(result_wide)}\")\n",
    "                    logger.info(f\"📅 Fecha más antigua: {result_wide['_time'].min()}\")\n",
    "                    logger.info(f\"📅 Fecha más reciente: {result_wide['_time'].max()}\")\n",
    "                    \n",
    "                    # Analizar por campo\n",
    "                    if '_field' in result_wide.columns:\n",
    "                        field_counts = result_wide['_field'].value_counts()\n",
    "                        logger.info(\"📊 Registros por campo:\")\n",
    "                        for field, count in field_counts.items():\n",
    "                            logger.info(f\"   - {field}: {count} registros\")\n",
    "                else:\n",
    "                    logger.error(\"❌ No se encontraron datos de Soiling Kit en ningún rango\")\n",
    "            else:\n",
    "                logger.info(f\"📊 Total de registros encontrados (2024-julio 2025): {len(result)}\")\n",
    "                logger.info(f\"📅 Fecha más antigua: {result['_time'].min()}\")\n",
    "                logger.info(f\"📅 Fecha más reciente: {result['_time'].max()}\")\n",
    "                \n",
    "                # Mostrar estadísticas por mes\n",
    "                result['_time'] = pd.to_datetime(result['_time'])\n",
    "                result['year_month'] = result['_time'].dt.to_period('M')\n",
    "                monthly_counts = result.groupby('year_month').size()\n",
    "                \n",
    "                logger.info(\"\\n📈 Registros por mes:\")\n",
    "                for month, count in monthly_counts.items():\n",
    "                    logger.info(f\"   - {month}: {count} registros\")\n",
    "                \n",
    "                # Verificar específicamente los últimos meses de interés\n",
    "                may_2024 = result[(result['_time'].dt.year == 2024) & (result['_time'].dt.month == 5)]\n",
    "                june_2024 = result[(result['_time'].dt.year == 2024) & (result['_time'].dt.month == 6)]\n",
    "                july_2024 = result[(result['_time'].dt.year == 2024) & (result['_time'].dt.month == 7)]\n",
    "                \n",
    "                may_2025 = result[(result['_time'].dt.year == 2025) & (result['_time'].dt.month == 5)]\n",
    "                june_2025 = result[(result['_time'].dt.year == 2025) & (result['_time'].dt.month == 6)]\n",
    "                july_2025 = result[(result['_time'].dt.year == 2025) & (result['_time'].dt.month == 7)]\n",
    "                \n",
    "                logger.info(f\"\\n🔍 Análisis específico 2024:\")\n",
    "                logger.info(f\"   - Mayo 2024: {len(may_2024)} registros\")\n",
    "                if len(may_2024) > 0:\n",
    "                    logger.info(f\"     Último registro Mayo 2024: {may_2024['_time'].max()}\")\n",
    "                logger.info(f\"   - Junio 2024: {len(june_2024)} registros\")\n",
    "                if len(june_2024) > 0:\n",
    "                    logger.info(f\"     Último registro Junio 2024: {june_2024['_time'].max()}\")\n",
    "                logger.info(f\"   - Julio 2024: {len(july_2024)} registros\")\n",
    "                if len(july_2024) > 0:\n",
    "                    logger.info(f\"     Último registro Julio 2024: {july_2024['_time'].max()}\")\n",
    "                \n",
    "                logger.info(f\"\\n🔍 Análisis específico 2025:\")\n",
    "                logger.info(f\"   - Mayo 2025: {len(may_2025)} registros\")\n",
    "                if len(may_2025) > 0:\n",
    "                    logger.info(f\"     Último registro Mayo 2025: {may_2025['_time'].max()}\")\n",
    "                logger.info(f\"   - Junio 2025: {len(june_2025)} registros\")\n",
    "                if len(june_2025) > 0:\n",
    "                    logger.info(f\"     Último registro Junio 2025: {june_2025['_time'].max()}\")\n",
    "                logger.info(f\"   - Julio 2025: {len(july_2025)} registros\")\n",
    "                if len(july_2025) > 0:\n",
    "                    logger.info(f\"     Último registro Julio 2025: {july_2025['_time'].max()}\")\n",
    "                    \n",
    "                # Verificar si hay datos recientes faltantes\n",
    "                if len(july_2025) == 0:\n",
    "                    logger.warning(\"⚠️ No se encontraron datos en julio 2025\")\n",
    "                if len(june_2025) == 0:\n",
    "                    logger.warning(\"⚠️ No se encontraron datos en junio 2025\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Error en la consulta de diagnóstico: {e}\")\n",
    "            \n",
    "        # Verificar también qué mediciones están disponibles en el bucket PSDA\n",
    "        logger.info(\"\\n🔍 Verificando mediciones disponibles en bucket PSDA...\")\n",
    "        try:\n",
    "            query_measurements = '''\n",
    "            import \"influxdata/influxdb/schema\"\n",
    "            schema.measurements(bucket: \"PSDA\")\n",
    "            '''\n",
    "            \n",
    "            measurements = influx_manager.query_api.query(query_measurements)\n",
    "            logger.info(\"📊 Mediciones disponibles en bucket PSDA:\")\n",
    "            for table in measurements:\n",
    "                for record in table.records:\n",
    "                    measurement_name = record.get_value()\n",
    "                    if 'soil' in measurement_name.lower():\n",
    "                        logger.info(f\"   🌪️ {measurement_name} (relacionado con soiling)\")\n",
    "                    else:\n",
    "                        logger.info(f\"   📊 {measurement_name}\")\n",
    "                        \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"⚠️ No se pudieron obtener las mediciones: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"❌ Error general en el diagnóstico: {e}\")\n",
    "    import traceback\n",
    "    logger.error(f\"🔍 Detalles del error:\\n{traceback.format_exc()}\")\n",
    "    \n",
    "finally:\n",
    "    # Cerrar conexión\n",
    "    influx_manager.disconnect()\n",
    "    logger.info(\"🔌 Conexión de diagnóstico cerrada\")\n",
    "\n",
    "logger.info(\"\\n🏁 DIAGNÓSTICO DE SOILING KIT COMPLETADO\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 10:17:47 - INFO - \n",
      "================================================================================\n",
      "2025-07-29 10:17:47 - INFO - 🔋 DESCARGA PVSTAND DESDE CLICKHOUSE\n",
      "2025-07-29 10:17:47 - INFO - ================================================================================\n",
      "2025-07-29 10:17:47 - INFO - �� Rango de fechas: 2024-07-01 a 2025-12-31\n",
      "2025-07-29 10:17:47 - INFO - 📁 Directorio de salida: /home/nicole/SR/SOILING/datos\n",
      "2025-07-29 10:17:47 - INFO - 🚀 Descarga desde ClickHouse - PSDA.perc1fixed y PSDA.perc2fixed\n",
      "2025-07-29 10:17:47 - INFO - Iniciando descarga de datos PVStand desde ClickHouse...\n",
      "2025-07-29 10:17:47 - INFO - Conectando a Clickhouse...\n",
      "2025-07-29 10:17:47 - INFO - Conexión a Clickhouse establecida\n",
      "2025-07-29 10:17:47 - INFO - Consultando datos PVStand desde ClickHouse...\n",
      "2025-07-29 10:17:47 - INFO - Ejecutando consulta: \n",
      "        SELECT \n",
      "            timestamp,\n",
      "            'perc1fixed' as module,\n",
      "            pmax,\n",
      "      ...\n",
      "2025-07-29 10:17:50 - INFO - Datos obtenidos: 221608 registros\n",
      "2025-07-29 10:17:50 - INFO - Procesando datos...\n",
      "2025-07-29 10:17:51 - INFO - Ordenando datos por timestamp...\n",
      "2025-07-29 10:17:51 - INFO - Rango de fechas en los datos:\n",
      "2025-07-29 10:17:51 - INFO - Fecha más antigua: 2024-07-01 00:00:00+00:00\n",
      "2025-07-29 10:17:51 - INFO - Fecha más reciente: 2025-07-29 14:05:00+00:00\n",
      "2025-07-29 10:17:51 - INFO - Distribución por módulo:\n",
      "2025-07-29 10:17:51 - INFO -    - perc1fixed: 110804 registros\n",
      "2025-07-29 10:17:51 - INFO -    - perc2fixed: 110804 registros\n",
      "2025-07-29 10:17:51 - INFO - Guardando datos en: /home/nicole/SR/SOILING/datos/raw_pvstand_clickhouse_data.csv\n",
      "2025-07-29 10:17:54 - INFO - Datos PVStand desde ClickHouse guardados exitosamente\n",
      "2025-07-29 10:17:54 - INFO - Total de registros: 221608\n",
      "2025-07-29 10:17:54 - INFO - Rango de fechas: 2024-07-01 00:00:00+00:00 a 2025-07-29 14:05:00+00:00\n",
      "2025-07-29 10:17:54 - INFO - Estadísticas de los datos por módulo:\n",
      "2025-07-29 10:17:54 - INFO - \n",
      "perc1fixed:\n",
      "2025-07-29 10:17:54 - INFO -    pmax - Rango: 0.000 a 439.380\n",
      "2025-07-29 10:17:54 - INFO -    imax - Rango: 0.000 a 13.797\n",
      "2025-07-29 10:17:54 - INFO -    umax - Rango: 0.065 a 48.757\n",
      "2025-07-29 10:17:54 - INFO - \n",
      "perc2fixed:\n",
      "2025-07-29 10:17:54 - INFO -    pmax - Rango: 0.000 a 449.910\n",
      "2025-07-29 10:17:54 - INFO -    imax - Rango: 0.000 a 12.597\n",
      "2025-07-29 10:17:54 - INFO -    umax - Rango: 0.097 a 52.696\n",
      "2025-07-29 10:17:54 - INFO - \n",
      "Estructura de datos del PVStand:\n",
      "2025-07-29 10:17:54 - INFO -    - module: Identificador del módulo (perc1fixed/perc2fixed)\n",
      "2025-07-29 10:17:54 - INFO -    - pmax: Potencia máxima del módulo\n",
      "2025-07-29 10:17:54 - INFO -    - imax: Corriente máxima del módulo\n",
      "2025-07-29 10:17:54 - INFO -    - umax: Voltaje máximo del módulo\n",
      "2025-07-29 10:17:54 - INFO - Cerrando conexión a Clickhouse...\n",
      "2025-07-29 10:17:54 - INFO - Conexión a Clickhouse cerrada\n",
      "2025-07-29 10:17:54 - INFO - \n",
      "============================================================\n",
      "2025-07-29 10:17:54 - INFO - 🎉 ¡DESCARGA PVSTAND DESDE CLICKHOUSE COMPLETADA!\n",
      "2025-07-29 10:17:54 - INFO - 📂 Archivo: raw_pvstand_clickhouse_data.csv\n",
      "2025-07-29 10:17:54 - INFO - �� Tamaño: 11.62 MB\n",
      "2025-07-29 10:17:54 - INFO - �� Total de registros: 221,608\n",
      "2025-07-29 10:17:54 - INFO - 🗂️ Ubicación: /home/nicole/SR/SOILING/datos\n",
      "2025-07-29 10:17:54 - INFO - �� Datos incluyen: pmax, imax, umax\n",
      "2025-07-29 10:17:54 - INFO - �� Período: Julio 2024 - Julio 2025\n",
      "2025-07-29 10:17:54 - INFO - ============================================================\n",
      "2025-07-29 10:17:54 - INFO - \n",
      "📋 Muestra de los datos:\n",
      "2025-07-29 10:17:54 - INFO -    Columnas: ['timestamp', 'module', 'pmax', 'imax', 'umax']\n",
      "2025-07-29 10:17:54 - INFO -    pmax rango: 0.000 - 0.000\n",
      "2025-07-29 10:17:54 - INFO -    imax rango: 0.000 - 0.001\n",
      "2025-07-29 10:17:54 - INFO -    umax rango: 0.448 - 0.655\n",
      "2025-07-29 10:17:54 - INFO - \n",
      "🏁 DESCARGA PVSTAND DESDE CLICKHOUSE COMPLETADA\n"
     ]
    }
   ],
   "source": [
    "# 🚀 EJECUTAR SOLO DESCARGA PVSTAND DESDE CLICKHOUSE\n",
    "# ============================================================================\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"🔋 DESCARGA PVSTAND DESDE CLICKHOUSE\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "logger.info(f\"�� Rango de fechas: {START_DATE.strftime('%Y-%m-%d')} a {END_DATE.strftime('%Y-%m-%d')}\")\n",
    "logger.info(f\"📁 Directorio de salida: {OUTPUT_DIR}\")\n",
    "logger.info(\"🚀 Descarga desde ClickHouse - PSDA.perc1fixed y PSDA.perc2fixed\")\n",
    "\n",
    "# Ejecutar descarga de PVStand desde ClickHouse\n",
    "success = download_pvstand_clickhouse(START_DATE, END_DATE, OUTPUT_DIR)\n",
    "\n",
    "if success:\n",
    "    # Verificar archivo generado\n",
    "    output_file = os.path.join(OUTPUT_DIR, 'raw_pvstand_iv_data.csv')\n",
    "    if os.path.exists(output_file):\n",
    "        file_size_mb = os.path.getsize(output_file) / (1024*1024)\n",
    "        total_lines = sum(1 for line in open(output_file)) - 1\n",
    "        \n",
    "        logger.info(\"\\n\" + \"=\"*60)\n",
    "        logger.info(\"🎉 ¡DESCARGA PVSTAND DESDE CLICKHOUSE COMPLETADA!\")\n",
    "        logger.info(f\"📂 Archivo: raw_pvstand_iv_data.csv\")\n",
    "        logger.info(f\"�� Tamaño: {file_size_mb:.2f} MB\")\n",
    "        logger.info(f\"�� Total de registros: {total_lines:,}\")\n",
    "        logger.info(f\"🗂️ Ubicación: {OUTPUT_DIR}\")\n",
    "        logger.info(\"�� Datos incluyen: pmax, imax, umax\")\n",
    "        logger.info(\"�� Período: Julio 2024 - Julio 2025\")\n",
    "        logger.info(\"=\"*60)\n",
    "        \n",
    "        # Mostrar muestra de los datos\n",
    "        try:\n",
    "            df_sample = pd.read_csv(output_file, nrows=5)\n",
    "            logger.info(\"\\n📋 Muestra de los datos:\")\n",
    "            logger.info(f\"   Columnas: {list(df_sample.columns)}\")\n",
    "            if 'pmax' in df_sample.columns:\n",
    "                logger.info(f\"   pmax rango: {df_sample['pmax'].min():.3f} - {df_sample['pmax'].max():.3f}\")\n",
    "            if 'imax' in df_sample.columns:\n",
    "                logger.info(f\"   imax rango: {df_sample['imax'].min():.3f} - {df_sample['imax'].max():.3f}\")\n",
    "            if 'umax' in df_sample.columns:\n",
    "                logger.info(f\"   umax rango: {df_sample['umax'].min():.3f} - {df_sample['umax'].max():.3f}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"⚠️ Error al mostrar muestra: {e}\")\n",
    "    else:\n",
    "        logger.warning(\"⚠️ Archivo no encontrado después de la descarga\")\n",
    "else:\n",
    "    logger.error(\"❌ Error en la descarga de PVStand desde ClickHouse\")\n",
    "\n",
    "logger.info(\"\\n🏁 DESCARGA PVSTAND DESDE CLICKHOUSE COMPLETADA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 09:29:46 - INFO - Verificando estructura de tablas PVStand en ClickHouse...\n",
      "2025-07-29 09:29:46 - INFO - Estructura de PSDA.perc1fixed:\n",
      "2025-07-29 09:29:47 - INFO -    timestamp: DateTime\n",
      "2025-07-29 09:29:47 - INFO -    pmax: Float64\n",
      "2025-07-29 09:29:47 - INFO -    imax: Float64\n",
      "2025-07-29 09:29:47 - INFO -    umax: Float64\n",
      "2025-07-29 09:29:47 - INFO -    ipmax: Float64\n",
      "2025-07-29 09:29:47 - INFO -    upmax: Float64\n",
      "2025-07-29 09:29:47 - INFO -    ff_raw: Float64\n",
      "2025-07-29 09:29:47 - INFO -    eta_raw: Float64\n",
      "2025-07-29 09:29:47 - INFO -    e0_pyr: Float64\n",
      "2025-07-29 09:29:47 - INFO -    mpp_fit: Float64\n",
      "2025-07-29 09:29:47 - INFO -    isc_fit: Float64\n",
      "2025-07-29 09:29:47 - INFO -    uoc_fit: Float64\n",
      "2025-07-29 09:29:47 - INFO -    eta_fit: Float64\n",
      "2025-07-29 09:29:47 - INFO -    impp_fit: Float64\n",
      "2025-07-29 09:29:47 - INFO -    umpp_fit: Float64\n",
      "2025-07-29 09:29:47 - INFO -    ff_fit: Float64\n",
      "2025-07-29 09:29:47 - INFO -    mse_mpp_fit: Float64\n",
      "2025-07-29 09:29:47 - INFO - \n",
      "Estructura de PSDA.perc2fixed:\n",
      "2025-07-29 09:29:47 - INFO -    timestamp: DateTime\n",
      "2025-07-29 09:29:47 - INFO -    pmax: Float64\n",
      "2025-07-29 09:29:47 - INFO -    imax: Float64\n",
      "2025-07-29 09:29:47 - INFO -    umax: Float64\n",
      "2025-07-29 09:29:47 - INFO -    ipmax: Float64\n",
      "2025-07-29 09:29:47 - INFO -    upmax: Float64\n",
      "2025-07-29 09:29:47 - INFO -    ff_raw: Float64\n",
      "2025-07-29 09:29:47 - INFO -    eta_raw: Float64\n",
      "2025-07-29 09:29:47 - INFO -    e0_pyr: Float64\n",
      "2025-07-29 09:29:47 - INFO -    mpp_fit: Float64\n",
      "2025-07-29 09:29:47 - INFO -    isc_fit: Float64\n",
      "2025-07-29 09:29:47 - INFO -    uoc_fit: Float64\n",
      "2025-07-29 09:29:47 - INFO -    eta_fit: Float64\n",
      "2025-07-29 09:29:47 - INFO -    impp_fit: Float64\n",
      "2025-07-29 09:29:47 - INFO -    umpp_fit: Float64\n",
      "2025-07-29 09:29:47 - INFO -    ff_fit: Float64\n",
      "2025-07-29 09:29:47 - INFO -    mse_mpp_fit: Float64\n",
      "2025-07-29 09:29:47 - INFO - \n",
      "Muestra de datos de PSDA.perc1fixed:\n",
      "2025-07-29 09:29:47 - INFO -    (datetime.datetime(2019, 12, 17, 14, 22, tzinfo=<StaticTzInfo 'Etc/UTC'>), 226.32, 6.5475, 44.5, 6.1577, 36.754, 0.777, 0.137, 827.5, 226.77, 6.5426, 44.5, 0.137, 6.0514, 37.475, 0.779, 3.97)\n",
      "2025-07-29 09:29:47 - INFO -    (datetime.datetime(2019, 12, 17, 14, 31, tzinfo=<StaticTzInfo 'Etc/UTC'>), 234.97, 6.8528, 44.34, 6.4186, 36.608, 0.773, 0.137, 858.4, 236.09, 6.8522, 44.34, 0.138, 6.3382, 37.249, 0.777, 14.09)\n",
      "2025-07-29 09:29:47 - INFO -    (datetime.datetime(2019, 12, 17, 14, 35, tzinfo=<StaticTzInfo 'Etc/UTC'>), 238.7, 6.9867, 44.246, 6.5322, 36.541, 0.772, 0.137, 871.1, 239.62, 6.9845, 44.246, 0.138, 6.4608, 37.089, 0.775, 10.65)\n",
      "2025-07-29 09:29:47 - INFO - \n",
      "Muestra de datos de PSDA.perc2fixed:\n",
      "2025-07-29 09:29:47 - INFO -    (datetime.datetime(2019, 12, 17, 14, 22, tzinfo=<StaticTzInfo 'Etc/UTC'>), 230.48, 6.6033, 44.937, 6.1923, 37.221, 0.777, 0.139, 827.5, 228.94, 6.5917, 44.937, 0.138, 6.0112, 38.085, 0.773, 4.49)\n",
      "2025-07-29 09:29:47 - INFO -    (datetime.datetime(2019, 12, 17, 14, 31, tzinfo=<StaticTzInfo 'Etc/UTC'>), 239.48, 6.903, 44.772, 6.4573, 37.086, 0.775, 0.139, 858.4, 238.63, 6.9032, 44.772, 0.139, 6.3162, 37.781, 0.772, 1.88)\n",
      "2025-07-29 09:29:47 - INFO -    (datetime.datetime(2019, 12, 17, 14, 35, tzinfo=<StaticTzInfo 'Etc/UTC'>), 243.43, 7.0412, 44.704, 6.5715, 37.043, 0.773, 0.14, 871.1, 241.47, 7.0414, 44.704, 0.139, 6.4007, 37.725, 0.767, 4.1)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 🔍 DIAGNÓSTICO: VERIFICAR ESTRUCTURA DE TABLAS PVSTAND\n",
    "# ============================================================================\n",
    "\n",
    "logger.info(\"Verificando estructura de tablas PVStand en ClickHouse...\")\n",
    "client = clickhouse_connect.get_client(\n",
    "    host=CLICKHOUSE_CONFIG['host'],\n",
    "    port=CLICKHOUSE_CONFIG['port'],\n",
    "    username=CLICKHOUSE_CONFIG['user'],\n",
    "    password=CLICKHOUSE_CONFIG['password']\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Verificar estructura de perc1fixed\n",
    "    logger.info(\"Estructura de PSDA.perc1fixed:\")\n",
    "    result1 = client.query(\"DESCRIBE PSDA.perc1fixed\")\n",
    "    for row in result1.result_set:\n",
    "        logger.info(f\"   {row[0]}: {row[1]}\")\n",
    "    \n",
    "    # Verificar estructura de perc2fixed\n",
    "    logger.info(\"\\nEstructura de PSDA.perc2fixed:\")\n",
    "    result2 = client.query(\"DESCRIBE PSDA.perc2fixed\")\n",
    "    for row in result2.result_set:\n",
    "        logger.info(f\"   {row[0]}: {row[1]}\")\n",
    "        \n",
    "    # Verificar algunos datos de muestra\n",
    "    logger.info(\"\\nMuestra de datos de PSDA.perc1fixed:\")\n",
    "    sample1 = client.query(\"SELECT * FROM PSDA.perc1fixed LIMIT 3\")\n",
    "    for row in sample1.result_set:\n",
    "        logger.info(f\"   {row}\")\n",
    "        \n",
    "    logger.info(\"\\nMuestra de datos de PSDA.perc2fixed:\")\n",
    "    sample2 = client.query(\"SELECT * FROM PSDA.perc2fixed LIMIT 3\")\n",
    "    for row in sample2.result_set:\n",
    "        logger.info(f\"   {row}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error en diagnóstico: {e}\")\n",
    "finally:\n",
    "    client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 09:19:02 - INFO - \n",
      "================================================================================\n",
      "2025-07-29 09:19:02 - INFO - 🔋 INICIANDO DESCARGA ESPECÍFICA: DATOS DE PVSTAND\n",
      "2025-07-29 09:19:02 - INFO - ================================================================================\n",
      "2025-07-29 09:19:02 - INFO - 📅 Rango de fechas: 2024-07-01 a 2025-12-31\n",
      "2025-07-29 09:19:02 - INFO - 📁 Directorio de salida: /home/nicole/SR/SOILING/datos\n",
      "2025-07-29 09:19:02 - INFO - 🚀 Descarga optimizada solo para PVStand\n",
      "2025-07-29 09:19:02 - INFO - Cliente InfluxDB y query_api inicializados.\n",
      "2025-07-29 09:19:02 - INFO - ✅ Conexión a InfluxDB establecida\n",
      "2025-07-29 09:19:02 - INFO - \n",
      "🔋 Iniciando descarga de PVStand...\n",
      "2025-07-29 09:19:02 - ERROR - ❌ Error general en el proceso: name 'download_pvstand' is not defined\n",
      "2025-07-29 09:19:02 - ERROR - 🔍 Detalles del error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3612/427214555.py\", line 24, in <module>\n",
      "    success = download_pvstand(influx_manager, START_DATE, END_DATE, OUTPUT_DIR)\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "NameError: name 'download_pvstand' is not defined. Did you mean: 'download_dustiq'?\n",
      "\n",
      "2025-07-29 09:19:02 - INFO - Conexión a InfluxDB cerrada.\n",
      "2025-07-29 09:19:02 - INFO - 🔌 Conexión a InfluxDB cerrada\n",
      "2025-07-29 09:19:02 - INFO - \n",
      "🏁 PROCESO DE DESCARGA ESPECÍFICA DE PVSTAND FINALIZADO\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 🔋 DESCARGA ESPECÍFICA: SOLO DATOS DE PVSTAND ACTUALIZADOS INFLUXDB\n",
    "# ============================================================================\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"🔋 INICIANDO DESCARGA ESPECÍFICA: DATOS DE PVSTAND\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "logger.info(f\"📅 Rango de fechas: {START_DATE.strftime('%Y-%m-%d')} a {END_DATE.strftime('%Y-%m-%d')}\")\n",
    "logger.info(f\"📁 Directorio de salida: {OUTPUT_DIR}\")\n",
    "logger.info(\"🚀 Descarga optimizada solo para PVStand\")\n",
    "\n",
    "# Inicializar cliente InfluxDB\n",
    "influx_manager = InfluxDBManager(INFLUX_CONFIG)\n",
    "\n",
    "try:\n",
    "    if not influx_manager.connect():\n",
    "        logger.error(\"❌ No se pudo establecer conexión con InfluxDB\")\n",
    "    else:\n",
    "        logger.info(\"✅ Conexión a InfluxDB establecida\")\n",
    "        \n",
    "        # Ejecutar descarga específica de PVStand\n",
    "        logger.info(\"\\n🔋 Iniciando descarga de PVStand...\")\n",
    "        success = download_pvstand(influx_manager, START_DATE, END_DATE, OUTPUT_DIR)\n",
    "        \n",
    "        # Verificar resultado y mostrar información del archivo\n",
    "        if success:\n",
    "            output_file = os.path.join(OUTPUT_DIR, 'raw_pvstand_iv_data.csv')\n",
    "            if os.path.exists(output_file):\n",
    "                file_size_mb = os.path.getsize(output_file) / (1024*1024)\n",
    "                \n",
    "                # Leer primeras líneas para verificar datos\n",
    "                try:\n",
    "                    df_check = pd.read_csv(output_file, nrows=5)\n",
    "                    logger.info(f\"📊 Columnas en el archivo: {list(df_check.columns)}\")\n",
    "                    logger.info(f\"📈 Primeros registros: {len(df_check)}\")\n",
    "                    \n",
    "                    # Contar total de registros\n",
    "                    total_lines = sum(1 for line in open(output_file)) - 1  # -1 para header\n",
    "                    \n",
    "                    logger.info(\"\\n\" + \"=\"*60)\n",
    "                    logger.info(\"🎉 ¡DESCARGA DE PVSTAND COMPLETADA!\")\n",
    "                    logger.info(f\"📂 Archivo: raw_pvstand_iv_data.csv\")\n",
    "                    logger.info(f\"📊 Tamaño: {file_size_mb:.2f} MB\")\n",
    "                    logger.info(f\"📈 Total de registros: {total_lines:,}\")\n",
    "                    logger.info(f\"🗂️ Ubicación: {OUTPUT_DIR}\")\n",
    "                    logger.info(\"=\"*60)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"⚠️ Error al verificar contenido del archivo: {e}\")\n",
    "                    logger.info(\"✅ Archivo generado exitosamente\")\n",
    "            else:\n",
    "                logger.warning(\"⚠️ Archivo no encontrado después de la descarga\")\n",
    "        else:\n",
    "            logger.error(\"\\n\" + \"=\"*50)\n",
    "            logger.error(\"❌ ERROR EN LA DESCARGA DE PVSTAND\")\n",
    "            logger.error(\"🔍 Revisa los logs anteriores para más detalles\")\n",
    "            logger.error(\"🔧 Verifica la conexión y configuración de InfluxDB\")\n",
    "            logger.error(\"=\"*50)\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"❌ Error general en el proceso: {e}\")\n",
    "    import traceback\n",
    "    logger.error(f\"🔍 Detalles del error:\\n{traceback.format_exc()}\")\n",
    "    \n",
    "finally:\n",
    "    # Cerrar conexión\n",
    "    influx_manager.disconnect()\n",
    "    logger.info(\"🔌 Conexión a InfluxDB cerrada\")\n",
    "\n",
    "logger.info(\"\\n🏁 PROCESO DE DESCARGA ESPECÍFICA DE PVSTAND FINALIZADO\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 12:54:18 - INFO - \n",
      "================================================================================\n",
      "2025-07-24 12:54:18 - INFO - 📊 INICIANDO DESCARGA ESPECÍFICA: DATOS DE DUSTIQ\n",
      "2025-07-24 12:54:18 - INFO - ================================================================================\n",
      "2025-07-24 12:54:18 - INFO - 📅 Rango de fechas: 2024-07-01 a 2025-12-31\n",
      "2025-07-24 12:54:18 - INFO - 📁 Directorio de salida: /home/nicole/SR/SOILING/datos\n",
      "2025-07-24 12:54:18 - INFO - 🚀 Descarga optimizada solo para DustIQ\n",
      "2025-07-24 12:54:18 - INFO - Cliente InfluxDB y query_api inicializados.\n",
      "2025-07-24 12:54:18 - INFO - ✅ Conexión a InfluxDB establecida\n",
      "2025-07-24 12:54:18 - INFO - \n",
      "📊 Iniciando descarga de DustIQ...\n",
      "2025-07-24 12:54:18 - INFO - Iniciando descarga de datos DustIQ...\n",
      "2025-07-24 12:54:18 - INFO - Consultando InfluxDB: bucket=PSDA, tables=['DustIQ'], attributes=['SR_C11_Avg', 'SR_C12_Avg']\n",
      "2025-07-24 12:54:57 - INFO - Datos DustIQ guardados exitosamente\n",
      "2025-07-24 12:54:57 - INFO - Total de registros: 537765\n",
      "2025-07-24 12:54:57 - INFO - Rango de fechas: 0 a 537764\n",
      "2025-07-24 12:54:57 - INFO - 📊 Columnas en el archivo: ['Unnamed: 0', 'result', 'table', '_start', '_stop', '_time', '_measurement', 'SR_C11_Avg', 'SR_C12_Avg']\n",
      "2025-07-24 12:54:57 - INFO - 📈 Primeros registros: 10\n",
      "2025-07-24 12:54:57 - INFO - 🔍 Datos del DustIQ detectados:\n",
      "2025-07-24 12:54:57 - INFO -    - SR_C11_Avg - Sensor 1: ✅\n",
      "2025-07-24 12:54:57 - INFO -    - SR_C12_Avg - Sensor 2: ✅\n",
      "2025-07-24 12:54:57 - INFO -    - Datos válidos SR_C11_Avg: 10/10\n",
      "2025-07-24 12:54:57 - INFO -    - Datos válidos SR_C12_Avg: 10/10\n",
      "2025-07-24 12:54:57 - INFO - \n",
      "============================================================\n",
      "2025-07-24 12:54:57 - INFO - 🎉 ¡DESCARGA DE DUSTIQ COMPLETADA!\n",
      "2025-07-24 12:54:57 - INFO - 📂 Archivo: raw_dustiq_data.csv\n",
      "2025-07-24 12:54:57 - INFO - 📊 Tamaño: 57.79 MB\n",
      "2025-07-24 12:54:57 - INFO - 📈 Total de registros: 537,765\n",
      "2025-07-24 12:54:57 - INFO - 🗂️ Ubicación: /home/nicole/SR/SOILING/datos\n",
      "2025-07-24 12:54:57 - INFO - 📊 Datos incluyen: SR_C11_Avg, SR_C12_Avg\n",
      "2025-07-24 12:54:57 - INFO - 📡 Sensores de irradiancia para análisis de soiling\n",
      "2025-07-24 12:54:57 - INFO - ============================================================\n",
      "2025-07-24 12:54:57 - INFO - Conexión a InfluxDB cerrada.\n",
      "2025-07-24 12:54:57 - INFO - 🔌 Conexión a InfluxDB cerrada\n",
      "2025-07-24 12:54:57 - INFO - \n",
      "🏁 PROCESO DE DESCARGA ESPECÍFICA DE DUSTIQ FINALIZADO\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 📊 DESCARGA ESPECÍFICA: SOLO DATOS DE DUSTIQ ACTUALIZADOS\n",
    "# ============================================================================\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"📊 INICIANDO DESCARGA ESPECÍFICA: DATOS DE DUSTIQ\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "logger.info(f\"📅 Rango de fechas: {START_DATE.strftime('%Y-%m-%d')} a {END_DATE.strftime('%Y-%m-%d')}\")\n",
    "logger.info(f\"📁 Directorio de salida: {OUTPUT_DIR}\")\n",
    "logger.info(\"🚀 Descarga optimizada solo para DustIQ\")\n",
    "\n",
    "# Inicializar cliente InfluxDB\n",
    "influx_manager = InfluxDBManager(INFLUX_CONFIG)\n",
    "\n",
    "try:\n",
    "    if not influx_manager.connect():\n",
    "        logger.error(\"❌ No se pudo establecer conexión con InfluxDB\")\n",
    "    else:\n",
    "        logger.info(\"✅ Conexión a InfluxDB establecida\")\n",
    "        \n",
    "        # Ejecutar descarga específica de DustIQ\n",
    "        logger.info(\"\\n📊 Iniciando descarga de DustIQ...\")\n",
    "        success = download_dustiq(influx_manager, START_DATE, END_DATE, OUTPUT_DIR)\n",
    "        \n",
    "        # Verificar resultado y mostrar información del archivo\n",
    "        if success:\n",
    "            output_file = os.path.join(OUTPUT_DIR, 'raw_dustiq_data.csv')\n",
    "            if os.path.exists(output_file):\n",
    "                file_size_mb = os.path.getsize(output_file) / (1024*1024)\n",
    "                \n",
    "                # Leer primeras líneas para verificar datos\n",
    "                try:\n",
    "                    df_check = pd.read_csv(output_file, nrows=10)\n",
    "                    logger.info(f\"📊 Columnas en el archivo: {list(df_check.columns)}\")\n",
    "                    logger.info(f\"📈 Primeros registros: {len(df_check)}\")\n",
    "                    \n",
    "                    # Contar total de registros\n",
    "                    total_lines = sum(1 for line in open(output_file)) - 1  # -1 para header\n",
    "                    \n",
    "                    # Mostrar información específica del DustIQ\n",
    "                    if 'SR_C11_Avg' in df_check.columns and 'SR_C12_Avg' in df_check.columns:\n",
    "                        logger.info(\"🔍 Datos del DustIQ detectados:\")\n",
    "                        logger.info(f\"   - SR_C11_Avg - Sensor 1: ✅\")\n",
    "                        logger.info(f\"   - SR_C12_Avg - Sensor 2: ✅\")\n",
    "                        \n",
    "                        # Verificar si hay datos válidos (no NaN)\n",
    "                        valid_c11 = df_check['SR_C11_Avg'].notna().sum()\n",
    "                        valid_c12 = df_check['SR_C12_Avg'].notna().sum()\n",
    "                        logger.info(f\"   - Datos válidos SR_C11_Avg: {valid_c11}/{len(df_check)}\")\n",
    "                        logger.info(f\"   - Datos válidos SR_C12_Avg: {valid_c12}/{len(df_check)}\")\n",
    "                    \n",
    "                    logger.info(\"\\n\" + \"=\"*60)\n",
    "                    logger.info(\"🎉 ¡DESCARGA DE DUSTIQ COMPLETADA!\")\n",
    "                    logger.info(f\"📂 Archivo: raw_dustiq_data.csv\")\n",
    "                    logger.info(f\"📊 Tamaño: {file_size_mb:.2f} MB\")\n",
    "                    logger.info(f\"📈 Total de registros: {total_lines:,}\")\n",
    "                    logger.info(f\"🗂️ Ubicación: {OUTPUT_DIR}\")\n",
    "                    logger.info(\"📊 Datos incluyen: SR_C11_Avg, SR_C12_Avg\")\n",
    "                    logger.info(\"📡 Sensores de irradiancia para análisis de soiling\")\n",
    "                    logger.info(\"=\"*60)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"⚠️ Error al verificar contenido del archivo: {e}\")\n",
    "                    logger.info(\"✅ Archivo generado exitosamente\")\n",
    "            else:\n",
    "                logger.warning(\"⚠️ Archivo no encontrado después de la descarga\")\n",
    "        else:\n",
    "            logger.error(\"\\n\" + \"=\"*50)\n",
    "            logger.error(\"❌ ERROR EN LA DESCARGA DE DUSTIQ\")\n",
    "            logger.error(\"🔍 Revisa los logs anteriores para más detalles\")\n",
    "            logger.error(\"🔧 Verifica la conexión y configuración de InfluxDB\")\n",
    "            logger.error(\"🔗 Bucket: PSDA, Table: DustIQ\")\n",
    "            logger.error(\"📊 Atributos: SR_C11_Avg, SR_C12_Avg\")\n",
    "            logger.error(\"=\"*50)\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"❌ Error general en el proceso: {e}\")\n",
    "    import traceback\n",
    "    logger.error(f\"🔍 Detalles del error:\\n{traceback.format_exc()}\")\n",
    "    \n",
    "finally:\n",
    "    # Cerrar conexión\n",
    "    influx_manager.disconnect()\n",
    "    logger.info(\"🔌 Conexión a InfluxDB cerrada\")\n",
    "\n",
    "logger.info(\"\\n🏁 PROCESO DE DESCARGA ESPECÍFICA DE DUSTIQ FINALIZADO\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 16:55:25 - INFO - Iniciando descarga de datos DustIQ desde ClickHouse...\n",
      "2025-07-28 16:55:25 - INFO - Conectando a Clickhouse...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 16:55:26 - INFO - Conexión a Clickhouse establecida\n",
      "2025-07-28 16:55:26 - INFO - Consultando datos DustIQ desde ClickHouse...\n",
      "2025-07-28 16:55:26 - INFO - Ejecutando consulta: \n",
      "        SELECT \n",
      "            Stamptime,\n",
      "            Attribute,\n",
      "            Measure\n",
      "        FROM PSDA...\n",
      "2025-07-28 16:55:51 - INFO - Datos obtenidos: 1156170 registros\n",
      "2025-07-28 16:55:51 - INFO - Procesando datos...\n",
      "2025-07-28 16:56:06 - INFO - Pivotando datos de long format a wide format...\n",
      "2025-07-28 16:56:06 - INFO - Manejando duplicados agrupando por promedio...\n",
      "2025-07-28 16:56:14 - INFO - Rango de fechas en los datos:\n",
      "2025-07-28 16:56:14 - INFO - Fecha más antigua: 2024-07-01 00:00:00+00:00\n",
      "2025-07-28 16:56:14 - INFO - Fecha más reciente: 2025-07-28 00:00:00+00:00\n",
      "2025-07-28 16:56:14 - INFO - Guardando datos en: /home/nicole/SR/SOILING/datos/raw_dustiq_data.csv\n",
      "2025-07-28 16:56:56 - INFO - Datos DustIQ desde ClickHouse guardados exitosamente\n",
      "2025-07-28 16:56:56 - INFO - Total de registros: 543525\n",
      "2025-07-28 16:56:56 - INFO - Rango de fechas: 2024-07-01 00:00:00+00:00 a 2025-07-28 00:00:00+00:00\n",
      "2025-07-28 16:56:56 - INFO - Estadísticas de los datos:\n",
      "2025-07-28 16:56:56 - INFO - SR_C11_Avg - Rango: 0.000 a 100.000\n",
      "2025-07-28 16:56:56 - INFO - SR_C12_Avg - Rango: 0.000 a 101.000\n",
      "2025-07-28 16:56:56 - INFO - Cerrando conexión a Clickhouse...\n",
      "2025-07-28 16:56:56 - INFO - Conexión a Clickhouse cerrada\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_dustiq_clickhouse(START_DATE, END_DATE, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 14:34:47 - INFO - \n",
      "================================================================================\n",
      "2025-07-25 14:34:47 - INFO - 🌪️ DESCARGA SOILING KIT DESDE CLICKHOUSE\n",
      "2025-07-25 14:34:47 - INFO - ================================================================================\n",
      "2025-07-25 14:34:47 - INFO - �� Rango de fechas: 2024-07-01 a 2025-12-31\n",
      "2025-07-25 14:34:47 - INFO - 📁 Directorio de salida: /home/nicole/SR/SOILING/datos\n",
      "2025-07-25 14:34:47 - INFO - 🚀 Descarga desde ClickHouse - PSDA.soilingkit\n",
      "2025-07-25 14:34:47 - INFO - Iniciando descarga de datos del Soiling Kit desde ClickHouse...\n",
      "2025-07-25 14:34:47 - INFO - Conectando a Clickhouse...\n",
      "2025-07-25 14:34:50 - INFO - Conexión a Clickhouse establecida\n",
      "2025-07-25 14:34:50 - INFO - Consultando datos del Soiling Kit desde ClickHouse...\n",
      "2025-07-25 14:34:50 - INFO - Ejecutando consulta: \n",
      "        SELECT \n",
      "            Stamptime,\n",
      "            Attribute,\n",
      "            Measure\n",
      "        FROM PSDA...\n",
      "2025-07-25 14:35:22 - INFO - Datos obtenidos: 1067240 registros\n",
      "2025-07-25 14:35:22 - INFO - Procesando datos...\n",
      "2025-07-25 14:35:44 - INFO - Pivotando datos de long format a wide format...\n",
      "2025-07-25 14:35:44 - INFO - Manejando duplicados agrupando por promedio...\n",
      "2025-07-25 14:35:45 - INFO - Rango de fechas en los datos:\n",
      "2025-07-25 14:35:45 - INFO - Fecha más antigua: 2024-07-01 12:02:15+00:00\n",
      "2025-07-25 14:35:45 - INFO - Fecha más reciente: 2025-07-25 14:07:14+00:00\n",
      "2025-07-25 14:35:45 - INFO - Guardando datos en: /home/nicole/SR/SOILING/datos/soiling_kit_clickhouse_data.csv\n",
      "2025-07-25 14:35:49 - INFO - Datos del Soiling Kit desde ClickHouse guardados exitosamente\n",
      "2025-07-25 14:35:49 - INFO - Total de registros: 68282\n",
      "2025-07-25 14:35:49 - INFO - Rango de fechas: 2024-07-01 12:02:15+00:00 a 2025-07-25 14:07:14+00:00\n",
      "2025-07-25 14:35:49 - INFO - Estadísticas de los datos:\n",
      "2025-07-25 14:35:49 - INFO - Isc(e) - Rango: 0.247 a 1.952\n",
      "2025-07-25 14:35:49 - INFO - Isc(p) - Rango: 0.063 a 1.977\n",
      "2025-07-25 14:35:49 - INFO - Te(C) - Rango: 0.0 a 312.5\n",
      "2025-07-25 14:35:49 - INFO - Tp(C) - Rango: 0.0 a 624.5\n",
      "2025-07-25 14:35:49 - INFO - Estructura de datos del Soiling Kit:\n",
      "2025-07-25 14:35:49 - INFO -    - Isc(e): Corriente de cortocircuito de la celda limpia (referencia)\n",
      "2025-07-25 14:35:49 - INFO -    - Isc(p): Corriente de cortocircuito de la celda sucia (panel)\n",
      "2025-07-25 14:35:49 - INFO -    - Te(C): Temperatura de la celda limpia en Celsius\n",
      "2025-07-25 14:35:49 - INFO -    - Tp(C): Temperatura de la celda sucia en Celsius\n",
      "2025-07-25 14:35:49 - INFO - Cerrando conexión a Clickhouse...\n",
      "2025-07-25 14:35:49 - INFO - Conexión a Clickhouse cerrada\n",
      "2025-07-25 14:35:50 - INFO - \n",
      "============================================================\n",
      "2025-07-25 14:35:50 - INFO - 🎉 ¡DESCARGA SOILING KIT DESDE CLICKHOUSE COMPLETADA!\n",
      "2025-07-25 14:35:50 - INFO - 📂 Archivo: soiling_kit_clickhouse_data.csv\n",
      "2025-07-25 14:35:50 - INFO - �� Tamaño: 6.14 MB\n",
      "2025-07-25 14:35:50 - INFO - �� Total de registros: 68,282\n",
      "2025-07-25 14:35:50 - INFO - 🗂️ Ubicación: /home/nicole/SR/SOILING/datos\n",
      "2025-07-25 14:35:50 - INFO - ��️ Datos incluyen: Isc(e), Isc(p), Te(C), Tp(C)\n",
      "2025-07-25 14:35:50 - INFO - �� Período: Julio 2024 - Julio 2025\n",
      "2025-07-25 14:35:50 - INFO - ============================================================\n",
      "2025-07-25 14:35:50 - INFO - \n",
      "📋 Muestra de los datos:\n",
      "2025-07-25 14:35:50 - INFO -    Columnas: ['timestamp', 'Isc(e)', 'Isc(p)', 'Te(C)', 'Tp(C)']\n",
      "2025-07-25 14:35:50 - INFO -    Isc(e) rango: 1.029 - 1.032\n",
      "2025-07-25 14:35:50 - INFO -    Isc(p) rango: 1.078 - 1.080\n",
      "2025-07-25 14:35:50 - INFO - \n",
      "�� DESCARGA SOILING KIT DESDE CLICKHOUSE COMPLETADA\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 🚀 EJECUCIÓN: DESCARGA SOILING KIT DESDE CLICKHOUSE\n",
    "# ============================================================================\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"🌪️ DESCARGA SOILING KIT DESDE CLICKHOUSE\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "logger.info(f\"�� Rango de fechas: {START_DATE.strftime('%Y-%m-%d')} a {END_DATE.strftime('%Y-%m-%d')}\")\n",
    "logger.info(f\"📁 Directorio de salida: {OUTPUT_DIR}\")\n",
    "logger.info(\"🚀 Descarga desde ClickHouse - PSDA.soilingkit\")\n",
    "\n",
    "# Ejecutar descarga del Soiling Kit desde ClickHouse\n",
    "success = download_soiling_kit_clickhouse(START_DATE, END_DATE, OUTPUT_DIR)\n",
    "\n",
    "if success:\n",
    "    # Verificar archivo generado\n",
    "    output_file = os.path.join(OUTPUT_DIR, 'soiling_kit_raw_data.csv')\n",
    "    if os.path.exists(output_file):\n",
    "        file_size_mb = os.path.getsize(output_file) / (1024*1024)\n",
    "        total_lines = sum(1 for line in open(output_file)) - 1\n",
    "        \n",
    "        logger.info(\"\\n\" + \"=\"*60)\n",
    "        logger.info(\"🎉 ¡DESCARGA SOILING KIT DESDE CLICKHOUSE COMPLETADA!\")\n",
    "        logger.info(f\"📂 Archivo: soiling_kit_raw_data.csv\")\n",
    "        logger.info(f\"�� Tamaño: {file_size_mb:.2f} MB\")\n",
    "        logger.info(f\"�� Total de registros: {total_lines:,}\")\n",
    "        logger.info(f\"🗂️ Ubicación: {OUTPUT_DIR}\")\n",
    "        logger.info(\"��️ Datos incluyen: Isc(e), Isc(p), Te(C), Tp(C)\")\n",
    "        logger.info(\"�� Período: Julio 2024 - Julio 2025\")\n",
    "        logger.info(\"=\"*60)\n",
    "        \n",
    "        # Mostrar muestra de los datos\n",
    "        try:\n",
    "            df_sample = pd.read_csv(output_file, nrows=5)\n",
    "            logger.info(\"\\n📋 Muestra de los datos:\")\n",
    "            logger.info(f\"   Columnas: {list(df_sample.columns)}\")\n",
    "            if 'Isc(e)' in df_sample.columns:\n",
    "                logger.info(f\"   Isc(e) rango: {df_sample['Isc(e)'].min():.3f} - {df_sample['Isc(e)'].max():.3f}\")\n",
    "            if 'Isc(p)' in df_sample.columns:\n",
    "                logger.info(f\"   Isc(p) rango: {df_sample['Isc(p)'].min():.3f} - {df_sample['Isc(p)'].max():.3f}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"⚠️ Error al mostrar muestra: {e}\")\n",
    "    else:\n",
    "        logger.warning(\"⚠️ Archivo no encontrado después de la descarga\")\n",
    "else:\n",
    "    logger.error(\"❌ Error en la descarga del Soiling Kit desde ClickHouse\")\n",
    "\n",
    "logger.info(\"\\n�� DESCARGA SOILING KIT DESDE CLICKHOUSE COMPLETADA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
